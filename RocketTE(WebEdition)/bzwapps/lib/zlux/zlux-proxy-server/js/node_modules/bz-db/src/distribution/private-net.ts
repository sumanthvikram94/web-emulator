import { PeerId } from '@libp2p/interface-peer-id'
import { PrivateNetNode } from './private-net-node.js'
import { InternalDatabaseInterface } from '../main/db-interface.js'
import { IPeerMeta, NodeMetadata, IPeerState } from './node-metadata.js'
import { NET_STATUS} from '../constants/actions.js'
import { CONFLICT_TYPE, STATUS } from '../constants/status.js'
import { CONFIG } from '../constants/config.js'
import { CHANNELS } from '../constants/channels.js'
import { NodeMsgHandler, IAutoScaleResult } from './node-msg-handler.js'
import { DbWorker } from '../main/db-worker.js'
import { Task, Workflow, TaskStatus } from '../workflow/index.js'
import { Utils, uid, elu, loggers, timeTracer } from '../services/index.js'
import { BlockChainStat } from '../blockchain/blockchain.js'
import { DEStatistics } from '../internal-db/data-entity.js'
import { FileSyncTask, FileSyncMsgType, FileSyncMsg, FileSync } from '../main/file-sync.js'  // BZ-15355, sync file for cluster
import { ClusterNodeType,DATA_CONFLICT_POLICY } from '../main/metadata.js'
import Queue from 'better-queue';
const checkinLogger = loggers.checkin
const clusterLogger = loggers.cluster
const pullLogger = loggers.pull

export enum AUTO_SCALE_STATUS {
    NONE = 0,
    ACCEPTING, // Auto scaling request received and processing
    ACCEPTED // Auto scaling accepted and joint cluster
}

class NodeStateHandler{
    _peers: Map<string, IPeerState>
    _privateNet: PrivateNet
    _interval: any
    _connectedPeers: any = []
    _debounceId: any;

    constructor(privateNet: PrivateNet){
        this._privateNet = privateNet
        this._peers = new Map()
    }

    setPeerState(peerId: string, peerState: IPeerState){
        this._peers.set(peerId, peerState)
    }

    get connectedPeers(){
        return this._connectedPeers
    }

    get peers(){
        return this._peers
    }

    getPeerState(peerId: string){
        const peer = this.peers.get(peerId)
        return peer
    }

    /**
     * 
     * @returns All the active peers
     */
    getActivePeers(): IPeerMeta[] {
        const peers:any[] = []
        this._peers.forEach((val) => {
            if (val.status === NET_STATUS.PEER_CONNECTED){
                peers.push(val.peer.getPeerMeta())
            }
        })
        return peers
    }

    getActivePeersAndSelf(): IPeerMeta[] {
        const peers:any[] = this.getActivePeers()
        const selfNode=this._privateNet._node.metadata;
        peers.push(selfNode)
        return peers
    }

    debounce(func: any, wait = 500) {
        if(this._debounceId){
            clearTimeout(this._debounceId);
        }
        this._debounceId = setTimeout(() => func(), wait);
    }

    async checkPeerState(){
        const peersRecord = this._privateNet._worker._internalDb.selectSync('meta_peers')
        let isAllReady = true
        if (peersRecord?.rowCount > 0){
            const selfId = this._privateNet._node.metadata.id;
            for await(const peer of peersRecord.data){
                if(peer.id !== selfId){
                    let peerState: any = this.getPeerState(peer.id);
                    if(!peerState){
                        const peerInst = await NodeMetadata.fromJSON(peer)
                        await this.addAddressBook(peerInst)
                        peerState = {
                            id: peerInst.id,
                            peer: peerInst
                        }
                        this.setPeerState(peerInst.id, peerState)
                    }
                    if(peerState.status !== NET_STATUS.PEER_CONNECTED){
                        try{
                            this._privateNet._node.dial(peerState.peer)
                        } catch (e) {}
                        isAllReady = false
                    }
                }
            }
        }
        return isAllReady
    }

    async setAddressBook(){
        const peersRecord = this._privateNet._worker._internalDb.selectSync('meta_peers')
        if (peersRecord && peersRecord.rowCount > 1){
            for await (let peer of peersRecord.data) {
                if (this.getPeerState(peer.id)) continue
                const peerInst = await NodeMetadata.fromJSON(peer)
                await this.addAddressBook(peerInst)
                this.setPeerState(peerInst.id, {
                    id: peerInst.id,
                    peer: peerInst
                })
            }
        }
    }

    /**
     * Adds a peer into address book
     * @param peer 
     * @returns 
     */
    async addAddressBook(peer: NodeMetadata){
        const ma = await this._privateNet._node._node.peerStore.addressBook.getPeerRecord(peer.peerIdInst)
        if (ma) return
        await this._privateNet._node._node.peerStore.addressBook.set(peer.peerIdInst, peer.multiaddrs)
    }
}

/**
 * Class to handle the private network logics
 */
class PrivateNet{

    _node: PrivateNetNode
    _msgHandler: NodeMsgHandler
    _internalDb: InternalDatabaseInterface
    _worker: DbWorker
    _peers: Map<string, IPeerState>
    _pullingProm: Promise<any> | undefined
    _autoScaleStatus: AUTO_SCALE_STATUS = AUTO_SCALE_STATUS.NONE
    _scalingNodeList: Map<string, any>
    _clusterResolve: Function
    _autoScaleQueue: Queue;
    _nodeStateHandler: NodeStateHandler

    constructor(worker: DbWorker){
        this._worker = worker
        this._peers = new Map()
        this._internalDb = this._worker._internalDb
        this._scalingNodeList = new Map()
        this._nodeStateHandler = new NodeStateHandler(this)
        this._autoScaleQueue = new Queue(async(peerId: any, callback) => {
            await this.autoScale(peerId);
            // clusterLogger.logInfo(`auto scale queue: ${JSON.stringify(result)}`)
            callback();
          }, { afterProcessDelay: 200 });  // a little delay
    }

    get autoScaleStatus(): AUTO_SCALE_STATUS {
        return this._autoScaleStatus
    }

    set autoScaleStatus(val: AUTO_SCALE_STATUS) {
        this._autoScaleStatus = val
    }

    /**
     * Launch the local node.
     * @param forceReLaunch 
     */
    private async launchNode(forceReLaunch: boolean = false){
        if (this._node && forceReLaunch){
            await this._node.stop()
        }
        if (!this._node || forceReLaunch){
            this._node = new PrivateNetNode(this._worker, this)
            await this._node.start()
            this._msgHandler = this._node._msgHandler
            const peerInfo = this._node.getPeerInfo()
            uid.setIp(this._node._node.getMultiaddrs()[0])
            clusterLogger.logWarn('Local node metadata: ' + JSON.stringify(peerInfo))
        }
    }

    /**
     * Send message to the specific peer for a task
     * @param peer 
     * @param channel 
     * @param taskid 
     * @param payload 
     * @returns 
     */
    async sendTask(peer: IPeerMeta, channel: string, taskid: string, payload: object, isUnknownPeer = false, isNotZipped = false){
        try{
            const data = {
                peer: isUnknownPeer? this._node.getPeerInfo(): {id: this._node.getPeerInfo().id},
                taskid, 
                payload
            }
            const msg = JSON.stringify(data)
            clusterLogger.logDebug(`Sending task to ${peer.id}, channel: ${channel}`, taskid)
            const peerData = peer && peer.multiaddrs? peer : this._internalDb.selectSync('meta_peers', {id: peer.id}).data[0]
            if (!peerData) { // In case the peer is kicked, it still can send data to the reject channel
                clusterLogger.logWarn(`Task Failed. ${peer.id} is already kicked.`, taskid)
            } else {
                return await this.sendMsg(peerData, channel, msg, isNotZipped)
            }
        } catch (e) {
            clusterLogger.logWarn('Error while sending message to peer ' + peer.id + ', channel: ' + channel, taskid)
            if (e.message === 'All promises were rejected') { // Libp2p will raise this error if peer can't be reached.
                const peerState = this._nodeStateHandler.getPeerState(peer.id)
                if (peerState){
                    peerState.status = NET_STATUS.PEER_CONNECT_FAILED
                }
                clusterLogger.logWarn('Peer not connected: ' + peer.id, taskid)
            } else {
                clusterLogger.logError(e, taskid)
            }
            this.worker.workflow.getTask(taskid)?.reject({status:false, message: e.message})
            return {status:false, message: e.message}
        }
    }

    getPeerConnection(id: PeerId): any{
        return this._node._node.getConnections(id)
    }

    private setTaskBroadcastStatus(task: Task, peerId: string, status: string){
        let broadcastStatus = task.getProperty('broadcast')
        if (!broadcastStatus){
            broadcastStatus = new Map<string, string>()
        }
        broadcastStatus.set(peerId, status)
        task.setProperty('broadcast', broadcastStatus)
    }
    
    async showStatus(isInfo = true){
        const selfId = this._node.getPeerInfo().id
        let output = 'Show node status: ' + 
            '\nSelf id: ' + selfId +
            '\nStatus: ' + this.worker.getStatus() +
            '\nCluster port: ' + this._node.getPeerInfo().port +
            '\nIs listening: ' + this._node.isListening() +
            '\nPeer states: '
        Array.from(this._nodeStateHandler.peers.values()).forEach(obj => {
            if (selfId === obj['id']) {
                return // Avoid printing self
            }
            output += '\n - ' + obj['id'] + ' - ' + obj['status']
        })
        output += '\nConnections: '
        Array.from(this._node._node.getConnections()).forEach( conn => {
            output += '\n - ' + conn.remoteAddr.toString() + ' - ' + conn.stat.status
        })
        output += '\nAddress book: '
        Array.from(await this._node._node.peerStore.all()).forEach( peerId => {
            output += '\n - ' + peerId.id.toString()
            peerId.addresses?.forEach(addr => {
                output += '\n    -- ' + addr.multiaddr.toString()
            })
        })
        if (isInfo){
            clusterLogger.logWarn(output)
        } else {
            clusterLogger.logInfo(output)
        }
        // this._worker.workflow.describeTasks(isInfo)
    }

    /**
     * Broadcast message to all peers for a task
     * @param task 
     * @param channel 
     * @param payload 
     */
    async broadcast(task: Task, channel: string, payload: object, workflow?: Workflow, timeoutMs = 60000,toPeerIds?: Array<string>){
        timeTracer.trace('broadcast' + channel)
        const tasks:Task[] = []
        const peerMap:any = {}
        // this.showStatus()
        const peersRecord = this._internalDb.selectSync('meta_peers')
        if(peersRecord?.rowCount > 0){
            if(toPeerIds){ //only send task to certain peers
                peersRecord.data=peersRecord.data.filter((p:any)=>{return toPeerIds?.includes(p.id )})
            }
            for(const p of peersRecord.data){
                peerMap[p.id] = p
            }
        }
        const wf = workflow? workflow : this.worker.workflow
        const taskId = task.id
        if (this._nodeStateHandler.peers.size > 0){
            for (let peerState of this._nodeStateHandler.peers.values()) {
                if (peerState.id === this._node.metadata.id) continue
                if(!peerMap[peerState.id]) continue
                if (peerState.status !== NET_STATUS.PEER_CONNECTED){
                    //TODO dial
                    this._nodeStateHandler.debounce(this._nodeStateHandler.checkPeerState, CONFIG.CHECK_NODE_STATE_DEBOUNCE_TIME);
                    clusterLogger.logWarn('Peer not connected: ' + peerState.id, taskId)
                    this.setTaskBroadcastStatus(task, peerState.id, NET_STATUS.BROADCAST_NOT_ONLINE)
                    continue
                }
                const subTaskId = taskId + ':cast:' + this.worker.genTaskIdShort()
                const t = wf.generateTask({timeoutMs, desc: 'Broadcast to peer: ' + peerState.id + ', channel: ' + channel}, async () => {
                    try{
                        clusterLogger.logDebug('Executing broadcast task to channel: ' + channel, subTaskId)
                        const data = {
                            peer: {id: this._node.getPeerInfo().id},
                            taskid:t.id,
                            payload
                        }
                        // records the broadcast status
                        
                        this.setTaskBroadcastStatus(task, peerState.id, NET_STATUS.BROADCAST_SENT)
                        await this.sendMsg(peerMap[peerState.id], channel, JSON.stringify(data))
                    } catch(e){
                        clusterLogger.logDebug('Error while sending message to peer ' + peerState.id + ', channel: ' + channel, subTaskId)
                        if (e.message === 'All promises were rejected') { // Libp2p will raise this error if peer can't be reached.
                            peerState.status = NET_STATUS.PEER_CONNECT_FAILED
                            clusterLogger.logWarn('Peer not connected: ' + peerState.id, subTaskId)
                        } else {
                            clusterLogger.logError(e, subTaskId)
                        }
                        this.setTaskBroadcastStatus(task, peerState.id, NET_STATUS.BROADCAST_NOT_ONLINE)
                        wf.getTask(t.id)?.resolve({status:false, message:'Peer not connected'})
                    }
                }, subTaskId)
                t.on('beforeResolve', () => {
                    let broadcastStatus = task.getProperty('broadcast')
                    if (broadcastStatus && broadcastStatus.get(peerState.id) === NET_STATUS.BROADCAST_SENT){
                        broadcastStatus.set(peerState.id, NET_STATUS.BROADCAST_DONE)
                        task.setProperty('broadcast', broadcastStatus)
                    }
                })
                tasks.push(t)
            }
        }
        task.on('afterResolve', () => {
            const broadcastStatus = task.getProperty('broadcast')
            if (broadcastStatus){
                let str = ''
                for (const peer of Array.from(broadcastStatus.keys())){
                    str += '\nPeer: ' + peer + ', status: ' + broadcastStatus.get(peer)
                }
                if (channel !== CHANNELS.STATUS){// hide the info for status broadcasting.
                    clusterLogger.logDebug(`Broadcast status of ${task.description}: ${str}`, taskId)
                }
            }
            timeTracer.trace('broadcast' + channel)
        })
        task.resetReadyFlag()
        task.addSubTasks(tasks)
        task.execSubTasks()
    }

    async getPeerMeta(peerId: any) {
        const ma = await this._node._node.peerStore.get(peerId)
        const multiaddrs: string[] = []
        ma.addresses.forEach((val) => {
            multiaddrs.push(val.multiaddr.toString())
        })
        const peerMeta = await NodeMetadata.fromJSON({
            id: peerId.toString(),
            peerId: peerId,
            multiaddrs
        })
        return peerMeta
    }

    isAutoScalingEnabled() {
        return this._worker._dbMeta.cluster.enabled && this._worker._dbMeta.cluster.autoScaling.enabled
    }

    isScalableNode() { // The node type is scalingNode. 
        return this._worker._dbMeta.cluster.autoScaling.nodeType === ClusterNodeType.SCALABLE
    }

    /**
     * For auto-scaling clustering. 
     * Persistent nodes send auto-scale request to new instance, and new instance will reply with its peer metadata. 
     * @param peerId 
     * @returns 
     */
    async autoScale(peerId: any) {
        return new Promise(async (resolve) => {
            try{
                const taskId = this._worker.genTaskId()
                if (!this.isAutoScalingEnabled()) {
                    clusterLogger.logInfo('Auto scaling is not enabled', taskId)
                    resolve({status: true, msg: 'Auto scaling is not enabled'}) // Auto scaling is not enabled. Do nothing.
                    return
                }
                if (this._scalingNodeList.has(peerId.toString())){ // The same node is already scaling
                    clusterLogger.logInfo('The same node is already scaling', taskId)
                    resolve({status: true, msg: 'The same node is already scaling'})
                    return
                }
                
                const metaPeers = await this._internalDb.selectSync('meta_peers');
                const exist = metaPeers.data.findIndex((d: any) => d.id === peerId.toString()) > -1
    
                if(exist) {
                    clusterLogger.logInfo('the same node is already in current cluster', taskId)
                    resolve({status: true, msg: 'the same node is already in current cluster'}) // the same node is already in current cluster.
                    return
                }
    
                clusterLogger.logInfo('Auto-scaling starts for node:' + peerId.toString(), taskId)
                this._scalingNodeList.set(peerId.toString(), 'scaling')
                
                const peerNode = await this.getPeerMeta(peerId)
                const asTask =  this.worker.workflow.generateTask({desc: 'Auto scale', timeoutMs: 300000}, async () => { // 
                    clusterLogger.logDebug('Auto scale task id: ' + asTask.id)
                    return await this.sendTask(peerNode.getPeerMeta()!, CHANNELS.AUTO_SCALE, asTask.id, {}) // Requests the new instance to provide peer metadata
                }, taskId)
                asTask.execute()
                const asr: IAutoScaleResult = await asTask.getPromise() // Gets the response from new instance
                clusterLogger.logDebug('Response from peer for auto-scale: ' + JSON.stringify(asr), taskId)
                if (asr && asr.status === true && asr.peerMeta){ // It's OK to continue the auto-scaling
                    const peerMeta = asr.peerMeta
                    try {
                        if (peerMeta.nodeType === ClusterNodeType.PERSISTENT) { // Both the nodes are persistent node. The node started earlier becomes primary.
                            const localStartTs = this._node._meta.timestamp !== undefined ? this._node._meta.timestamp : 0
                            const remoteStartTs = peerMeta.timestamp !== undefined? peerMeta.timestamp : 0
                            if ((localStartTs > remoteStartTs) || (localStartTs === remoteStartTs && peerMeta.id < this._node._meta.id)) {
                                clusterLogger.logInfo('Awaiting to be introduced by node:' + peerId.toString(), taskId)
                                return
                            }
                        }
                        clusterLogger.logInfo('Introducing node:' + peerId.toString(), taskId)
                        await this._worker.introduceNode(peerMeta) // The primary node introduces the secondary node.
                        this._scalingNodeList.delete(peerId.toString())
                        clusterLogger.logInfo('Auto-scaling finish add node into cluster:' + peerId.toString(), taskId)
                        resolve({status: true, msg: 'Successed auto scale'})
                    } catch(e) {
                        clusterLogger.logSevere('Introducing failed for peer: ' + peerMeta.id, taskId)
                        clusterLogger.logError(e)
                        this._scalingNodeList.delete(peerId.toString())
                        resolve({status: false, msg: 'Faild to introduce peer'})
                    }
                } else {
                    clusterLogger.logWarn('Auto-scale failed. No peer metadata provided.', taskId)
                    this._scalingNodeList.delete(peerId.toString())
                    resolve({status: false, msg: 'Auto-scale failed. No peer metadata provided.'})
                }
            } catch (e) {
                console.error(e)
                resolve({status: false, msg: 'Failed auto scale', error: e})
            }
        })
    }

    /**
     * add remote peer into queue
     * @param peerId 
     */
    public pushAutoScale(peerId: any) {
        clusterLogger.logInfo(`PushAutoScale:: peerId: ${JSON.stringify(peerId)}`)
        this._autoScaleQueue.push(peerId);
    }

    /**
     * auto scale mode: check all nodes has added into cluster and max wait 20 seconds.
     * Ingore checking: 1. not autoscale mode; 2. the node which has in cluster; 3. no peer list; 4. one node in peerlist and which is itself 
     */
    async waitCluster() {
        if (!this.isAutoScalingEnabled()) {
            return // Auto scaling is not enabled. Do nothing.
        }

        const peerList = this.worker._dbMeta.cluster?.autoScaling?.peerList || []
        const metaNode = await this._internalDb.selectSync('meta_node')
        const id = metaNode.rowCount > 0 ? metaNode.data[0].id : ''

        if(this.isInCluster() || peerList.length === 0 || (peerList.length === 1 && peerList[0] === id)) {
            return
        }

        const prom = new Promise((resolve) => {
            this._clusterResolve = resolve;
        })

        setTimeout(() => {
            this._clusterResolve();
        }, 20000)

        await prom;
    }

    /**
     * Used in case of auto-scaling with bootstrap method
     * @returns The array of multiaddrs
     */
    async getClusterNodeAddrs(){
        await this._node.waitForClusterNodeStart()
        return this._node._meta.multiaddrs || []
    }


    /**
     * Process to ping a peer
     * @param peerInfo 
     * @returns 
     */
    async ping(peerInfo: IPeerMeta){
        const peer = await NodeMetadata.fromJSON(peerInfo)
        await this._nodeStateHandler.addAddressBook(peer)
        try{
            await this._node._node.dial(peer.peerIdInst) // connect to the peer
            const pingTask =  this.worker.workflow.generateTask({desc: 'PING task'}, async () => {
                clusterLogger.logDebug('Ping peer: ' + peerInfo.id)
                return await this.sendTask(peerInfo, CHANNELS.PING, pingTask.id, {})
            })
            pingTask.on('executed', (pingResult) => {
                if (pingResult && pingResult.status === false){ // In case the sendTask failed
                    pingTask.resolve({status: false, message: 'Target peer is not online'})
                }
            })
            pingTask.execute()
            return pingTask.getPromise()
        }catch(err){
            clusterLogger.logError(err)
            return {status: false, message: err.message}
        }
    }

    /**
     * Process to ping a peer
     * @param peerInfo 
     * @returns 
     */
     async checkStatus(peerInfo: IPeerMeta, isUnknownPeer = false, taskId: string){
        const statusTask =  this.worker.workflow.generateTask({timeoutMs: 60000, desc: 'STATUS task'}, async () => {
            clusterLogger.logDebug('Check peer status: ' + peerInfo.id)
            return await this.sendTask(peerInfo, CHANNELS.STATUS, statusTask.id, {"extendRequired":true}, isUnknownPeer)
        }, taskId)

        statusTask.execute()
        return statusTask.getPromise()
    }

    /**
     * Process to pull chains and data from a peer
     * @param peerId 
     * @returns 
     */
    async pull(peerId: string,entities?:Array<string>) {
        const worker = this._worker
        const cit = worker.workflowExec.searchTaskDesc('PULL process')
        if (cit.length > 0){ 
            return await cit[0].getPromise() // avoid double pull
        }
        const taskId = this.worker.genTaskIdShort()
        pullLogger.logInfo('Pulling chains from peer: ' + peerId, taskId)
        const peerInfo = this._nodeStateHandler.getPeerState(peerId)?.peer.getPeerMeta()
        if (!peerInfo){  // TBD, the peerState of the new node to introduce is not correct.
            pullLogger.logInfo('Peer not found: ' + peerId, taskId)
            return
        }
        const pullTask = worker.workflowExec.generateTask({desc: 'PULL process'}, async () => {
            let pullPayload
            if(entities){ //partial
                pullLogger.logInfo(`Pulling entities ${entities.join()} from peer: ` + peerId, taskId)
                pullPayload = {entities}
            }else{
                pullPayload = Utils.mapToArray((await this.worker.doGetChainsStat(undefined,peerId, taskId)).stats)
            }
            
            this._worker.statusService.status = STATUS.PULLING
            this.sendTask(peerInfo, CHANNELS.PULL, pullTask.id, pullPayload) // provide local chain stats to do partially pull
        }, taskId)
        // await worker.workflowExec.enqueueTask(pullTask)
        pullTask.execute()
        this._pullingProm = pullTask.getPromise()
        await this._pullingProm
        this._pullingProm = undefined
        worker.statusService.status = STATUS.READY
        return {status: true, message: 'Pull process complete, source peer: ' + peerId}
    }

    async waitForPull(){
        if (this._pullingProm !== undefined){
            return await this._pullingProm
        } else {
            return true
        }
    }
    
    /**
     * Process to force pull all the chains and data from a peer
     * @param peerId 
     * @returns 
     */
     async pullAll(peerId: string) {
        const taskId = this.worker.genTaskIdShort()
        pullLogger.logInfo('Start fully pull from peer: ' + peerId, taskId)
        const peerInfo = this._nodeStateHandler.getPeerState(peerId)?.peer.getPeerMeta()
        if (!peerInfo){  // TBD, the peerState of the new node to introduce is not correct.
            pullLogger.logInfo('Peer not found: ' + peerId, taskId)
            return
        }
        const pullTask = this.worker.workflowExec.generateTask({desc: 'PULL process'}, () => {
            this._worker.statusService.status = STATUS.PULLING
            this.sendTask(peerInfo, CHANNELS.PULL, pullTask.id, {})
        }, taskId)
        // pullTask.on('beforeResolve', async () => {
        // })
        pullTask.execute()
        this._pullingProm = pullTask.getPromise()
        await this._pullingProm
        this._pullingProm = undefined
        await this.checkFileSync4PullAll(peerId);  // BZ-15355, sync file for cluster
        this._worker.statusService.status = STATUS.READY
        return {status: true, message: 'Pull process complete, source peer: ' + peerId}
    }

    /**
     * Process to check if FileSync is done
     * @param peerId 
     * @returns 
     */
    async checkFileSync4PullAll(peerId: string) {
        clusterLogger.logInfo(`FileSync::checkIsDone4PullAll -START-`);
        const peerInfo = this._nodeStateHandler.getPeerState(peerId)?.peer.getPeerMeta()
        if (!peerInfo ){
            clusterLogger.logInfo(`FileSync::checkIsDone4PullAll, peer not found ${peerId}`);
            return;
        }
        const checkTask = this.worker.workflow.generateTask({desc: 'checkFileSyncDone4PullAll'}, () => {
            this._worker.statusService.status = STATUS.PULLING;
            const fsm: FileSyncMsg = this._worker._fileSync.getFileSyncMsg4PullAll(checkTask.id);
            clusterLogger.logInfo(`FileSync::checkIsDone4PullAll, request FST_PULL_DONE(${fsm.taskId})`);
            this.sendTask(peerInfo, CHANNELS.FILE_TRANSFER_REQ, checkTask.id, [fsm]);
        })
        checkTask.execute();
        await checkTask.getPromise();
        this._worker.statusService.status = STATUS.READY;
        return {status: true, message: 'FileSync::checkIsDone4PullAll -DONE-, source peer: ' + peerId};
    }

    /**
     * Process to checkin to private network
     */
    async checkin(isMustDo: boolean = false, tskId?: string) {
        if (elu.isBusy && isMustDo !== true) {
            const msg = 'BZDB is busy, postponing checkin process.'
            checkinLogger.logWarn(msg)
            this.cancelAutoCheckin()
            this.bookAutoCheckin(600000) // postpone the checkin for 10 mins.
            return {status: false, message: msg}
        }
        await this._worker.waitForExecutingBlocks()
        const cit = this._worker.workflow.searchTaskDesc('Checkin process')

        // avoid double checkin
        //for case, if mutiple nodes start in a short period time
        //run max 2 checkin process in the queue to solve the issue
        if (isMustDo) {
            if (cit.length > 1) { return await cit[1].getPromise(); }
        } else {
            if (cit.length > 0) { return await cit[0].getPromise(); }
        }
        const taskId = tskId? tskId : this.worker.genTaskIdShort()
        checkinLogger.logWarn('checkin process start', taskId)
        const checkinProcess = await this.createCheckinTask(taskId)
        try{
            const val = await this.worker.workflow.enqueueTask(checkinProcess)
            checkinLogger.logInfo(val, taskId)
            return val
        } catch (e) {
            checkinLogger.logError(e, taskId)
            return {status: false, message: e.message}
        }
    }

    private async createCheckinTask(taskId: string){
        const chainsStat = Utils.mapToArray((await this._worker.getStatistics('chain', taskId)).chain.stats)
        let correctPeerid = ''
        const checkinProcess = this.worker.workflow.generateTask({desc: 'Checkin process'}, ()=>{
            this.cancelAutoCheckin()
            this._worker.statusService.status = STATUS.CHECKIN
            this.broadcast(checkinProcess, CHANNELS.CHECKIN, {chainsStat}, undefined, 180000) // Increased checkin process waiting time to 3 mins
            checkinLogger.logInfo('checkin task executed', taskId)
        }, taskId)
        checkinLogger.logInfo('create checkin task', taskId)
        checkinProcess.onAsync('subTasksClose', async () => {
            checkinLogger.logInfo('checkin task back', taskId);
            const nodeData:Map<string, any> = checkinProcess.getProperty('nodeData')
            if (!nodeData){
                if (this.isInCluster()){
                    this._worker.statusService.status = STATUS.LONELY_ISLAND
                    checkinLogger.logWarn('finish, no peer found,set status to LONELY_ISLAND', taskId);
                }
                return checkinProcess.resolve('No peer found')
            }
            const mySize = this.worker.chainsSize
            const myStats = await this.worker.getStatistics('both', taskId)
            const myChainStats = myStats.chain.stats
            const myDbStats: Map<string, DEStatistics> = myStats.db
            let maxSize = mySize
            let isNetInConflictStat = false
            checkinLogger.logInfo('Comparing chains and db', taskId)
            let conflictDataArr: Array<any> = [];
            for (const peerid of Array.from(nodeData.keys())){
                if (nodeData.get(peerid).isConflict){
                    // checkinLogger.logSevere(`Data conflict status returned by peer, peerid is ${peerid}`, taskId)
                    const message = `Data conflicts due to peer`;
                    const detail = `Data conflicts due to peer, peerid is ${peerid}`;
                    const data = {
                        task : 'event',
                        cmd: STATUS.DATA_CONFLICT,
                        parames: Utils.getParamObject(message, detail, CONFLICT_TYPE.PEER, peerid,this._node?.metadata)
                    }
                    checkinLogger.logSevere(detail, taskId);
                    conflictDataArr.push(data);
                    // this._worker.taskSender.sendEvent(data, false);
                    this._worker.statusService.status = STATUS.DATA_CONFLICT
                }
                if (nodeData.get(peerid).isInConflictStat){
                    isNetInConflictStat = true
                }
                const chains: Map<string, BlockChainStat> = nodeData.get(peerid).chainsStat
                if (!chains){
                    continue // chains of the peer is not recorded for any reason
                }
                // Here we are marking the peer with max size of chains as the correct one. 
                const peerSize = Utils.sumArray(Array.from(chains.values()), 'size')
                if (peerSize > maxSize){
                    maxSize = peerSize
                    correctPeerid = peerid
                }
            }
            // In case local chains total size is max, check each chain.
            // if chain total size is equal or is the max,then check each chain to caculate the correct peer 
            // check which one has more longer chains, especail for the peer which certain chains are longer then me, need pull 
            if (correctPeerid === ''){ 
                const peerCount = new Map<number, string>()
                for (const peerid of Array.from(nodeData.keys())){
                    const chains: Map<string, BlockChainStat> = nodeData.get(peerid).chainsStat
                    const largerChains = new Map<string, any>()
                    for (const deName of myChainStats.keys()){
                        const myDeStat = myChainStats.get(deName) 
                        const deStat = chains.get(deName)
                        const chain = this._worker.findChain(deName)
                        if (deStat && myDeStat){
                            if (deStat.size === myDeStat.size && deStat.lastHash !== myDeStat.lastHash){
                                const resolve=await this.autoResolveConflict(deName,{peerid})
                                if(!resolve){
                                    const msg = `Data conflict discovered. Peer: ${peerid}, Data entity: ${deName}, Block index: ${deStat.lastIndex},`
                                    +` local hash: ${myDeStat.lastHash}, peer hash: ${deStat.lastHash}. Remote size is equal to local size but local hash is not equal to peer hash`;
                                    const message =  'Data conflicts due to different hashes between the remote and the local';
                                    const data = {
                                        task : 'event',
                                        cmd: STATUS.DATA_CONFLICT,
                                        
                                        parames: Utils.getParamObject(message, msg, CONFLICT_TYPE.BLOCKCHAIN, peerid,this._node?.metadata,deName,String(myDeStat?.lastTimeStamp || ''),String(deStat?.lastTimeStamp || ''))
                                    }
                                    checkinLogger.logSevere(JSON.stringify(data), taskId);
                                    conflictDataArr.push(data);
                                    this._worker.statusService.status = STATUS.DATA_CONFLICT;
                                }
                            } else if (deStat.size < myDeStat.size){

                                const localBlock = chain.getBlock(deStat.lastIndex)
                                if (localBlock.index === 0) {
                                    // When clearing historical blocks from chain, the 0 index block won't be cleared. It could be 0, 801, 802, 803 ...
                                    // And if on other node, it's not cleared yet, and the indexes are like: 0, 790, 791, ..., 800, 801 ..., it will compare the 0 block with the 800th block. 
                                    // This case should not be marked as data conflict.
                                } else {
                                    const localHash = localBlock.hash
                                    if (localHash !== deStat.lastHash) { // Compare the same index to discover conflict.
                                        const resolve=await this.autoResolveConflict(deName,{peerid}) //self is correct 
                                        if(!resolve){
                                            const msg = `Data conflict discovered. Peer: ${peerid}, Data entity: ${deName}, Block index: ${deStat.lastIndex},`
                                            +` local hash: ${localHash}, peer hash: ${deStat.lastHash}. Remote size is less than local size and local hash is not equal to peer hash`;
                                            const message = 'Data conflicts due to different size between the remote and the local';
                                            const data = {
                                                task : 'event',
                                                cmd: STATUS.DATA_CONFLICT,
                                                parames: Utils.getParamObject(message, msg, CONFLICT_TYPE.LASTHASH, peerid,this._node?.metadata,deName,String(myDeStat?.lastTimeStamp || ''),String(deStat?.lastTimeStamp || ''))
                                            }
                                            checkinLogger.logSevere(JSON.stringify(data), taskId);
                                            conflictDataArr.push(data);
                                            this._worker.statusService.status = STATUS.DATA_CONFLICT;
                                        }
                                    }
                                }
                            } else if (deStat.size > myDeStat.size){
                                largerChains.set(deName, deStat) // record the chain that is newer than local
                            }
                        }
                    }
                    if (largerChains.size > 0){
                        peerCount.set(Utils.sumArray(Array.from(largerChains.values()), 'size'), peerid) // record all the chains that are newer than local
                    }
                }
                if (peerCount.size > 0){
                    const key = Math.max(... Array.from(peerCount.keys()))
                    correctPeerid = peerCount.get(key) || ''
                }
            // }else{
            //     conflictDataArr.forEach((data: any) => {
            //         data.parames.correctPeerid = correctPeerid;
            //         this._worker.taskSender.sendEvent(data, false);
            //     })
            //     conflictDataArr = [];
            }
            if (this._worker.statusService.status === STATUS.DATA_CONFLICT){
                conflictDataArr.forEach((data: any) => {
                    data.parames.correctPeerid = !!correctPeerid ? correctPeerid : '';
                    this._worker.taskSender.sendEvent(data, data.parames.type !== CONFLICT_TYPE.PEER);
                })
            }
            if (correctPeerid !== '' && correctPeerid != undefined){ // Local chain is shorter than this "correctPeer"
                try{
                    if (this._worker.statusService.status === STATUS.DATA_CONFLICT){
                        //const conflictMsg = 'Data conflict discovered\n'
                        // conflictDataArr.forEach((data: any) => {
                        //     data.parames.correctPeerid = correctPeerid;
                        //     this._worker.taskSender.sendEvent(data, data.parames.type !== CONFLICT_TYPE.PEER);
                        // })
                        // if (this._worker._dbMeta.dataConflictPolicy === DATA_CONFLICT_POLICY.CUT_SHORTER_CHAIN){
                        //     //checkinLogger.logSevere(conflictMsg + 'As current data conflict policy, will force pull the longer chain', taskId)
                        //     //this.pullAll(correctPeerid) // In case data conflict, shall we force pull directly???
                        // } else if (this._worker._dbMeta.dataConflictPolicy === DATA_CONFLICT_POLICY.MARK_CONFLICT){
                        //     //checkinLogger.logSevere(conflictMsg + 'As current data conflict policy, will rely on administrators decision for further oprations', taskId)
                        //     // Just show the data conflict status to admin console
                        // } else {
                        //     // any other solution for data conflict?
                        // }
                    } else if (isNetInConflictStat === true){
                        // The private net is in data conflict status, local node doesn't know who is correct, so not going to pull from anyone.
                        // Already exist one peer which chain is the longest
                        // Remote peer has one which conflict status is in the second time,then mark my self as out of date? 
                        this._worker.statusService.status = STATUS.OUT_OF_SYNC
                    } else {
                        checkinLogger.logInfo('Will do pull', taskId)
                        await this.pull(correctPeerid)
                    }
                    await this.refreshPeers()
                    checkinProcess.resolve('Checkin process done')
                } catch (e) {
                    // console.error(e)
                    checkinProcess.reject(e)
                }
            } else if (mySize === maxSize){ // Local chain size is the same as max size
                for (const peerid of Array.from(nodeData.keys())){
                    const dbStat: Map<string, DEStatistics> = nodeData.get(peerid).dbStat
                    const chains: Map<string, BlockChainStat> = nodeData.get(peerid).chainsStat
                    if (!dbStat || // db statistics of the peer is not recorded for any reason
                        mySize !== Utils.sumArray(Array.from(chains.values()), 'size')){ // only compare the peers with same chain size
                        continue 
                    }
                    for (const deName of Array.from(myDbStats.keys())){
                        if (['meta_node', 'meta_chain','meta_peers', 'meta_config'].includes(deName) || deName.startsWith('meta_block_')){
                            continue
                        }
                        const chStatPeer = chains.get(deName)
                        const chStatLocal = myChainStats.get(deName)
                        if (chStatPeer && chStatLocal){
                            if (chStatPeer.size !== chStatLocal.size){ // Chain size different, avoid compare data.
                                continue
                            }
                        }
                        const peerDeStat:DEStatistics|undefined = dbStat?.get(deName)
                        const myDeStat:DEStatistics|undefined = myDbStats.get(deName)
                        if (myDeStat?.rowStatsHash === peerDeStat?.rowStatsHash ){
                            continue
                        } else {
                            checkinLogger.logSevere(`Data entity "${deName}" differs from peer ${peerid} 
                            where local rowStatsHash is ${myDeStat?.rowStatsHash}, but peer rowStatsHash is ${peerDeStat?.rowStatsHash}`, taskId)
                            const myRowsStatMap = new Map(myDeStat?.rowStats)
                            const peerRowsStatMap = new Map(peerDeStat?.rowStats)

                            let differCount = 0
                            let isConflict=false;
                            let conflictData
                            for (const rowStat of myRowsStatMap.entries()) {
                                const key = rowStat[0]
                                const val = rowStat[1]
                                const temval = peerRowsStatMap.get(key)
                                if (!temval || val !== temval){
                                    const resolve=await this.autoResolveConflict(deName,{peerid}) //local is correct
                                    if(resolve){
                                        break
                                    }else{
                                        const msg = ` Local statistics is not equal to peer statistics. Data entity: ${deName}, row key: ${key}, local statistics: ${val}, peer statistics: ${temval}`;
                                        const message = 'Data conflicts due to different statistics between the remote and the local.';
                                        conflictData= {
                                            task : 'event',
                                            cmd: STATUS.DATA_CONFLICT,
                                            parames: Utils.getParamObject(message, msg, CONFLICT_TYPE.STATISTICS, peerid,this._node?.metadata,deName,String(chStatLocal?.lastTimeStamp || ''),String(chStatPeer?.lastTimeStamp || ''))
                                        }
                                        checkinLogger.logSevere(JSON.stringify(conflictData), taskId);
                                        // this._worker.taskSender.sendEvent(data)
                                        isConflict=true;
                                        this._worker.statusService.status = STATUS.DATA_CONFLICT;
                                        if (++differCount > CONFIG.DATA_CONFLICT_PRINT_COUNT) {
                                            checkinLogger.logSevere('More differences are hidden...', taskId)
                                            break
                                        }
                                    }

                                }
                            }
                            differCount = 0
                            for (const rowStat of peerRowsStatMap.entries()) {
                                const key = rowStat[0]
                                const val = rowStat[1]
                                const myval = myRowsStatMap.get(key)
                                if(!myval){
                                    const resolve=await this.autoResolveConflict(deName,{peerid}) //peer is correct
                                    if(resolve){
                                        break
                                    }else{
                                        // const msg = `Data entity: ${deName}, row key: ${key}, local statistics: undefined, peer statistics: ${val}`;
                                        // const message = 'Data conflicts due to  undefined statistics in local';
                                        const msg = ` Local statistics is not equal to peer statistics. Data entity: ${deName}, row key: ${key}, peer statistics: ${val}, local statistics: ${myval}`;
                                        const message = 'Data conflicts due to different statistics between the remote and the local.';
                                        checkinLogger.logSevere(msg, taskId)
                                        conflictData = {
                                            task : 'event',
                                            cmd: STATUS.DATA_CONFLICT,
                                            parames: Utils.getParamObject(message, msg, CONFLICT_TYPE.STATISTICS, peerid,this._node?.metadata,deName,String(chStatLocal?.lastTimeStamp || ''),String(chStatPeer?.lastTimeStamp || ''))
                                        }
                                        checkinLogger.logSevere(JSON.stringify(conflictData), taskId);
                                        // this._worker.taskSender.sendEvent(data)
                                        isConflict=true;
                                        this._worker.statusService.status = STATUS.DATA_CONFLICT;
                                        if (++differCount > CONFIG.DATA_CONFLICT_PRINT_COUNT) {
                                            checkinLogger.logSevere('More differences are hidden...', taskId)
                                            break
                                        }
                                    }
                                    
                                }
                            }
                            if(isConflict){
                                this._worker.taskSender.sendEvent(conflictData)
                            }
                        }
                    }
                }
                checkinProcess.resolve('Checkin process done')
            } else { // Local chain is bigger than all peers
                checkinProcess.resolve('Checkin process done')
            }

            await this.worker.clearChainsHist()
        })
        checkinProcess.once('subTasksErr', () => {
            // Should set status to NOT READY
        })
        checkinProcess.on('afterResolve', () =>{
            if (![STATUS.LONELY_ISLAND, STATUS.DATA_CONFLICT, STATUS.OUT_OF_SYNC].includes(this._worker.statusService.status)){
                this._worker.statusService.status = STATUS.READY
            }
        })
        checkinProcess.prependListener('finally',() => {
            this.bookAutoCheckin()
        })
        return checkinProcess
    }

    /**
     * Books the automatic checkin process
     */
    bookAutoCheckin(interval: number = CONFIG.AUTO_CHECKIN_INTERVAL){
        if (!this.worker.workflow.existsTaskDesc('auto checkin')){ // avoid double booking auto checkin
            const autoCheckinTask = this.worker.workflow.generateTask({desc:'auto checkin'}, () => {
                this.checkin()
            })
            this.worker.workflow.bookTask(autoCheckinTask, interval)
            checkinLogger.logInfo('Auto checkin task booked', autoCheckinTask.id)
        }
    }

    cancelAutoCheckin(){
        const autoCheckinTasks = this.worker.workflow.searchTaskDesc('auto checkin')
        if (autoCheckinTasks && autoCheckinTasks.length > 0 && autoCheckinTasks[0].status < TaskStatus.EXECUTING){
            checkinLogger.logInfo('Cancelling Auto Checkin Task', autoCheckinTasks[0].id)
            autoCheckinTasks[0].destroy()
            process.nextTick(async () => { // Don't want to make cancelAutoCheckin as async, so, print status in nextTick.
                await this.showStatus(false)
            })
        }
    }

    /**
     * check whether auto resolve confict or not
     * @param metaName 
     * @param correctPeerid 
     * @returns 
     */
    async autoResolveConflict(conflictEntity: string,parameters?:object) {
        const parasObj=Object.assign({peerid:'',correctPeerid:'',type:''},parameters)
        const metaData = this._worker._dbMeta.getDataEntities().get(conflictEntity);
        if(!conflictEntity) return false
        if (metaData && (metaData.owner || this._worker._dbMeta.conflictPolicy === DATA_CONFLICT_POLICY.CUT_SHORTER_CHAIN)) {
            const preferPeerId = this.getPreferPeerId(parasObj, metaData?.owner)
            if(!preferPeerId) return false //not prefer peer
            pullLogger.logInfo(`Auto resolve Conflict,conflict Entity:${conflictEntity},self:${this._node._nodeMeta.id},
                conflictPeers:${parasObj.peerid},correctPeerid:${parasObj.correctPeerid},preferPeerId:${preferPeerId}`)
            const taskId = this.worker.genTaskIdShort()
            const preferPeer = this._nodeStateHandler.getPeerState(preferPeerId)?.peer.getPeerMeta()
            if (preferPeerId && preferPeer) {
                const resolveTask = this.worker.workflow.generateTask({ desc: 'Auto resolve conflict process' }, async () => {
                    // If self is the correct, do nothing,ignore
                    // If self if not, send AUTO_RESOLVE request task to the correct peer with payload that inlcude conflict peer id and conflict entity
                    // Then the correct peer will send pull request to the conflict peers
                    if (this._node._nodeMeta.id !== parasObj.peerid) { //can not conflict with self
                        if (preferPeerId === this._node._nodeMeta.id) { //correct is self, do nothing
                            return;
                        } else { //both are not the correct, push data from correct peer
                            const toPeers = [this._node._nodeMeta.id]
                            if (parasObj.peerid && preferPeerId != parasObj.peerid) {
                                toPeers.push(parasObj.peerid) //push to conflict peer as well
                            }
                            let pullPayload = { entities: [conflictEntity], toPeers } //self and conflict peer
                            pullLogger.logInfo(`Send autoResolve request to perfer peer ${preferPeerId} with conflict entity ${conflictEntity} and conflict peers ${toPeers}`)
                            this.sendTask(preferPeer, CHANNELS.AUTO_RESOLVE, resolveTask.id, pullPayload) // provide local chain stats to do partially pull
                        }
                    }
                }, taskId)
                resolveTask.execute()
                return true
            }
        }
        return false
    }
    
    getPreferPeerId(parasObj?:any,ownerId?:string): string{
        //whether check the status of the owner 
        pullLogger.logDebug(`Passed Correct peerId is ${parasObj?.correctPeerid} and owner Id is ${ownerId}`)
        if(ownerId /* && this._peers.get(ownerId)?.status=== NET_STATUS.PEER_CONNECTED */)  return ownerId
        let correctPeerid=parasObj && parasObj.correctPeerid
        if(correctPeerid && correctPeerid===this._node._nodeMeta.id) return correctPeerid // self always running
        if(correctPeerid && this._nodeStateHandler.peers.get(correctPeerid)?.status=== NET_STATUS.PEER_CONNECTED) return correctPeerid
        
        const activePeers:IPeerMeta[] = this._nodeStateHandler.getActivePeersAndSelf();
        if(parasObj && parasObj.type==='chain'){
            activePeers.filter((e)=>{return e.id!=this._node._nodeMeta.id})  //can not get the chain from self
        }
        if(activePeers.length==0) return ''
        activePeers.sort((a,b)=>{
            if((!Number.isNaN(Number(a.localIp)) && !Number.isNaN(Number(b.localIp)) && Number(a.localIp) < Number(b.localIp))
                || (a.localIp || '') < (b.localIp || '')
             ){
                return -1
            }else{
                return 1
            }
        })
        correctPeerid=activePeers[0]?activePeers[0].id : ''
        pullLogger.logDebug(`Preference correct peerId is ${correctPeerid}`)
        return correctPeerid
    }


    /**
     * Is local node inside a cluster or not
     * @returns 
     */
    isInCluster(){
        return this._internalDb.selectSync('meta_peers').rowCount > 1
        // return this.peers.size > 1
        // TBD, consider a more effective way?
    }

    /**
     * Lonely island means, all the peers of the cluster are disconnected.
     * @returns 
     */
    async isLonelyIsland(): Promise<boolean>{
        if (!this.isInCluster()){
            return false
        }
        await this.worker.waitLoadReady()
        let isLonelyIsland = true
        for (const ps of Array.from(this._nodeStateHandler.peers.values())){
            if (ps.status != undefined && ps.status != NET_STATUS.PEER_CONNECT_FAILED){
                isLonelyIsland = false
                break
            }
        }
        return isLonelyIsland
    }

    /**
     * 
     * @returns Summary of all chains size
     */
    getChainsSize(){
        const chains = Array.from(this._worker.chains.values())
        const size = Utils.sumArray(chains, 'size')
        return size
    }

    // setPeerState(peerId: string, peerState: IPeerState){
    //     this._peers.set(peerId, peerState)
    // }

    // get peers(){
    //     return this._peers
    // }

    // getPeerState(peerId: string){
    //     const peer = this.peers.get(peerId)
    //     return peer
    // }

    /**
     * Start local node and checkin to private network
     */
    async start(){
        await this.launchNode()
        if (this.isInCluster()){ // avoid checkin if not in cluster
            clusterLogger.logInfo('local node started, starting checkin');
            await this.checkin(true)
        } else {
            await this.worker.clearChainsHist() // clears chain history while startup.
            this._worker.statusService.status = STATUS.READY
        }
    }

    /**
     * Sends message to a specific peer and channel
     * @param peer 
     * @param channel 
     * @param msg 
     * @returns 
     */
    async sendMsg(peer: IPeerMeta, channel: string, msg: string, isNotZipped = false){
        return await this._node.getMsgHandler().sendMsgToPeer(peer, channel, msg, isNotZipped)
    }
    
    /**
     * BZ-15355, sync file for cluster
     * create read scream and sent file content 
     */
    public async runFileSyncTask(task: FileSyncTask) {
        const peerId = task.peerId;
        const fsm = task.fsm;
        if ( (!peerId) || (!fsm) || (!fsm.type) || (!fsm.fsi)
          || (!FileSync.isFileSyncMsg(fsm))
          || ((fsm.type === FileSyncMsgType.FST_SEND_FILE) && !FileSync.isFileSyncInfo(fsm.fsi))) {
            const err = 'invalid input parameters';
            clusterLogger.logWarn(`FileSync::runTask, '${err}'`);
            return;
        }
        if ( FileSyncMsgType.FST_SEND_FILE === fsm.type) {
            clusterLogger.logInfo(`FileSync::runTask(FST_SEND_FILE), '${fsm.taskId}', '${fsm.fsi.relative_path}'`);
            return await this._node.getMsgHandler().sendFileToPeer(peerId, fsm.taskId, fsm.fsi);
        } else if (FileSyncMsgType.FST_PULL_DONE) {
            clusterLogger.logInfo(`FileSync::runTask(FST_PULL_DONE), '${fsm.taskId}'`);
            const peerInfo = this._nodeStateHandler.getPeerState(peerId)?.peer.getPeerMeta()
            if (!peerInfo ){
                clusterLogger.logInfo(`FileSync:runTask, peer not found ${peerId}`);
                return;
            }
            return await this.sendTask(peerInfo, CHANNELS.FILE_TRANSFER_PULL_DONE, fsm.taskId, {});
        } else {
            return;
        }
    }

    get worker(){
        return this._worker
    }

    // /**
    //  * deprecated
    //  * @returns All the active peers
    //  */
    // getActivePeers(): IPeerMeta[] {
    //     const peers:any[] = []
    //     this._peers.forEach((val) => {
    //         if (val.status === NET_STATUS.PEER_CONNECTED){
    //             peers.push(val.peer.getPeerMeta())
    //         }
    //     })
    //     return peers
    // }

    /**
     * Sets addressbook and peer state for all the peers in meta_peers
     */
    // async setAddressBook(){
    //     const peersRecord = this._internalDb.selectSync('meta_peers')
    //     if (peersRecord && peersRecord.rowCount > 1){
    //         for await (let peer of peersRecord.data) {
    //             if (this._nodeStateHandler.getPeerState(peer.id)) continue
    //             const peerInst = await NodeMetadata.fromJSON(peer)
    //             await this.addAddressBook(peerInst)
    //             this._nodeStateHandler.setPeerState(peerInst.id, {
    //                 id: peerInst.id,
    //                 peer: peerInst
    //             })
    //         }
    //     }
    // }

    /**
     * Dial all the peers
     */
    async dialAll(){ // is there any problem?
        for (const val of Array.from(this._nodeStateHandler.peers.values())){
            const peerInst = val.peer
            if (peerInst.id === this._node.getPeerInfo().id){ // avoid dial self
                continue
            }
            // if (this._node._node.connections.has(peerInst.id)){ // avoid double dialing a peer
            //     continue
            // }
            try{
                await this._node.dial(peerInst)
                const ps = this._nodeStateHandler.getPeerState(peerInst.id)
                if (ps){
                  ps.status = NET_STATUS.PEER_CONNECTED
                }
            } catch (e) {
                clusterLogger.logWarn(e)
                clusterLogger.logWarn('Peer not online: ' + peerInst.id)
            }
        }
    }

    // /**
    //  * Adds a peer into address book
    //  * @param peer 
    //  * @returns 
    //  */
    // async addAddressBook(peer: NodeMetadata){
    //     const ma = await this._node._node.peerStore.addressBook.getPeerRecord(peer.peerIdInst)
    //     if (ma) return
    //     await this._node._node.peerStore.addressBook.set(peer.peerIdInst, peer.multiaddrs)
    // }

    async refreshPeers(){
        for await (const peerState of Array.from(this._nodeStateHandler.peers.values())){
            const result = this._internalDb.selectSync('meta_peers', {id: peerState.id})
            if (result.rowCount === 0){
                await this.deletePeer(peerState.peer)
            }
        }
        await this._nodeStateHandler.setAddressBook()
        await this.dialAll()
        await this.showStatus()
    }

    async deletePeer(peer: NodeMetadata){
        if (peer === undefined) {
           // console.log(111111)
        }
        // await this.worker.delete('meta_peers', {id: peer.id}) // This shouldn't happen here
        const ma = await this._node._node.peerStore.addressBook.getPeerRecord(peer.peerIdInst)
        if (ma){
            await this._node._node.hangUp(peer.peerIdInst) // Disconnect the peer
            await this._node._node.peerStore.delete(peer.peerIdInst) // Delete the peer from libp2p peerStore
        }
        if (this._nodeStateHandler.peers.has(peer.id)){
            this._nodeStateHandler.peers.delete(peer.id) // Delete the peer from peer states
        }
        if (this._worker.statusService.status === STATUS.LONELY_ISLAND && ! (await this.isLonelyIsland())){
            this._worker.statusService.status = STATUS.READY
        }
    }
}

export{
    PrivateNet, NodeStateHandler
}
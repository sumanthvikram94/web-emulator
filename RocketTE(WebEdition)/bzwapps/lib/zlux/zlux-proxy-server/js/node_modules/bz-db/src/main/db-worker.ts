/**
 * Main entry for bz-db functions. This worker could run in seperate thread or in the main thread.
 */

/*
 * bcrypto library uses NODE_BACKEND to decide whether to use "js" or "native" libraries. "native" is OS dependent, we should use "js".
 */
process.env.NODE_BACKEND = 'js'


/*
 * globalThis doesn't exist in older version node.js (v10), so, we need play a trick here to compate different node versions
 * When worker thread enabled, db-worker will run in seperate thread, and these tricks are required.
 */
// declare var globalThis: NodeJS.Global;
// declare var global: {
//     globalThis: any,
//     AbortController:any,
//     TextDecoder:any,
//     TextEncoder:any
// };
// if ( typeof(globalThis) === 'undefined'){
//     console.log('Running in node.js v10 or lower')
//     global.globalThis = global
// }

/*
 * Dependent package timeout-abort-controller has a logic to get it's parent class AbortController class, 
 * But the logic is based on the self object, and self is different in different version of node.js
 */
// const { AbortController } = require('abort-controller')
// declare var self:{
//     AbortController:any
// }
// if (typeof(self) !== 'undefined'){
//     self.AbortController = AbortController
// }
// if (typeof(global) !== 'undefined'){
//     global.AbortController = AbortController
// }
// if (!global.TextDecoder){ // Polifill the TextDecode/TextEncoder for node.js v10
//     const util= require('util');
//     global.TextDecoder = util.TextDecoder;
//     global.TextEncoder = util.TextEncoder;
// }

// eslint-disable-next-line @typescript-eslint/ban-ts-comment
// @ts-ignore
// import { isWorkerRuntime } from "threads"
// @ts-ignore
import { expose, isWorkerRuntime } from 'threads/worker'
// @ts-ignore
import { Observable } from 'threads/observable'

import { DatabaseInterface, BatchTxnData, InternalDatabaseInterface } from './db-interface.js'
import { BlueZoneDatabase } from '../internal-db/bluezone-database.js'
import { DatabaseMetadata, DataEntityMetadata, PersistMethod, PersistType, SchemaType, SyncMode } from './metadata.js'
// import { ExternalDatabase } from '../external-db/external-database.interface'
import { PrivateNet } from '../distribution/private-net.js'
import { ITaskCreationOptions, Task, Workflow } from '../workflow/index.js'
import { BlockChain, BlockChainStat } from '../blockchain/blockchain.js'
import { Transaction } from '../blockchain/transaction.js'
import { Block } from '../blockchain/block.js'
import { Utils, Status, uid, elu, enableElu, yaml, loggers, LoggerConfigurer, Logger, timeTracer, ILoggerConfig } from '../services/index.js'
import { CHANNELS } from '../constants/channels.js'
import { CONFIG } from '../constants/config.js'
import { STATUS, CONFLICT_TYPE, CONFLICT_STATUS } from '../constants/status.js'
import { VERSION } from '../constants/version.js'
import { ACTION, NET_STATUS } from '../constants/actions.js'
import { SelectConstraints, BulkLoadConstraints } from './metadata.js'
import { IPeerMeta, NodeMetadata } from '../distribution/node-metadata.js'
import { TransactionError } from '../blockchain/error.js'
import { BatchTxnValidation } from '../distribution/node-msg-handler.js'
import { DEPartitions, DEStatistics } from '../internal-db/data-entity.js'
import { FileSyncInfo, FileSyncMsg, FileSync } from './file-sync.js'  // BZ-15355, sync file for cluster
import util from 'util'
import fse from 'fs-extra'
import path from 'path'
import _ from 'lodash'

const txnLogger = loggers.txn
const checkinLogger = loggers.checkin
const performanceLogger = loggers.performance
const clusterLogger = loggers.cluster
const pullLogger = loggers.pull

//const ENTITY_FILE_SYNC :string = '_fileSync4Clustor';  // BZ-15355, sync file for cluster

class DbWorker implements DatabaseInterface{
    [key: string]: any; // This is required when the class is used as a type
    _dbMeta: DatabaseMetadata
    _metaNames: string[]
    _internalDb : InternalDatabaseInterface
    _externalDb : InternalDatabaseInterface // for LDAP MSSQL or any other
    _pNet: PrivateNet
    _workflow: Workflow
    _workflow_exec: Workflow
    _chains: Map<string, BlockChain>
    _fileSync: FileSync  // BZ-15355, sync file for cluster
    // _dataEntityWorkflows: Map<string, Workflow>
    _status: Status
    _regCmd: Map<string, Function>
    _txnRedundent: number
    _executingBlocks: Map<string, any>
    _executingStats: Map<string, any>
    _taskSender: TaskSender
    _conflictManager: ConflictManager

    constructor(){
        this._status = new Status()
        this._regCmd = new Map()
        this._txnRedundent = CONFIG.MIN_TXN_REDUNDENT
        this._executingBlocks = new Map()
        this._executingStats = new Map()
        this._taskSender = new TaskSender(this)
        this._conflictManager = new ConflictManager(this)
    }

    get taskSender (): TaskSender {
        return this._taskSender
    }

    get statusService(): Status{
        return this._status
    }

    get appName(): string {
        return this._dbMeta.getMetaData().appName || 'Anonymous application'
    }

    get hostSpicificIp(): string {
        return this._dbMeta.getMetaData().hostSpicificIp
    }

    configLogging(logging: any) {
        if (logging && logging.logLevels !== undefined){
            if (logging.logLevels.default) {
                loggers.default.setLogLevel(logging.logLevels.default)
            }
            if (logging.logLevels.txn) {
                loggers.txn.setLogLevel(logging.logLevels.txn)
            }
            if (logging.logLevels.cluster) {
                loggers.cluster.setLogLevel(logging.logLevels.cluster)
            }
            if (logging.logLevels.metadata) {
                loggers.metadata.setLogLevel(logging.logLevels.metadata)
            }
            if (logging.logLevels.performance) {
                loggers.performance.setLogLevel(logging.logLevels.performance)
            }
        }
        if (isWorkerRuntime() && logging && logging.type === 'file') {
            LoggerConfigurer.setWriter(logging) // The console.log inside worker could get lost. So, writing into file directly. Node.js issue: https://github.com/nodejs/node/issues/30491
        }
    }

    private initProperties(dbMeta: object){
        const metadata = new DatabaseMetadata(dbMeta)
        this.configLogging(metadata.logging)
        Logger.logInfo('DBWorker PID: ' + process.pid)
        enableElu()
        this._chains = new Map()
        this._workflow = new Workflow()
        this._workflow_exec =  new Workflow()
        // this._dataEntityWorkflows = new Map()
        Logger.logInfo('Starting db worker...')
        
        this._dbMeta = this.addNetMeta(metadata)

        // Array.from(this._dbMeta.getDataEntities().keys()).forEach( deName => {
        //     this._dataEntityWorkflows.set(deName, new Workflow())
        // })
        let dbName = 'dbMain';
        if (isWorkerRuntime()){
            dbName = 'dbWorker'
        }
        this._internalDb = new BlueZoneDatabase(this._dbMeta, dbName)
        this._pNet = new PrivateNet(this)
        const runTask = this._pNet.runFileSyncTask.bind(this._pNet);  // BZ-15355, sync file for cluster
        this._fileSync = new FileSync(metadata.getStorePath(), runTask)  // BZ-15355, sync file for cluster
        // TBD, here should create the external db according to dbMeta
    }

    // public getDataEntityWorkflow(deName: string): Workflow{
    //     const w = this._dataEntityWorkflows.get(deName)
    //     if (!w){
    //         throw Error('Data entity not exist')
    //     }
    //     return w
    // }

    /**
     * Starts the db worker
     * @param dbMeta 
     * @returns 
     */
    async start(dbMeta: object){
        try{
            this.initProperties(dbMeta)
            await this._internalDb.waitLoadReady()
            this.taskSender.sendEvent({
                task : 'event',
                cmd: 'internalDbReady',
                parames: {
                    status: 'ready'
                }
            })
            await this.loadExtendEntity()
			if (this._dbMeta.cluster.enabled === true) {
				await this.loadChains()
				await this.joinPNet()
                await this.waitCluster()
			} else {
				this.statusService.status = STATUS.READY // In cluster mode, this is done after checkin. It is required here for singleton.
			}
            await this.taskSender.waitForObserverSetting()
            this.statusService.markLoadReady()
            this._conflictManager.clearDataConflict()
            this.getStatusObservable().subscribe((status: any) => {
                if(status === STATUS.LONELY_ISLAND || status === STATUS.READY){
                    this._conflictManager.updateDataConflictStatus()
                }
            })
            return 'DB worker started'
        } catch(e){
            Logger.logError(e)
            throw e
        }
    }

    getStatusObservable() {
        return new Observable((observer: any) => {
            this.statusService.setObserver(observer)
        })
    }

    
    /**
     * Stops the node listener
     */
    async stop(){
        Logger.logWarn('Stopping DB worker')
        if (this._pNet && this._pNet._node && this._pNet._node._node){
            const node = this._pNet._node._node
            for (let peerState of Array.from(this._pNet._nodeStateHandler.peers.values())){
                node.hangUp(peerState.peer.peerIdInst) // Disconnect the peer
                node.peerStore.delete(peerState.peer.peerIdInst) // Delete the peer from libp2p peerStore
            }
            node.stop() // changed not to await for the libp2p node stop. The stop() function is async, but bzdb is compiled to es2015, yield async function will hangs there...
            // libp2p already has stop logics, so below lines are not required.
            // await node.addressManager.removeAllListeners()
            // await node.transportManager.close()
            // await node.transportManager.removeAll()
        }

        process.stdin.removeAllListeners()
        this.workflow.destroy()
        this.statusService.markLoadReady()
    }

    get workflow(){
        return this._workflow
    }
    
    get workflowExec(){
        return this._workflow_exec
    }

    private addNetMeta(dbMeta: DatabaseMetadata){
        this._metaNames = ['meta_node', 'meta_chain', 'meta_config', 'internal_de_lock'] //, 'meta_peers' peers should be in chain
        dbMeta.setDataEntity(new DataEntityMetadata({
            name: 'meta_node',
            primaryKeys: ['localAddrs'],
            persistMethod: PersistMethod.PERSIST_METHOD_LIST_FILE,
            filePath: '_metadata/node',
            fileName: 'node.json',
            isInternal: true
        }))

        dbMeta.setDataEntity(new DataEntityMetadata({
            name: 'meta_config',
            primaryKeys: ['value'],
            persistMethod: PersistMethod.PERSIST_METHOD_LIST_FILE,
            filePath: '_metadata/config',
            fileName: 'config.json',
            isInternal: true
        }))
        
        dbMeta.setDataEntity(new DataEntityMetadata({
            name: 'meta_peers',
            primaryKeys: ['id'],
            persistMethod: PersistMethod.PERSIST_METHOD_LIST_FILE,
            filePath: '_metadata/peers',
            isInternal: true
        }))
        
        dbMeta.setDataEntity(new DataEntityMetadata({
            name: 'meta_chain',
            primaryKeys: ['subject'],
            persistMethod: PersistMethod.PERSIST_METHOD_LIST_FILE,
            filePath: '_metadata/chain',
            rejectPKonUpdate: false,
            isInternal: true
        }))

        dbMeta.setDataEntity(new DataEntityMetadata({
            name: 'data_conflict',
            primaryKeys: ['dataEntity', 'type', 'remotePeerId', 'date'],
            persistMethod: PersistMethod.PERSIST_METHOD_LIST_FILE,
            filePath: '_internal/conflict',
            rejectPKonUpdate: false,
            syncFile4Cluster: false,
            isInternal: true,
            syncMode: SyncMode.SYNC_LOCAL
        }))

        /*// BZ-15355, support File Sync
        dbMeta.setDataEntity(new DataEntityMetadata({
            name: ENTITY_FILE_SYNC,
            primaryKeys: ['relative_path'],
            syncFile4Cluster: true
        }))*/
        if(dbMeta.getMetaData().isSingleton === undefined || dbMeta.getMetaData().isSingleton === false){
            dbMeta.getDataEntities().forEach((de) => {
                const deName = de.name
                if (this._metaNames.includes(deName) || de.persistMethod === PersistMethod.PERSIST_METHOD_ONLY_MEMORY) return // create the block de only there is block for the de
                const blockDEName = 'meta_block_' + deName
                this._metaNames.push(blockDEName)
                dbMeta.setDataEntity(new DataEntityMetadata({
                    name: blockDEName,
                    primaryKeys: ['index'],
                    filePath: '_metadata/block/'+blockDEName,
                    rejectPKonUpdate: false,
                    // dataStrategy: DataStrategy.NO_CACHE,
                    partitions: {
                        index: 200
                    },
                    isInternal: true
                }))
            })
        }
        return dbMeta
    }


    private async loadExtendEntity(){
        const entendEntity=new DataEntityMetadata(CONFIG.EXTEND_ENTITY_META)
        //this._metaNames.push(CONFIG.EXTEND_ENTITY)
        const meta=this._dbMeta.getMetaData().dataEntities.filter((obj:any)=>{return obj.name===entendEntity.name})[0];
        if(!meta){ //not exist
            this._dbMeta.getMetaData().dataEntities.push(entendEntity)
        }
        
        if(!this._dbMeta.getDataEntities().get(CONFIG.EXTEND_ENTITY)){ //not exist
            await this._internalDb._loadEntity(CONFIG.EXTEND_ENTITY_META);  //append new entity
        }
        if(this._dbMeta.getMetaData().isSingleton === undefined || this._dbMeta.getMetaData().isSingleton === false){
            await this._setBlockEntity(CONFIG.EXTEND_ENTITY_META);
        }

        //get all the entities
        const entityMetas = await this._internalDb.select(CONFIG.EXTEND_ENTITY);
        if (entityMetas.rowCount > 0 && entityMetas.data.length>0){
            for await (let entity of entityMetas.data){
                const dMetaData = new DataEntityMetadata(entity);
                if(dMetaData){
                    if (this._metaNames.includes(dMetaData.name) || dMetaData.persistMethod === PersistMethod.PERSIST_METHOD_ONLY_MEMORY) return // create the block de only there is block for the de
                    
                    const meta=this._dbMeta.getMetaData().dataEntities.filter((obj:any)=>{return obj.name===dMetaData.name})[0];
                    if(!meta){ //not exist
                        this._dbMeta.getMetaData().dataEntities.push(dMetaData)
                    }
                    if(!this._dbMeta.getDataEntities().get(entity.name)){ //not exist
                        await this._internalDb._loadEntity(entity);  //append new entity
                    }
                    await this._setBlockEntity(entity);
                }
            
            }
        }
        Logger.logInfo(`load ${entityMetas.rowCount} extend entities `)
    }

    private async _setBlockEntity(dMetaData:any){
        const blockDEName = 'meta_block_' + dMetaData.name
        
        const blockDENameMeta=new DataEntityMetadata({
            name: blockDEName,
            primaryKeys: ['index'],
            filePath: '_metadata/block/'+blockDEName,
            rejectPKonUpdate: false,
            partitions: {
                index: 200
            },
            isInternal: true
        })

        if(!this._metaNames.includes(blockDEName)){
            this._metaNames.push(blockDEName)
        }
        if(!this._dbMeta.getDataEntities().has(blockDEName)){
            await this._internalDb._loadEntity(blockDENameMeta)
            this._dbMeta.getDataEntities().set(blockDEName,blockDENameMeta) 
        }
        const meta=this._dbMeta.getMetaData().dataEntities.filter((obj:any)=>{return obj.name===blockDENameMeta.name})[0];
        if(!meta){ //add block
            this._dbMeta.getMetaData().dataEntities.push(blockDENameMeta)
        }

    }

    /**
     * Parse the chains data as BlockChains, and store in memory
     */
    private async loadChains(){
        const peersCount = this._internalDb.count('meta_peers').count
        this._txnRedundent = Math.max(CONFIG.MIN_TXN_REDUNDENT, peersCount)
        const entityNames = Array.from(this._dbMeta.getDataEntities().keys())
        for await (let name of entityNames){
           await this.loadChain(name);
        }
    }


    private async loadChain(entityName: string) {
        const name = entityName
        const deMeta: any = this._dbMeta.getDataEntities().get(name) || {}
        const inMemory = (deMeta.persistMethod as PersistMethod) === PersistMethod.PERSIST_METHOD_ONLY_MEMORY
        const inLocal = (deMeta.syncMode as SyncMode) === SyncMode.SYNC_LOCAL

        if (!this._metaNames.includes(name) && !inMemory && !inLocal) {
            const blockDEName = 'meta_block_' + name
            const chainData = this._internalDb.selectSync('meta_chain', { subject: name })
            const selectOptions = { orderBy: { fields: [(row: any) => { return _.parseInt(row.index, 10) }] } }
            let blockData = (await this._internalDb.select(blockDEName, {}, selectOptions)).data // reads the blocks data
            let bc: BlockChain
            if (chainData.rowCount === 0 || blockData.length === 0) { // first time server starts up
                bc = new BlockChain({ subject: name }, this._txnRedundent) // create a new chain
                await this._internalDb.insert('meta_chain', bc.toJSONNoBlock(), {isSilent: true}) // save the chain data
                Logger.logDebug('Insert index 0 block ' + blockDEName + ' ' + JSON.stringify(bc.chain))
                const result = await this._internalDb.bulkLoad(blockDEName, bc.chain, {isSilent: true}) // save the blocks data
                if (result.status === false) {
                    throw Error('Error while create 0th block for :' + name + '. Error is: ' + result.message)
                }
            } else { // chain data already exists
                bc = new BlockChain(chainData.data[0], this._txnRedundent)
                if (bc.chain.length > 0) { // In case bc includes chain, it's the old data format. Move the blocks data into : meta_block_xxxx
                    blockData = bc.chain.concat(blockData)
                    await this._internalDb.bulkLoad(blockDEName, blockData, {isSilent: true}) // save all blocks data into meta_block_xxxx
                    await this._internalDb.updateOrInsert('meta_chain', bc.toJSONNoBlock(), {isSilent: true})
                }
                bc.chain = blockData // assign blocks data into bc.chain
            }
            const validResult = bc.selfValidate()
            if (validResult.status === false){
                await this._internalDb.updateOrInsert('meta_chain', bc.toJSONNoBlock(), {isSilent: true})
                if (validResult.cleanBlocks){
                    await this._internalDb.delete(blockDEName)
                    await this._internalDb.bulkLoad(blockDEName, validResult.cleanBlocks)
                }
            }
            this._chains.set(name, bc)
        }
    }

    /**
     * Replace the local chains with the chains provided
     * @param chains 
     * @returns 
     */
     async replaceChains(chains: Array<any>, taskId: string){
        txnLogger.logInfo('Replacing chains', taskId)
        const chainsData = Utils.arrayToChainMap(chains)
        const chainsArr = []
        for await (let subject of Array.from(chainsData.keys())){
            const obj:any = chainsData.get(subject)
            const bc = new BlockChain(obj, this._txnRedundent)
            this._chains.set(subject, bc)
            const blockDEName = 'meta_block_' + subject
            await this._internalDb.delete(blockDEName) // Replace the blocks data for the de
            await this._internalDb.bulkLoad(blockDEName, obj.chain)
            chainsArr.push(bc.toJSONNoBlock())// The chain value in meta_chain should always be []
            await this._internalDb.delete('meta_chain', {subject})
        }
        return await this._internalDb.bulkLoad('meta_chain', chainsArr)
    }

    
    /**
     * updates the txn redundent for all chains
     */
    updateBCRedundents(){
        clusterLogger.logInfo('Updating txn redundent for block chains')
        const chains = this._chains
        for (let chain of Array.from(chains.values())){
            chain.txn_redundancy_factor = this._txnRedundent
        }
    }

    /**
     * Append the missed blocks to the local chains
     * @param peer: BZ-15355, sync file for cluster
     * @param chains 
     */
    async appendChains(peer: IPeerMeta, chains: Array<any>, txnId: string){
        txnLogger.logDebug('Appending chains', txnId)
        const chainsData: Map<string, any> = Utils.arrayToMap(chains) // it's Map<string, any[]>
        for await (const subject of Array.from(chainsData.keys())){
            const chain = this.findChain(subject)
            const blocksArr = chainsData.get(subject)
            if (Array.isArray(blocksArr)){ // contains the missing blocks
                for await(const blockData of blocksArr){
                    const block = new Block(blockData)
                    await this.appendBlock(chain,block, false, txnId)
                }
            } else if (blocksArr && blocksArr['chain'] && blocksArr['data']) { // contains the whole chain and data
                const {chain, data} = blocksArr
                await this.replaceChains([[subject, chain]], txnId)
                await this.replaceData(peer, [data], txnId)
            }
        }
    }

    async execFileTransferForBlock(blocks: Block[], subject: string){
        // \/\/\/ BZ-15355, sync file for cluster
        if (blocks.length === 0){
            return
        }
        const createdByPeerId = blocks[0].createdBy
        if (createdByPeerId !== this._pNet._node.metadata.id){ // not the local node
            if (!this._pNet.getPeerConnection(createdByPeerId)){
                clusterLogger.logWarn('Peer not connected: ' + createdByPeerId)
                return
            }
            let fsiMsgArr: FileSyncMsg[] = [];
            for (const block of blocks) {
                fsiMsgArr = fsiMsgArr.concat(this.checkFileSync4Block(/*peer,*/ subject, block));
            }
            const peerState = this._pNet._nodeStateHandler.peers.get(createdByPeerId)
            const peerMeta = peerState?.peer?.getPeerMeta()
            if (peerMeta) {
                await this.sendFileSyncMsg(peerMeta, fsiMsgArr);
            }
        }
        // /\/\/\ BZ-15355, sync file for cluster
    }

    // async appendBlocks(blocks: Map<string, Block[]>, txnId: string){
        // await this.waitForStatistics()
        // const t = this.workflowExec.generateTask({desc: 'append blocks'}, async() => {
        //     txnLogger.logDebug('Appending blocks start', txnId)
        //     const res = await this.doAppendBlocks(blocks, txnId)
        //     t.resolve(res)
        //     txnLogger.logDebug('Appending blocks done', txnId)
        // })
        // // const result = await this.workflowExec.enqueueTask(t)
        // t.execute()
        // this._executingBlocks.set(t.id, t.getPromise())
        // const result = await t.getPromise()
        // this._executingBlocks.delete(t.id)
        // return result
    // }

    async waitForExecutingBlocks(){
        if (this._executingBlocks.size > 0){
            return await Promise.all(Array.from(this._executingBlocks.values()))
        }
        return true
    }

    /**
     * Append the missed blocks to the local chains
     * @param chains 
     */
    async appendBlocks(blocks: Map<string, Block[]>, txnId: string){
        txnLogger.logDebug('Appending blocks', txnId)
        const results: any[] = []
        const result = {status: true, message: '', results}
        for await (const subject of Array.from(blocks.keys())){
            const options: ITaskCreationOptions = {
                groups: ['append-block-' + subject], // So it's convenient to wait for append block tasks to finish before statistics or pulling-res
                desc: 'Append blocks for ' + subject, 
                timeoutMs: 60000
            }
            const appendBlocksTask = this.workflow.generateTask(options, async () => {
                const chain = this.findChain(subject)
                const blocksArr = blocks.get(subject)
                if (blocksArr && blocksArr.length > 0){
                    blocksArr.sort((a:any,b:any) => {
                        return a.index - b.index
                    })
                    const valid = await this.validateBlock(chain, blocksArr[0], txnId) // JERRY: It's only validating the 1st block.
                    if (valid && !valid.status) {
                        result.status = false
                        result.message += valid.message + '.'
                        result.results.push({dataEntityName: subject, action: blocksArr[0].transactions[0].type, result: valid})
                    } else {
                        try{
                            const addedBlocks = chain.addBlocks(blocksArr, false, txnId)
                            if (addedBlocks && addedBlocks.length > 0) {
                                const resultmc = await this._internalDb.updateOrInsert('meta_chain', chain.toJSONNoBlock(), {txnId})
                                if (resultmc && !resultmc.status){
                                    // chain save failed...
                                    txnLogger.logWarn(`Failed to save meta_chain. Result: ${resultmc}`, txnId)
                                }
                                const blockDEName = 'meta_block_' + chain.subject
                                for await(const block of blocksArr){
                                    if (block.index < chain.size){
                                        const resultmb = await this._internalDb.updateOrInsert(blockDEName, block.toJSON(), {txnId}) // it should be an insert, but... to avoid potential failes.
                                        if (resultmb && !resultmb.status){
                                            // blocks save failed...
                                            txnLogger.logWarn(`Failed to save ${blockDEName}. Result: ${resultmb}`, txnId)
                                            break
                                        }
                                    }
                                }
                                for await (const block of addedBlocks){
                                    let restxn: any
                                    try{
                                        const subject = chain.subject
                                        const execDb = this.isInternalDb(subject)? this._internalDb: this._externalDb
                                        restxn = await block.executeTxns(execDb)
                                        if (Utils.andArray(restxn)){
                                            txnLogger.logDebug(`Block successfully appended, index: ${block.index}, timestamp: ${block.timestamp}`, txnId)
                                        } else {
                                            txnLogger.logWarn(`Block appended with false status. Results: ${results}`, txnId)
                                        }
                                        const r = restxn[0] // each block only contain 1 txn, so this is ok.
                                        if (!r.status){
                                            result.status = false
                                            result.message += r.message + '.'
                                        }
                                        result.results.push({dataEntityName: subject, action: block.transactions[0].type, result: r})
                                    } catch(e) { // Block appended, but transaction execute failed. Chain is healthy, but data is not. So it's a data conflict
                                        txnLogger.logSevere(`Error while executing transaction for added block!\nChain info: ${chain.statistics}\nFailed block: ${JSON.stringify(block.toJSON())}`, txnId)
                                        txnLogger.logError(e, txnId)
                                        // this.statusService.status = STATUS.DATA_CONFLICT
                                        result.status = false
                                        result.message += e.message + '.'
                                        result.results.push({dataEntityName: subject, action: block.transactions[0].type, result: {status: false, message: e.message}})
                                    }
                                    await this.execFileTransferForBlock(blocksArr, chain.subject)
                                }
                            }
                        }catch(e){ // Failed when append block to chain. The block will be lost. It causes data out of sync.
                            txnLogger.logSevere(`Error while appending approved blocks!\nChain info: ${JSON.stringify(chain.statistics)}\nFailed blocks: ${util.inspect(blocksArr)}`, txnId)
                            blocksArr.forEach( b => {
                                txnLogger.logSevere(JSON.stringify(b.toJSON()), txnId)
                            })
                            txnLogger.logError(e, txnId)
                            this.statusService.status = STATUS.OUT_OF_SYNC
                            result.status = false
                            result.message += e.message + '.'
                            result.results.push({dataEntityName: subject, action: 'append block', result: {status: false, message: e.message}})
                        }
                    }
                }
            })
            await this.workflow.waitForTaskGroups(['statistics-' + subject, 'snapshot-' + subject]) // Waits until the current statistics and snapshot action to finish before adding more blocks.
            await appendBlocksTask.executeAndResolve()
        }
        return result
    }

    async clearChainsHist(){
        this.chains.forEach(async (chain) => {
            const isCleared = chain.clearHistory()
            if (isCleared) {
                await this._internalDb.update('meta_chain', {'subject': chain.subject}, chain.toJSONNoBlock())
                const mbde = 'meta_block_' + chain.subject
                await this._internalDb.delete(mbde)
                await this._internalDb.bulkLoad(mbde, chain.chain)
            }
        })
    }

    /**
     * Replace local data with the data provided
     * @param peer: BZ-15355, sync file for cluster
     * @param data 
     */
    async replaceData(peer: IPeerMeta, data: any, taskId: string){
        if (!data){
            throw Error('Empty data to repace')
        }

        data.forEach(async (de: any) => {
            const dataEntityName = de['dataEntityName']
            const rowCount = de['rowCount']
            const partitionCount = de['partitionCount']
            const data = de['data']
            const deMeta: any = this._dbMeta.getDataEntities().get(dataEntityName) || {}
            const inMemory = (deMeta.persistMethod as PersistMethod) === PersistMethod.PERSIST_METHOD_ONLY_MEMORY
            const inLocal = (deMeta.syncMode as SyncMode) === SyncMode.SYNC_LOCAL
            txnLogger.logDebug('Replacing data entity ' + dataEntityName, taskId)
            if(!inMemory && !inLocal) {
                await this._internalDb.delete(dataEntityName)
            }

            // Data entity in memory mode don't need to bulkLoad when pull or forcePull
            
            if ((rowCount > 0 || partitionCount > 0) && !inMemory && !inLocal){
                if (this.isApplyToPartition(deMeta)){
                    await this._internalDb.writePartitions(de)
                } else {
                    await this._internalDb.bulkLoad(dataEntityName, data)
                }
                const fsiMsgArr: FileSyncMsg[] = this.checkFileSync4BulkLoad(/*peer,*/ dataEntityName, data);  // BZ-15355, sync file for cluster
                await this.sendFileSyncMsg(peer, fsiMsgArr);  // BZ-15355, sync file for cluster
            }
        })
    }

    get chains(){
        return this._chains
    }

    /**
     * calculates the BlockChainStat for block chains comparison
     * @returns 
     */
    async doGetChainsStat(peerChainsStat? :Map<string, BlockChainStat>, peerId?: string, taskId?: string){
        const stats = new Map<string, BlockChainStat>()
        const chainKeyVals = Array.from(this.chains.entries())
        const result = {
            stats, isConflict:false
        }
        for (const chainKeyVal of chainKeyVals){
            // console.log('>>>>>>> chain stat for: ' + chainKeyVal[0])
            const localChainStats = chainKeyVal[1].statistics
            stats.set(chainKeyVal[0], localChainStats)
            const peerChainStat = peerChainsStat?.get(chainKeyVal[0])
            if (peerChainStat && !chainKeyVal[1].isNoConflict(peerChainStat)) {
                const resolve = await this._pNet.autoResolveConflict(chainKeyVal[0],{peerid:peerId})
                if (!resolve) {
                    const msg = `Data conflict Encountered! Block chain conflict spotted for data entity: ${chainKeyVal[0]} `
                        + `Local chain stats: ${JSON.stringify(localChainStats)}, `
                        + `Peer chain stats: ${JSON.stringify(peerChainStat)}.`;
                    checkinLogger.logSevere(msg, taskId)
                    const message = 'Block chain conflict spotted for data entity';
                    const data = {
                        task: 'event',
                        cmd: STATUS.DATA_CONFLICT,
                        parames: Utils.getParamObject(message, msg, CONFLICT_TYPE.BLOCKCHAIN, peerId, this._pNet?._node?.metadata, chainKeyVal[0], String(localChainStats?.lastTimeStamp || ''), String(peerChainStat?.lastTimeStamp || ''))
                    }
                    checkinLogger.logSevere(JSON.stringify(data), taskId);
                    this.taskSender.sendEvent(data);
                    this.statusService.status = STATUS.DATA_CONFLICT;
                    result.isConflict = true
                }
            }
        }
        return result
    }

    /**
     * 
     * @param type : 'db' or 'chain' or 'both'
     * @param peerChainsStat 
     * @returns 
     */
    async getStatistics(type = 'both', taskId: string, peerChainsStat? :Map<string, BlockChainStat>, peerId? :string){
        // await this.waitForExecutingBlocks() // wait for append block to finish
        let result:any
        checkinLogger.logInfo('Generating statistics start ' + type, taskId)
        if (type === 'chain'){
            result = {
                chain: (await this.doGetChainsStat(peerChainsStat, peerId, taskId))
            }
        } else if (type === 'both'){
            result = await this.doGetChainsAndDBStat(peerChainsStat, peerId, taskId)
        }
        checkinLogger.logInfo('Generating statistics done ' + type, taskId)
        return result
    }

    async waitForStatistics(){
        if (this._executingStats.size > 0) {
            return await Promise.all(Array.from(this._executingStats.values()))
        }
        return true
    }

    /**
     * Calculates the BlockChainStat and DBStat for data comparison
     * @returns 
     */
    async doGetChainsAndDBStat(peerChainsStat? :Map<string, BlockChainStat>, peerId?: string, taskId?: string) {
        const chainStats = new Map<string, BlockChainStat>()
        const dbStats = new Map<string, DEStatistics>()
        const result = {
            chain: {
                stats: chainStats, isConflict:false
            },
            db: dbStats
        }

        const chainKeyVals = Array.from(this.chains.entries())
        for (const chainKeyVal of chainKeyVals){
            const subject = chainKeyVal[0]
            const statTask = this.workflow.generateTask({groups: ['statistics-' + subject], desc: 'Generates statistics for ' + subject}, async () => {
                const localChainStats = chainKeyVal[1].statistics
                chainStats.set(subject, localChainStats)
                const peerChainStat = peerChainsStat?.get(subject)
                if (peerChainStat && !chainKeyVal[1].isNoConflict(peerChainStat)){
                    const resolve = await this._pNet.autoResolveConflict(chainKeyVal[0],{peerid:peerId})
                    if(!resolve){
                            const msg = `Data conflict Encountered! Block chain conflict spotted for data entity: ${subject} `
                            + `Local chain stats: ${JSON.stringify(localChainStats)}, `
                            + `Peer chain stats: ${JSON.stringify(peerChainStat)}.`;
                        checkinLogger.logSevere(msg, taskId)
                        const message = 'Block chain conflict spotted for data entity';
                        const data = {
                            task : 'event',
                            cmd: STATUS.DATA_CONFLICT,
                            parames: Utils.getParamObject(message, msg, CONFLICT_TYPE.BLOCKCHAIN, peerId,this._pNet?._node?.metadata, subject, String(localChainStats?.lastTimeStamp || ''), String(peerChainStat?.lastTimeStamp|| ''))
                        }
                        checkinLogger.logSevere(JSON.stringify(data), taskId);
                        this.taskSender.sendEvent(data);
                        this.statusService.status = STATUS.DATA_CONFLICT;
                        result.chain.isConflict = true
                    }
                }
                
                const deMeta: any = this._dbMeta.getDataEntities().get(subject) || {}
                const inMemory = (deMeta.persistMethod as PersistMethod) === PersistMethod.PERSIST_METHOD_ONLY_MEMORY
                const inLocal = (deMeta.syncMode as SyncMode) === SyncMode.SYNC_LOCAL
                if(!inMemory && !inLocal) {
                    dbStats.set(subject, await this._internalDb.analyzeDataEntity(subject))
                }
            })
            await this.workflow.waitForTaskGroups(['append-block-' + subject])
            await statTask.executeAndResolve()
        }
        return result
    }

    /**
     * calculates the BlockChainStat for block chains comparison
     * @returns 
     */
    // async doGetDBStat(){
    //     const stats = new Map<string, DEStatistics>()
    //     const deNames = Array.from(this._dbMeta.getDataEntities().keys())
    //     for (const deName of deNames){
    //         const deMeta: any = this._dbMeta.getDataEntities().get(deName) || {}
    //         const inMemory = (deMeta.persistMethod as PersistMethod) === PersistMethod.PERSIST_METHOD_ONLY_MEMORY
    //         const inLocal = (deMeta.syncMode as SyncMode) === SyncMode.SYNC_LOCAL
    //         if(!inMemory && !inLocal) {
    //             // console.log('>>>>>>> analyze data entity for: ' + deName)
    //             stats.set(deName, await this._internalDb.analyzeDataEntity(deName))
    //         }
    //     }
    //     return stats
    // }

    /**
     * Summary the total size of all chains.
     */
    get chainsSize(){
        return Utils.sumArray(Array.from(this.chains.values()), 'size')
    }

    // get status(){
    //     return this.statusService.status
    // }

    getStatus(){
        return this.statusService.status
    }

    async printClusterStatus() {
        await this._pNet.showStatus()
    }

    set status(s: string){
        this.statusService.status = s
    }
    
    findChain(subject: string): BlockChain {
        const chain = this.chains.get(subject)
        if (!chain){
            throw Error(`Invalid subject ${subject}`)
        }
        return chain
    }

    /**
     * Starts the local node and checkin
     */
    async joinPNet(){
        await this._pNet.start()
    }

    async waitCluster(){
        await this._pNet.waitCluster()
    }

    /**
     * Get whether the dataentity stores in internal DB
     * @param de 
     * @returns 
     */
    isInternalDb(de: string){
        const deMeta = this._dbMeta.getDataEntities().get(de)
        return deMeta && !deMeta.externalStorage
    }

    private isTxnApproved(txnp: Task){
        let isApproved = true
        const approvals: Map<string, any> = txnp.getProperty('approvals')
        if (approvals){
            for (let valid of (Array.from(approvals.values()))){
                if (valid && typeof(valid.isValid) === 'boolean' && valid.isValid === false){
                    isApproved = false
                    break
                }
            }
        }
        return isApproved
    }
    
    private isBatchTxnApproved(txnp: Task){
        let isApproved = true
        const approvals: Map<string, BatchTxnValidation> = txnp.getProperty('approvals')
        if (approvals){
            for (let batchApproval of (Array.from(approvals.values()))){
                if (batchApproval['isValid'] === false){
                    isApproved = false
                    break
                }
            }
        }
        return isApproved
    }

    private getRejectedChain(txnp: Task){
        const approvals: Map<string, BatchTxnValidation> = txnp.getProperty('approvals')
        const chains:BlockChain[] = []
        if (approvals){
            for (let subject of (Array.from(approvals.keys()))){
                const bt = approvals.get(subject)
                if (bt && bt['isValid'] === false && bt.inValidSubjects && bt.inValidSubjects.length > 0){
                    const chain = this.chains.get(bt.inValidSubjects[0])
                    if (chain){
                        chains.push(chain)
                    }
                }
            }
        }
        return chains
    }

    private blocksNotRejected(blocks: Block[]){
        for (const b of blocks){
            if (b.isConflictWithApproved) return false
        }
        return true
    }

    clearPendingBlockForBatchTxn(blocks: Map<string, Block[]>){
        Array.from(blocks.keys()).map( subject => {
            const blockArr = blocks.get(subject) || []
            const ch = this.findChain(subject)
            ch.clearPendingBlocks(blockArr)
        })
    }

    async createBatchTxnProcess(blocks: Map<string, Block[]>, txnId: string){
        let payload: any[] = []
        const tempBlockList: Block[] = []
        const workflow = this.workflow
        const txnp = workflow.generateTask({desc: 'BATCH_TXN process'}, () => {
            // The block index and hash should be updated on excecute, this is for the txns invoked without await.
            Array.from(blocks.keys()).map( subject => {
                const blockArr = blocks.get(subject)
                if (blockArr && blockArr.length > 0){
                    const ch = this.findChain(subject)
                    ch.updateBlockIdx(blockArr[0]) // Only the first txn should update the hash
                    for (let i = 0; i < blockArr.length; i ++){
                        const block = blockArr[i]
                        tempBlockList.push(block)
                        payload.push({
                            subject,
                            block: block.toJSON()
                        })
                    }
                }
            })
            this._pNet.broadcast(txnp, CHANNELS.BATCH_TXN, payload, workflow)
            txnLogger.logDebug('create proof of work task for batch txn', txnId)
        }, txnId)
        txnLogger.logDebug('Create batch TXN task', txnId)
        let tryTime = 0
        txnp.onAsync('subTasksClose', async () => {
            txnLogger.logDebug('subtasksClose of batch txn task', txnId)
            if (this.isBatchTxnApproved(txnp) && this.blocksNotRejected(tempBlockList)){
                const txnGoTask = workflow.generateTask({desc: 'BATCH_TXN_GO task'}, () => {
                    txnLogger.logDebug('Sending BATCH_TXN_GO for task', txnId)
                    this._pNet.broadcast(txnGoTask, CHANNELS.BATCH_TXN_GO, payload, workflow)
                }, txnId + ':txngo:' + this.genTaskIdShort())
                txnGoTask.on('subTasksClose', () => {
                    txnGoTask.resolve({status: true})
                })
                txnGoTask.execute()
                const result = await this.appendBlocks(blocks, txnId)
                this.clearPendingBlockForBatchTxn(blocks)
                await txnGoTask.getPromise()
                return txnp.resolve(result)
            } else { // The TXN is rejected by any peer
                txnLogger.logWarn('Batch TXN rejected by peer', txnId)
                if (tryTime >= CONFIG.TXN_MAX_TRY){
                    this.clearPendingBlockForBatchTxn(blocks)
                    const e = new TransactionError('Transaction exceed max try limitation')
                    return txnp.reject({status: false, message: e.message})
                }
                const rejChains = this.getRejectedChain(txnp)
                if (rejChains && rejChains.length > 0){
                    const proms = []
                    for (const rejChain of rejChains){
                        proms.push(rejChain.touched(CONFIG.TXN_WAIT_TOUCH, txnId))
                    }
                    await Promise.all(proms)
                }
                tryTime ++
                txnp.clearProperties()
                txnp.clearSubTasks()
                payload = []
                Array.from(blocks.keys()).map( subject => {
                    const blockArr = blocks.get(subject)
                    if (blockArr && blockArr.length > 0){
                        const ch = this.findChain(subject)
                        ch.updateBlockIdx(blockArr[0])
                        for (let i = 0; i < blockArr.length; i ++){
                            const block = blockArr[i]
                            txnLogger.logInfo(`Updated block for ${subject}, index: ${block.index}`, txnId)
                            payload.push({
                                subject,
                                block: block.toJSON()
                            })
                        }
                    }
                })
                await this._pNet.broadcast(txnp, CHANNELS.BATCH_TXN, payload, workflow)
            }
        })
        txnp.once('finally', () => {
            if (CONFIG.TASK_TRACK_PERF === true){
                const inspected: string = util.inspect(this._workflow.getPerfRecords())
                txnLogger.logInfo('Tasks performance records: ' + inspected, txnId)
            }
        })
        return txnp
    }

    /**
     * Creates the task for a transaction, like insert, updateOrInsert etc.
     * @param block 
     * @param chain 
     * @returns 
     */
    async createTxnProcess(block: Block, chain: BlockChain, txnId: string, neverGiveUp = false){
        const payload = {
            subject: chain.subject,
            block: ''
        }
        // const workflow = this.getDataEntityWorkflow(chain.subject)
        const workflow = this.workflow
        const meta=this._dbMeta.getDataEntities().get(chain.subject)
        const txnGo = async()=>{
            const txnGoTask = workflow.generateTask({desc: 'TXN_GO task'}, async () => {
                txnLogger.logDebug(`Sending TXN_GO for TXN task, id: ${block.timestamp}`, txnId)
                // if (process.env.DBTEST_DUPLICATED_BLOCK === 'true') { // For testing only. Comment this out.
                //     await Utils.waitTime(20000)
                // }
                this._pNet.broadcast(txnGoTask, CHANNELS.TXN_GO, payload, workflow)
            }, txnId + ':go:' + this.genTaskIdShort())
            txnGoTask.on('subTasksClose', () => {
                txnGoTask.resolve({status: true})
            })
            txnGoTask.execute()
            timeTracer.trace('start appendBlock')
            const results = await this.appendBlock(chain, block, false, txnId)
            timeTracer.trace('stop appendBlock')
            await txnGoTask.getPromise()
            return {results,txnGoTask};
        }
        txnLogger.logDebug('Create TXN task', txnId)

        if(meta?.owner){// owner entity skip the approve process
            chain.updateBlockIdx(block)
            payload.block = block.toJSON()
            chain.setApproved(block)
            const {results,txnGoTask}=await txnGo()
            txnGoTask.resolve(results[0]) // There should be only one txn per block
            return txnGoTask
        }

        const txnp = workflow.generateTask({desc: 'TXN process'}, () => {
            // The block index and hash should be updated on excecute, this is for the txns invoked without await.
            chain.updateBlockIdx(block)
            payload.block = block.toJSON()
            this._pNet.broadcast(txnp, CHANNELS.TXN, payload, workflow)
            txnLogger.logDebug(`create proof of work task for block index: ${block.index}, timestamp: ${block.timestamp}`, txnId)
        }, txnId)
        let tryTime = 0
        txnp.onAsync('subTasksClose', async () => {
            txnLogger.logDebug('subtasksClose', txnId)
            if (this.isTxnApproved(txnp) && !block.isConflictWithApproved){ // Block approved by all peers, and block is not conflict with approved block
                chain.setApproved(block)
                const {results}=await txnGo()
                return txnp.resolve(results[0]) // There should be only one txn per block
            } else { // The TXN is rejected by any peer
                txnLogger.logWarn('Local block rejected by peer', txnId)
                if ( neverGiveUp ? !neverGiveUp : (tryTime >= CONFIG.TXN_MAX_TRY)){
                    const e = new TransactionError('Transaction exceed max try limitation')
                    // Logger.logError(e)
                    chain.clearPendingBlock(block)
                    return txnp.reject({status: false, message: e.message})
                }
                await chain.touched(CONFIG.TXN_WAIT_TOUCH, txnId)
                tryTime ++
                txnp.clearProperties()
                txnp.clearSubTasks()
                chain.updateBlockIdx(block)
                await this._pNet.broadcast(txnp, CHANNELS.TXN, payload, workflow)
            }
        })
        txnp.once('finally', () => {
            if (CONFIG.TASK_TRACK_PERF === true){
                const inspected: string = util.inspect(this._workflow.getPerfRecords())
                performanceLogger.logInfo('Tasks performance records: ' + inspected)
            }
        })
        return txnp
    }

    private isWeekIntegraty(dataEntityName: string): boolean | undefined {
        const deMeta = this._dbMeta.getDataEntities().get(dataEntityName);
        return deMeta && deMeta.integratyLevel !== undefined && deMeta.integratyLevel >= 3;
    }

    /**
     * Handling a transaction, e.g. insert, updateOrInsert etc.
     * @param dataEntityName 
     * @param action 
     * @param value 
     * @param options 
     * @returns 
     */
    async handleTxn(dataEntityName: string, action: string, value: any, options: any) {
        const txnId = options.txnId
        if (!this.statusService.ready && !(dataEntityName === 'meta_peers' && action === ACTION.DB_DELETE)){  // In case database is not ready, txns should be rejected.
            if (this.isWeekIntegraty(dataEntityName) === true) {
                // Here we allow the data entities with week integraty to continue the txn even when status is data conflict or data out of date
            } else {
                return {status: false, message: 'Database is not in ready status'}
            }
        }
        if (this.workflow.isOverwhelmed()){ //  || elu.isCrazy, It seems we should still let the txn go as much as possible
            return {status: false, message: 'Database is busy'}
        }
        txnLogger.logDebug(`Handle txn for data entity: ${dataEntityName}, action: ${action}, value: ${JSON.stringify(value)}`, txnId)
        timeTracer.trace('handleTxn')
        const txn = this.createTxn(dataEntityName, action, value, options)
        const chain = this.findChain(dataEntityName)
        const block = chain.generateBlockCandidate([txn.toJSON()], this._pNet._node.metadata.id)
        txnLogger.logInfo(`Create block for ${dataEntityName}, index: ${block.index}`, txnId)
        const txnProcess = await this.createTxnProcess(block, chain, txnId, options.neverGiveUp)
        // put the block into pending block
        // put the txnProcess into await queue...
        try{
            // return await this.getDataEntityWorkflow(dataEntityName).enqueueTask(txnProcess)
            const result = await this.workflow.enqueueTask(txnProcess)
            timeTracer.trace('handleTxn')
            return result
            
        }catch(e){
            // Logger.logError(e)
            return {status: false, message: e.message}
        }
    }

    async appendLostBlocks(chain: BlockChain, block: Block, txnId: string){
        const redundentBlocks = block.getRedundentBlocks()
        txnLogger.logInfo('Trying to supplement lost blocks with redundent data brought with the received future block', txnId)
        txnLogger.logInfo(`Redundent data size: ${redundentBlocks.length}`, txnId)
        for (const redBlockData of redundentBlocks){
            const redBlock = new Block(redBlockData)
            if (chain.isFurtureBlock(redBlock)){
                txnLogger.logInfo(`Not enough redundent data brought. Index of 1st redundent block: ${redBlock.index}`, txnId)
                return {status: false, message: 'Can not append future block'}
            }
            if (chain.isExistingBlock(redBlock)){
                continue
            }
            txnLogger.logInfo(`Appending redundent block: ${redBlock.index}`, txnId)
            await this.appendBlock(chain, redBlock, false, txnId)
        }
        return {status: true}
    }

    async validateBlock(chain: BlockChain, block: Block, txnId: string){
        if (chain.isExistingBlock(block)){
            // JERRY: should add logic to handle this.
            return {status: true, message: 'Appending existing block'}
        }
        if (chain.isFurtureBlock(block)){ // It's future block, try to append the lost blocks
            txnLogger.logWarn(`Received future block for ${chain.subject}, ` +
                `\ncurrent chain size: ${chain.size}, \nreceived block: ${JSON.stringify(block.toJSON())}, ` +
                `\n last block is: ${JSON.stringify(chain.getBlock(-1))}`, txnId)
            await this.appendLostBlocks(chain, block, txnId)
        }
        if (chain.isFurtureBlock(block)){ // Its' still future block, this means, the appendLostBlocks failed
            txnLogger.logWarn(`Failed for chain supplementing`, txnId)
            this.status = STATUS.OUT_OF_SYNC
            this._pNet.checkin()
            return {status: false, message: 'Appending future block'}
        }
        if (!chain.validateBlock(block, true, undefined, txnId).isValid){
            const resolve = await this._pNet.autoResolveConflict(chain.subject,{type:"chain"})
            if(!resolve){
                const msg = `Failed appending block. Approved block is invalid! Chain info: ${chain.statistics} Failed block: ${JSON.stringify(block.toJSON())}`;
                txnLogger.logSevere(msg, txnId)
                const message = 'Data conflict due to approved block is invalid';
                const data = {
                    task : 'event',
                    cmd: STATUS.DATA_CONFLICT,
                    parames: Utils.getParamObject(message, msg, CONFLICT_TYPE.BLOCKINVALID, '',this._pNet?._node?.metadata,chain.subject, String(chain?.statistics?.lastTimeStamp || ''), String(chain?.statistics?.lastTimeStamp|| ''))
                }
                txnLogger.logSevere(JSON.stringify(data), txnId);
                this.taskSender.sendEvent(data);
                this.statusService.status = STATUS.DATA_CONFLICT;
                return {status: false, message: 'Appending invalid block'}
            }else{
                return {status: false, message: `Failed appending block, the chain of ${chain.subject} is refreshing`}
            }

        }
        return {status: true}
    }

    // async appendBlock(chain: BlockChain, block: Block, isFromPeer: boolean = false, txnId: string){
    //     await this.waitForStatistics()
    //     const t = this.workflowExec.generateTask({desc: 'append block to ' + chain.subject + ', index: ' + block.index}, async() => {
    //         txnLogger.logInfo('Appending block start', txnId)
    //         const res = await this.doAppendBlock(chain, block, isFromPeer, txnId)
    //         t.resolve(res)
    //         txnLogger.logInfo('Appending block done', txnId)
    //     })
    //     // const result = await this.workflowExec.enqueueTask(t)
    //     t.execute()
    //     this._executingBlocks.set(t.id, t.getPromise())
    //     const result = await t.getPromise()
    //     this._executingBlocks.delete(t.id)
    //     return result
    // }

    /**
     * Appends a block to chain, and do the transaction of the block
     * @param chain 
     * @param block 
     * @param isFromPeer: whether the block is sent from other peer 
     * @returns 
     */
    async appendBlock(chain: BlockChain, block: Block, isFromPeer = false, txnId: string): Promise<any []>{
        const valid = await this.validateBlock(chain, block, txnId)
        if (valid && !valid.status) {
            return [valid]
        }
        try{
            const subject = chain.subject
            const options: ITaskCreationOptions = {
                groups: ['append-block-' + subject], // So it's convenient to wait for append block tasks to finish before statistics or pulling-res
                desc: 'Append block for ' + subject, 
                timeoutMs: 60000
            }
            const appendBlockTask = this.workflow.generateTask(options, async () => {
                let results: any
                const addedBlock = chain.addBlock(block, isFromPeer, txnId)
                if (addedBlock) { // The block is appended to end or as branch.
                    const resultmc = await this._internalDb.updateOrInsert('meta_chain', chain.toJSONNoBlock(), {txnId})
                    if (resultmc && !resultmc.status){
                        // chain save failed...
                        txnLogger.logSevere(`Failed to save meta_chain. Result: ${resultmc}`, txnId)
                    }
                    const blockDEName = 'meta_block_' + chain.subject
                    const resultmb = await this._internalDb.updateOrInsert(blockDEName, addedBlock.toJSON(), {txnId}) // Write the new block or branch into DB.
                    if (resultmb && !resultmb.status){
                        // blocks save failed...
                        txnLogger.logWarn(`Failed to save ${blockDEName}. Result: ${resultmb}`, txnId)
                    }
                    try{
                        const subject = chain.subject
                        const execDb = this.isInternalDb(subject)? this._internalDb: this._externalDb
                        results = await block.executeTxns(execDb)
                        if (Utils.andArray(results)){
                            txnLogger.logDebug(`Block successfully appended, index: ${block.index}, timestamp: ${block.timestamp}`, txnId)
                        } else {
                            txnLogger.logWarn(`Block appended with false status. Results: ${results}`, txnId)
                        }
                        await this.execFileTransferForBlock([block], subject)
                    } catch(e) { // Block appended, but transaction execute failed. Chain is healthy, but data is not. So it's a data conflict
                        const resolve = await this._pNet.autoResolveConflict(chain.subject,{type:"chain"})
                        if(!resolve){
                            const msg = `Error while appending approved block! Chain info: ${chain.statistics} Failed block: ${JSON.stringify(block.toJSON())}`;
                            txnLogger.logSevere(msg, txnId)
                            txnLogger.logError(e, txnId)
                            const message = 'Data conflict due to error while appending approved block';
                            const data = {
                                task : 'event',
                                cmd: STATUS.DATA_CONFLICT,
                                
                                parames: Utils.getParamObject(message, msg, CONFLICT_TYPE.BLOCKINVALID, '',this._pNet?._node?.metadata,chain?.subject, String(chain?.statistics?.lastTimeStamp || ''), String(chain?.statistics?.lastTimeStamp|| ''))
                            }
                            txnLogger.logSevere(JSON.stringify(data), txnId);
                            this.taskSender.sendEvent(data);
                            this.statusService.status = STATUS.DATA_CONFLICT;
                            results = [{status: false, message: e.message}]
                        }else{
                            results = [{status: false, message: `Failed appending approved block, the chain of ${chain.subject} is refreshing`}]
                        }

                    }
                    return results
                }
            })
            await this.workflow.waitForTaskGroups(['statistics-' + subject, 'snapshot-' + subject]) // Waits until the current statistics and snapshot action to finish before adding more blocks.
            return await appendBlockTask.executeAndResolve()
        }catch(e){ // Failed when append block to chain. The block will be lost. It causes data out of sync.
            txnLogger.logSevere(`Error while appending approved block!\nChain info: ${JSON.stringify(chain.statistics)}\nFailed block: ${JSON.stringify(block.toJSON())}`, txnId)
            txnLogger.logError(e, txnId)
            this.statusService.status = STATUS.OUT_OF_SYNC
            return [{status: false, message: e.message}]
        }
    }
    
    /**
     * BZ-15355, sync file for cluster
     * Check if entity support file sync
     * @param subject: entity name
     */
    private isSupportFileSync(subject: string): boolean {
        if (!this._pNet.isInCluster()) {
            return false;
        }
        const deMeta = this._dbMeta.getDataEntities().get(subject);
        if (!deMeta || !deMeta.syncFile4Cluster) {
            return false;
        }
        return true;
    }

     /**
     * BZ-15355, sync file for cluster
     * Perform file sync action
     * @param peer
     * @param dataArr
     */
    public async sendFileSyncMsg(peer: IPeerMeta, fsiMsgArr: FileSyncMsg[]) {
        if (0 === fsiMsgArr.length) {
            clusterLogger.logDebug(`FileSync::sendFileSyncMsg, fsiMsgArr is empty..`);
            return;
        }
        clusterLogger.logInfo(`FileSync::sendFileSyncMsg, fsiMsgArr is ${JSON.stringify(fsiMsgArr)}`);
        let taskId = 'FSMs:|';
        for (const fsm of fsiMsgArr) {
            taskId += `${fsm.taskId}|`;
        }
        clusterLogger.logInfo(`FileSync::sendFileSyncMsg, request FST_SEND_FILE(${taskId})`, taskId);
        await this._pNet.sendTask(peer, CHANNELS.FILE_TRANSFER_REQ, taskId, fsiMsgArr);
      }

    /**
     * BZ-15355, sync file for cluster
     * Perform file sync action
     * @param peer
     * @param dataArr
     */
    private performFileSync(/*peer: NodeInfo,*/ dataArr: Array<any>, isBulkLoad : boolean = false): FileSyncMsg[] {
        const result: FileSyncMsg[] = [];
        for (const data of dataArr) {      
            if (!FileSync.isFileSyncInfo(data)) {
                continue;
            }
            const fsi: FileSyncInfo = data as FileSyncInfo;
            clusterLogger.logInfo(`FileSync::performFileSync(${isBulkLoad}), isRemove(${fsi.removed}), payload is ${JSON.stringify(fsi)}`);
            try {
                if (!fsi.removed) {
                    /* move the logic to FileSync::writeFileFromStream()
                    // delete files first
                    if (isBulkLoad) { this._fileSync.deleteFile(fsi); } */
                    const fsm: FileSyncMsg = this._fileSync.getFileSyncMsg4SendFile(fsi, isBulkLoad);
                    result.push(fsm);
                    /*Logger.logInfo(`FileSync::performFileSync, request FST_SEND_FILE(${fsm.taskId})`);
                    await this._pNet.sendTask(peer, CHANNELS.FILE_TRANSFER_REQ, fsm.taskId, fsm);*/
                } else {
                    this._fileSync.deleteFile(fsi);
                }
            } catch (e) {
                return result;
            }
        }
        return result;
    }
    
    /**
     * BZ-15355, sync file for cluster
     * check the block if we need sync files
     * @param peer
     * @param subject: entity name
     * @param block: block
     */
    public checkFileSync4Block(/*peer: NodeInfo,*/ subject: string, block: Block): FileSyncMsg[] {
        if (!this.isSupportFileSync(subject)) {
            return [];
        }
        let result: FileSyncMsg[] = [];
        clusterLogger.logInfo(`FileSync::checkFileSync4Block, file sync for entity '${subject}'`);
        const blockData = block.toJSON();
        for (const txn of blockData.transactions) {
            const txnData = txn.toJSON();
            const action = txnData.type;
            const data = txnData.data;
            if (ACTION.DB_UPDATE_OR_INSERT !== action && !FileSync.isFileSyncInfo(data)) {
                continue;
            }
            result = result.concat(this.performFileSync(/*peer,*/ [data]));
        }
        return result;
    }
    
    /**
     * BZ-15355, sync file for cluster
     * check file sync for blukLoad
     * @param peer
     * @param subject: entity name
     * @param dataArr
     */
    public checkFileSync4BulkLoad(/*peer: NodeInfo,*/ subject: string, dataArr: Array<any>): FileSyncMsg[] {
        if (!this.isSupportFileSync(subject)) {
            return [];
        }
        
        clusterLogger.logInfo(`FileSync::checkFileSync4bulkLoad, file sync for entity '${subject}'`);
        return this.performFileSync(/*peer,*/ dataArr, true);
    }

    /**
     * Expose the getFileSyncInfo function to db manager.
     * @param filePath 
     * @param isSync4Add 
     * @returns 
     */
    getFileSyncInfo(filePath: string, isSync4Add: boolean = true): FileSyncInfo {
        return this._fileSync.getFileSyncInfo(filePath, isSync4Add);
    }
    
    /**
     * BZ-15355, sync file for cluster
     * sync [add/update/delete] arbitrary file to other cluster nodes
     * @param filePath absulute file path
     */
    async testSyncArbitraryFile(filePath: string, subject: string, isAdd: boolean) {
        if (!fse.existsSync(filePath)) {
            return 'file does not exist';
        }
        if (!this.isSupportFileSync(subject)) {
            return 'not support file sync';
        }
        const stat = fse.lstatSync(filePath);
        if (stat.isFile()) {
            const fsi: FileSyncInfo = this._fileSync.getFileSyncInfo(filePath, isAdd);
            if (!isAdd) {
                this._fileSync.deleteFile(fsi);
            }
            return await this.updateOrInsert(subject, fsi);
        } else if (stat.isDirectory()) {
            const batchTxnData = [];
            const files = fse.readdirSync(filePath);
            for (const file of files) {
                const fp = path.join(filePath, file);
                if (fse.lstatSync(fp).isFile()) {
                    const fsi: FileSyncInfo = this._fileSync.getFileSyncInfo(fp, isAdd);
                    batchTxnData.push({dataEntityName: subject, action:'UPDATEORINSERT', value: fsi});
                    if (!isAdd) {
                        this._fileSync.deleteFile(fsi);
                    }
                }
            }
            return await this.batchTxn(batchTxnData);
        } else {
            return 'not file or dir';
        }
    }

    /**
     * Generates ID for transaction.
     * @returns 
     */
    public genTaskId() {
        return uid.generate(CONFIG.UID_NUMBER_STR_RADIX, 3) // ID of the process for batch txn
    }

    public genTaskIdShort() {
        return uid.generate(CONFIG.UID_NUMBER_STR_RADIX, 1) // ID of the process for batch txn
    }

    async batchTxn(txns: BatchTxnData[]){
        if (!this.statusService.ready){  // In case database is not ready, txns should be rejected.
            let isAllWeekIntegraty = true
            for (const txn of txns) {
                if (this.isWeekIntegraty(txn.dataEntityName) === true) {
                    // Only when all the txns are for week integraty data entity, the batchtxn can still go even when data conflict or out of date.
                } else {
                    isAllWeekIntegraty = false
                    break
                }
            }
            if (isAllWeekIntegraty === false) {
                return {status: false, message: 'Database is not in ready status'}
            }
        }
        if (this.workflow.isOverwhelmed()){  //  || elu.isCrazy, It seems we should still let the txn go as much as possible
            return {status: false, message: 'Database is busy'}
        }
        const txnId = this.genTaskId()
        txnLogger.logDebug('Starts batch txn', txnId)
        const batchts = Date.now()
        const competeFactor = Math.round(Math.random() * CONFIG.BLOCK_COMPETE_FACTOR_LENGTH)
        const blocks: Map<string, Block[]> = new Map()
        const results: any[] = []
        const isInCluster = this._pNet.isInCluster()  && this._dbMeta.cluster.enabled === true
        clusterLogger.logDebug('isInCluster: ' + isInCluster)
        let result = {status: true, message: '', results}
        for (const txnData of txns){
            try{
                let {dataEntityName, action, value, options} = txnData;
                if (!options){ // Incase options is not provided in data.
                    options = {}
                }
                options.txnId = txnId // Sets txnId into the txn options, so when the txn is executed, it can get the txnId.
                if (action === ACTION.DB_INSERT || action === ACTION.DB_UPDATE || action === ACTION.DB_UPDATE_OR_INSERT ){
                    this.injectTimestamp(dataEntityName, value)
                } else if (action === ACTION.DB_BULKLOAD && Array.isArray(value)){
                    value.forEach(value => {
                        this.injectTimestamp(dataEntityName, value)
                    })
                }
                txnLogger.logDebug(`Handle txn for data entity: ${dataEntityName}, action: ${action}`, txnId)
                const txn = this.createTxn(dataEntityName, action, value, options)
                txnLogger.logDebug('Txn created', txnId)
                const deMeta: any = this._dbMeta.getDataEntities().get(dataEntityName) || {};
                const inLocal = deMeta.syncMode === SyncMode.SYNC_LOCAL;
                if (!isInCluster || inLocal) {
                    const r = await txn.execute(this._internalDb)
                    if (!r.status){
                        result.status = false
                        result.message += r.message + '.'
                    }
                    result.results.push({dataEntityName, action, result: r})
                    continue
                }
                const chain = this.findChain(dataEntityName)
                const block = chain.generateBlockCandidate([txn.toJSON()], this._pNet._node.metadata.id, batchts, competeFactor)
                txnLogger.logInfo(`Batch txn block for ${dataEntityName}, index: ${block.index}`, txnId)
                let blockArr = blocks.get(dataEntityName)
                if (!blockArr) blockArr = []
                if (blockArr.length > 0){ // In case there are mutiple txns for the same data entity, the blocks index and prev hash need update
                    block.index = block.index + blockArr.length
                    block.prevHash = blockArr[blockArr.length - 1].hash
                }
                blockArr.push(block)
                blocks.set(dataEntityName, blockArr)
            }catch(e){
                continue;
            }
        }
        if (!isInCluster){
            return result
        }
        const txnProcess = await this.createBatchTxnProcess(blocks, txnId)
        try{
            return await this.workflow.enqueueTask(txnProcess)
        }catch(e){
            // Logger.logError(e)
            return {status: false, message: e.message}
        }
    }

    injectTimestamp(dataEntityName: string, value: any){
        const deMeta = this._dbMeta.getDataEntities().get(dataEntityName)
        if (deMeta && deMeta.primaryKeys && deMeta.primaryKeys.length === 0){  // No PK, the value should inlcudes a "data"
            if (deMeta.persistType !== PersistType.PERSIST_TYPE_RAW && value.data && !value.data.timestamp ){ // ignore none-JSON data
                value.data.timestamp  = Date.now()
            }
        } else if (!value.timestamp){
            value.timestamp = Date.now()
        }
    }

    async refreshDataEntity(dataEntityName: string){
        return await this._internalDb.refreshDataEntity(dataEntityName)
    }

    /**
     * API entry of DB, insert
     * @param dataEntityName 
     * @param value 
     * @returns 
     */
    async insert(dataEntityName: string, value: any, isSilent = false, taskId?: string, neverGiveUp = false){
        this.injectTimestamp(dataEntityName, value)
        const txnId = taskId? taskId: this.genTaskId()
        const options = {
            txnId,
            isSilent,
            neverGiveUp: neverGiveUp || false
        }
        txnLogger.logInfo(`Starts inserting ${dataEntityName}`, txnId)
        if (this.isLocalUpdate(dataEntityName, 'insert')) {
            return await this._internalDb.insert(dataEntityName, value, options)
        } else if (this._pNet.isInCluster() || dataEntityName === 'meta_peers'){ // When introduce the 1st node, it's not in cluster.
            return await this.handleTxn(dataEntityName, ACTION.DB_INSERT, value, options)
        } else {
            return await this._internalDb.insert(dataEntityName, value, options)
        }
    }
    
    /**
     * API entry of DB, update
     * @param dataEntityName 
     * @param filter 
     * @param value 
     * @param constraints 
     * @returns 
     */
    async update(dataEntityName: string, filter:any, value: any, constraints?: SelectConstraints, isSilent = false){
        const txnId = this.genTaskId()
        const options = {
            filter,
            constraints,
            txnId,
            isSilent
        }
        txnLogger.logInfo(`Starts updating ${dataEntityName}`, txnId)
        this.injectTimestamp(dataEntityName, value)
        if (this.isLocalUpdate(dataEntityName, 'update')) {
            return await this._internalDb.update(dataEntityName, filter, value, options)
        } else if (this._pNet.isInCluster()){
            return await this.handleTxn(dataEntityName, ACTION.DB_UPDATE, value, options)
        } else {
            return await this._internalDb.update(dataEntityName, filter, value, options)
        }
    }

    /**
     * API entry of DB, delete
     * @param dataEntityName 
     * @param filter 
     * @param constraints 
     * @returns 
     */
    async delete(dataEntityName: string, filter?: any, constraints?: SelectConstraints, isSilent = false, taskId?:string){
        const txnId = taskId? taskId: this.genTaskId()
        const options = {
            filter,
            constraints,
            txnId,
            isSilent
        }
        txnLogger.logInfo(`Starts deleting ${dataEntityName}`, txnId)
        if (this.isLocalUpdate(dataEntityName, 'delete')) {
            return await this._internalDb.delete(dataEntityName, filter, options)
        } else if (this._pNet.isInCluster()){
            return await this.handleTxn(dataEntityName, ACTION.DB_DELETE, {}, options)
        } else {
            return await this._internalDb.delete(dataEntityName, filter, options)
        }
    }

    /**
     * API entry of DB, updateOrInsert
     * @param dataEntityName 
     * @param value 
     * @returns 
     */
    async updateOrInsert(dataEntityName: any, value: any, isSilent = false){
        this.injectTimestamp(dataEntityName, value)
        const txnId = this.genTaskId()
        const options = {
            txnId,
            isSilent
        }
        txnLogger.logInfo(`Starts updateOrInsert ${dataEntityName}`, txnId)
        if (this.isLocalUpdate(dataEntityName, 'updateOrInsert')) {
            return await this._internalDb.updateOrInsert(dataEntityName, value, options)
        } else if (this._pNet.isInCluster()){
            const result = await this.handleTxn(dataEntityName, ACTION.DB_UPDATE_OR_INSERT, value, options)
            return result
        } else {
            const result =  await this._internalDb.updateOrInsert(dataEntityName, value, options)
            return result
        }
    }

    /**
     * API entry of DB, bulkLoad
     * @param dataEntityName 
     * @param values 
     * @param constraints 
     * @returns 
     */
    async bulkLoad(dataEntityName: string, values: any[], constraints?: BulkLoadConstraints, isSilent = false){
        values.forEach(value => {
            if (!value.timestamp) value.timestamp = Date.now()
        })
        const txnId = this.genTaskId()
        const options = {
            txnId,
            isSilent,
            bulkloadConstraints: constraints
        }
        txnLogger.logInfo(`Starts bulkload ${dataEntityName}`, txnId)
        if (this.isLocalUpdate(dataEntityName, 'bulkLoad')) {
            return await this._internalDb.bulkLoad(dataEntityName, values, options)
        } else if (this._pNet.isInCluster()){
            return await this.handleTxn(dataEntityName, ACTION.DB_BULKLOAD, values, options)
        } else {
            return await this._internalDb.bulkLoad(dataEntityName, values, options)
        }
    }

    /**
     * !!!!  Deprecated !!!!
     * This should be used inside BZDB only. We will not expose a sycn function for select.
     * @param dataEntityName 
     * @param filter 
     * @param options 
     * @returns 
     */
    selectSync(dataEntityName: string, filter?: Object, options?: Object){
        return this._internalDb.selectSync(dataEntityName, filter, options)
    }

    /**
     * API entry of DB, select
     * @param dataEntityName 
     * @param filter 
     * @param options 
     * @returns 
     */
    async select(dataEntityName: string, filter: Object = {}, options: any = {}){
        await this.statusService.waitLoadReady() // Avoid returning result during PULLING, CHECKIN, LOADING etc.
        const deMeta: any = this._dbMeta.getDataEntities().get(dataEntityName) || {}
        const inMemory = this.selectInMemory(dataEntityName, 'select');
        const inLocal = (deMeta.syncMode as SyncMode) === SyncMode.SYNC_LOCAL
        if ((inMemory || inLocal) && options.selectCluster) {
            // send to other peer node
            try{
                let data: any = [];
                const desc = `Select memory value: dataEntityName is ${dataEntityName} and filter is ${JSON.stringify(filter)}`
                let ct = this.workflow.searchTaskDesc(desc)
                if (ct.length > 0){ 
                    return await ct[0].getPromise() // avoid double check
                }
                const timeoutMs = 180000
                const checkTask = this.workflow.generateTask({desc, timeoutMs: timeoutMs}, async () => {
                    this._pNet.broadcast(checkTask, CHANNELS.SELECT_MEMORY, {type: 'broadcast', dataEntityName, filter, options}, this.workflow, timeoutMs)
                })
              
                const peers = this._internalDb.selectSync('meta_peers')
                const id = this._pNet._node.getPeerInfo().id
                checkTask.on('subTasksClose', async () => {
                    const statuses = checkTask.getProperty('statuses')
                    if (statuses) {
                        data = statuses.get('statuses')
                    }
                    const self = {
                        id: id,
                        status: this.statusService.status,
                        serverName: (peers.data.find((d: any) => d.id === id) || {}).serverURL,
                        inNet: peers.rowCount > 1,
                        isLocal: true,
                        data: (await this._internalDb.select(dataEntityName, filter, options))
                    }
                    data.push(self)
                    checkTask.resolve({dataEntityName, data, rowCount: data.length})
                })
                
                this.workflow.enqueueTask(checkTask)
                return await checkTask.getPromise()
            } catch(e) {
                Logger.logError(e)
                return {status: false, message: e.message}
            }
        } else if(inMemory) {
            const peers = this._internalDb.selectSync('meta_peers')
            const id = this._pNet._node?.getPeerInfo()?.id
            const self = {
                id: id || 'local', // TBD, give it an uuid?
                serverName: (peers.data.find((d: any) => d.id === id) || {}).serverURL,
                status: this.statusService.status,
                inNet: peers.rowCount > 1,
                isLocal: true,
                data: (await this._internalDb.select(dataEntityName, filter, options))
            }
            
            return {dataEntityName, data: [self], rowCount: 1}
        } else {
            return await this._internalDb.select(dataEntityName, filter, options)
        }
       
    }

    async partitions(dataEntityName: string): Promise<DEPartitions> {
        return await this._internalDb.partitions(dataEntityName)
    }

    async writePartitions(parts: DEPartitions): Promise<any> {
        return await this._internalDb.writePartitions(parts)
    }

    /**
     * API entry of DB, select
     * @param dataEntityName 
     * @returns 
     * return data with fileName
     */
    async selectNoPKData(dataEntityName: string){
        return await this._internalDb.selectNoPKData(dataEntityName)
    }

    /**
     * API entry of DB, count
     * @param dataEntityName 
     * @returns 
     * return data with fileName
     */
    async count(dataEntityName: string){
        return await this._internalDb.count(dataEntityName)
    }

    /**
     * API entry of DB, waitLoadReady.
     * Resolves a promise when the DB loading completes, includes internal DB loading, local node starting, and network checkin
     * @returns 
     */
    async waitLoadReady(){
        await this._internalDb.waitLoadReady()
        return await this.statusService.waitLoadReady()
    }

    async waitStatusReady(){
        return await this.statusService.waitStatusReady()
    }

    async getUID(radix?:number, level?:number): Promise<string>{
        return uid.generate(radix, level)
    }

    /**
     * In case you want to execute a function on other peers, you can register the function as a command, then call it with exec(cmd)
     * @param cmd 
     * @param execFunction 
     * @returns 
     */
    async registerCommand(cmd: string, execFunction: Function){
        return this._regCmd.set(cmd, execFunction)
    }

    /**
     * Executes a command on a node and return the output
     * @param cmd 
     * @param peerId 
     */
    async exec(cmd: string, parames?: any[], peerId?: string): Promise<any>{
        if (peerId && peerId !== this._pNet._node.getPeerInfo().id){
            const peerState = this._pNet._nodeStateHandler.peers.get(peerId)
            if (peerState ){
                if (peerState.status !== NET_STATUS.PEER_CONNECTED){
                    return {status: false, message: 'Peer not in active state'}
                }
                const taskId = this.genTaskIdShort()
                const execTask =  this.workflow.generateTask({desc: 'CMD task', timeoutMs: 30000}, async () => {
                    txnLogger.logDebug('Executing ' + cmd + ' to : ' + peerId, taskId)
                    return await this._pNet.sendTask(peerState.peer.getPeerMeta()!, CHANNELS.CMD, execTask.id, {cmd, parames})
                }, taskId)
                execTask.execute()
                const result =  await execTask.getPromise()
 
                return result
            }
        }
        if (isWorkerRuntime()) { // TBD, still need more work to invoke the function in main thread.
            const result = await this.taskSender.send({
                task: 'exec',
                cmd,
                parames
            })
            return result
        }
        /**
         * Built-in commands: shutdown
         */
        switch(cmd){
            case 'shutdown': {
                const msg = 'Shutdown command received, will shutdown in 5 seconds'
                Logger.logSevere(msg)
                setTimeout(async () => {
                    await Utils.shutdown(this)
                }, CONFIG.CMD_SHUTDOWN_TIMEOUT);
                return {status: true, message: msg}
            }
            default: {
                const fun = this._regCmd.get(cmd)
                if (fun) { // registered command
                    return await fun(...(parames? parames: []))
                } else { // unknown command
                    return {status: false, message: 'Unknown command'}
                }
            }
        }
    }

    /**
     * Creates the Transaction instance. This is the transaction inside Block
     * @param subject 
     * @param type 
     * @param data 
     * @param options 
     * @returns 
     */
    private createTxn(subject: string, type: string, data: any, options?: any){
        return new Transaction({subject, type, data, options})
    }

    /**
     * Check whether the data entity can use partitions for force pull
     * @param metadata 
     * @returns 
     */
    public isApplyToPartition(metadata: DataEntityMetadata) {
        return !metadata.isAllowSubFolder && !metadata.backupFilePaths && metadata.persistMethod !== PersistMethod.PERSIST_METHOD_ONLY_MEMORY && metadata.syncFile4Cluster !== true
    }

    /**
     * Returns the whole data of internalDB, purpose is to provide whole data for full pull.
     */
    async getDbSnapshot(){
        const results: any[] = []
        const entityNames = Array.from(this._dbMeta.getDataEntities().keys())
        for (let name of entityNames){
            if (!this._metaNames.includes(name)){
                const metadata=this._dbMeta.getMetaData().dataEntities.filter((obj:any)=>{return obj.name===name})[0];
                let records;
                if(metadata){
                    if (this.isApplyToPartition(metadata)){ // Use partitions as much as possible, it has better performance.
                        records = await this._internalDb.partitions(name)
                    } else if(metadata.fileName || (Array.isArray(metadata.primaryKeys) && metadata.primaryKeys.length>0)){
                        records = await this._internalDb.select(name)
                    } else {
                        records = await this._internalDb.selectNoPKData(name)
                    }
                    results.push(records)
                }
                
            }
        }
        return results
    }

    /**
     * Kick a node out of private net
     * @param peerInfo 
     * @returns 
     */
    async kickNode(peerInfo: IPeerMeta){
        if (!this._pNet._nodeStateHandler.peers.has(peerInfo.id)){ // Reject if kicking a unknown peer
            return {status: false, message: 'Target peer not in private net'}
        }
        const taskId = this.genTaskId()
        clusterLogger.logInfo(`Kicking node from private net, id: ${peerInfo.id}`, taskId)
        // TBD, send a fairwell to the node before kick
        try {
            let deleteResult = {status: true};
            let isKickingOffline = false
            const result = await this._pNet.ping(peerInfo)
            if (result && result.status === false){ // the peer to kick is offline
                isKickingOffline = true
                clusterLogger.logWarn('Kicking offline peer!', taskId)
            }

            if (peerInfo.id === this._pNet._node.getPeerInfo().id){ // kicking self out, remove all peers, add self back
                // await this.delete('meta_peers')
                // await this.insert('meta_peers', peerInfo)
                return {status: false, message: 'Can not kick self'}
            } else {
                const peer = await NodeMetadata.fromJSON(peerInfo)
                const peers = this._internalDb.selectSync('meta_peers')
               
                let sendTask: Task
                if (!isKickingOffline){ // send KICK to the peer only if it's online
                    sendTask = this.workflow.generateTask({desc: 'Kick the peer'}, async () => {
                        this._pNet.sendTask(peerInfo, CHANNELS.KICK, sendTask.id, {peers, peerInfo})
                    }, taskId + ':toBeKicked:' + this.genTaskIdShort())
                }
                const kickTask = this.workflow.generateTask({desc: 'Kick process'}, async () => {
                    const deleteResult = await this.delete('meta_peers', {id: peer.id}, undefined, false, taskId) // Record the delete into blockchain
                    if (deleteResult.status === true){
                        if (!isKickingOffline){ // send KICK to the peer only if it's online
                            kickTask.addSubTask(sendTask)
                        }
                        await this._pNet.broadcast(kickTask, CHANNELS.KICK, {peers, peerInfo}) // Broadcast the KICK
                    }
                }, taskId)

                kickTask.on('subTasksClose', () => {
                    if (!this._pNet.isInCluster()){
                        this._pNet.cancelAutoCheckin()
                        this.statusService.status = STATUS.READY
                    }
                    if (deleteResult.status === true){
                        this._txnRedundent -- // when kicked peer, it needs decrease the txn redundent.
                        this.updateBCRedundents()
                        kickTask.resolve({status: true, message: 'Kick node done'})
                    } else {
                        kickTask.resolve(deleteResult)
                    }
                })
                await kickTask.execute()
                const taskResult = await kickTask.getPromise()
                if (deleteResult.status === true){
                    // disconnect the peer. This should be below the delete, otherwise the delete could be cancelled by lonely island
                    await this._pNet.deletePeer(peer)
                }
                await this._pNet.showStatus()
                return taskResult
            }
        } catch(err) {
            clusterLogger.logError(err)
            return {status: false, message: err.message}
        }
    }

    /**
     * deleta all data in the data entities of memory only and local.
     */
    async clearLocalDE(){
        const deMetaArry = Array.from(this._dbMeta.getDataEntities().values())
        for (const deMeta of deMetaArry){
            const inMemory = (deMeta.persistMethod as PersistMethod) === PersistMethod.PERSIST_METHOD_ONLY_MEMORY
            const inLocal = (deMeta.syncMode as SyncMode) === SyncMode.SYNC_LOCAL
            if(inMemory || inLocal) {
                await this._internalDb.delete(deMeta.name)
            }
        }
    }

    /**
     * Introduce a new node into the private net
     * @param peerInfo 
     * @returns 
     */
    async introduceNode(peerInfo: IPeerMeta){
        if (elu.isCrazy && !this._dbMeta.isWorkOnBusy()) {
            return {status: false, message: 'Database is busy'}
        }
        if (this._pNet._nodeStateHandler.peers.has(peerInfo.id)){ // Reject if introducing a known peer
            return {status: false, message: 'Target peer already in private net'}
        }
        if (!this.statusService.ready || !this._pNet._node.isListening() ){  // In case database is not ready, txns should be rejected.
            return {status: false, message: 'Database is not in ready status'}
        }
        if (!peerInfo.developmentMode && (!peerInfo.version || !_.isEqual(peerInfo.version, VERSION))){
            return {status: false, message: 'Application version not compatible'}
        }
        const taskId = this.genTaskId()
        clusterLogger.logInfo(`Introducing new node to private net, id: ${JSON.stringify(peerInfo)}`, taskId)
        try{
            await this._pNet._nodeStateHandler.setAddressBook()
            let result:any
            try{
                result = await this._pNet.checkStatus(peerInfo, true, taskId + ':status:' + this.genTaskIdShort())
                if (!result) {
                    return {status: false, message: 'Failed to get peer status'}
                }
                if (!peerInfo.developmentMode && result.appName !== this.appName){
                    return {status: false, message: 'Application not compatible'}
                }
                if (result.inNet){
                    return {status: false, message: 'Introduce node failed, because the target node is in a private net'}
                }
                if (!peerInfo.developmentMode && (!result.version || VERSION.major !== result.version.major || VERSION.minor !== result.version.minor)){
                    return {status: false, message: 'Internal DB version not compatible'}
                }
                if (!this._pNet.isInCluster()){ // If this is the first time, triggers the auto checkin of local node.
                    this._pNet.bookAutoCheckin()
                }
            }catch(err){
                clusterLogger.logError(err, taskId)
                return {status: false, message: 'Failed to get peer status'}
            }
            await this.insert('meta_peers', peerInfo, false, taskId, true)
            await this._pNet._nodeStateHandler.setAddressBook()
           
            const introduceTask = this.workflow.generateTask({desc: 'Introduce process'}, async () => {
                const peer = await NodeMetadata.fromJSON(peerInfo)
                this._pNet._nodeStateHandler.setPeerState(peerInfo.id, {id: peer.id, peer, status: NET_STATUS.PEER_CONNECTED})
                const peers = this._internalDb.selectSync('meta_peers')
                const hisDbExtend=result.dbExtend  //his node extend entity
                if(hisDbExtend){
                    if(hisDbExtend.entity){
                        await this.createEntities(hisDbExtend.entity.data)  //add his extend entity into mine
                    }
                }
                const myDbExtend=await this._internalDb.select(CONFIG.EXTEND_ENTITY)// search my extend entity
                //myDbExtend.data=myDbExtend.data.filter((e:any)=>{return e.owner===this._pNet._node._meta.id})
                const myDbExtendInfo=myDbExtend.data?{"entity":myDbExtend.data} : {}
                this._pNet.broadcast(introduceTask, CHANNELS.INTRODUCE, {peers, newbie: peerInfo,dbExtend:myDbExtendInfo}) 
            }, taskId)
            introduceTask.on('subTasksClose', () => {
                if (this.statusService.status === STATUS.LONELY_ISLAND){
                    this.statusService.status = STATUS.READY
                }
                introduceTask.resolve({status: true, message: 'Introduce node done'})
                this._txnRedundent ++ // when introduced new peer, it needs increase the txn redundent.
                this.updateBCRedundents()
            })
            introduceTask.execute()
            await introduceTask.getPromise()
            await this._pNet.showStatus()
            return {status: true}
        }catch(e){
            clusterLogger.logError(e)
            const peer = await NodeMetadata.fromJSON(peerInfo)
            this._pNet.deletePeer(peer)
            return {status: false, message: e.message}
        }
    }

    async checkStatus(){
        try{
            let data: any = [];
            let ct = this.workflow.searchTaskDesc('Check status process')
            if (ct.length > 0){ 
                return await ct[0].getPromise() // avoid double check
            }
            if (this._dbMeta.cluster.enabled === false) {
                return {status: true, data: [{
                    id: 'local',
                    status: this.statusService.status,
                    inNet: false,
                    isLocal: true,
                    listening: false
                }]}
            }
            const checkTask = this.workflow.generateTask({desc: 'Check status process', timeoutMs: 60000}, async () => {
                this._pNet.broadcast(checkTask, CHANNELS.STATUS, {type: 'broadcast'}, this.workflow)
            })
            checkTask.on('subTasksClose', async () => {
                const statusesProperty = checkTask.getProperty('statuses')?.get('statuses')
                const statuses = statusesProperty? statusesProperty: []
                const peers = this._internalDb.selectSync('meta_peers').data
                const peerIds:string[] = []
                peers.forEach((p:any) => {peerIds.push(p.id)})
                const dataIds:string[] = []

                const metaDataConflict = await this.select('data_conflict',{status: CONFLICT_STATUS.OPEN},{selectCluster: true});
                const dataConflictMap: any = {};
                const dataConflictDetails: Array<any> = [];
                metaDataConflict.data.forEach((item: any) => {
                    if(item.data.status !== false){
                        dataConflictMap[item.id] = item.data;
                        if(item.data.rowCount > 0){
                            dataConflictDetails.push(...item.data.data)
                        }
                    }
                })
                let correctId = ''
                if(dataConflictDetails.length > 0){
                    correctId=this._conflictManager.getCorrectId(dataConflictDetails) || this._pNet.getPreferPeerId()
                }
                const localId = this._pNet._node.getPeerInfo().id
                dataIds.push(localId)
                data.push({ // Add local node status
                    id: localId,
                    status: this.statusService.status,
                    inNet: peers.length > 1,
                    isLocal: true,
                    correctId,
                    listening: this._pNet._node.isListening(),  // whether the cluster node started or not
                    conflictDetails: this.statusService.status === STATUS.DATA_CONFLICT ? dataConflictMap[localId]?.data : []
                })

                statuses.forEach((p:any) => {
                    if (p.id === localId) return
                    p.isLocal = false
                    p.listening = true
                    if (peerIds.includes(p.id)){ // If a peer is already kicked, drop the peer status. Otherwise, keep the status
                        dataIds.push(p.id)
                        p.conflictDetails = p.status === STATUS.DATA_CONFLICT ? dataConflictMap[p.id]?.data: [];
                        data.push(p)
                    }
                })

                const offLinePeerIds = _.difference(peerIds, dataIds) // Find offline peers
                offLinePeerIds.forEach((peerId: string) => {
                    data.push({ // Add status for offline peers
                        id: peerId,
                        status: STATUS.OFFLINE,
                        inNet: true,
                        isLocal: false,
                        listening: false,
                        conflictDetails: []
                    })
                })

                checkTask.resolve({status: true, data})
            })
            
            checkTask.execute()
            return await checkTask.getPromise()
        } catch(e) {
            clusterLogger.logError(e)
            return {status: false, message: e.message}
        }
    }

    async forcePull(peerId: string) {
        const taskId = this.genTaskIdShort()
        pullLogger.logInfo(`force Pull, id: ${peerId}`, taskId)
        try{
            if (this._status.status === STATUS.PULLING) {
                return {status: false, message: 'Can not force the peer to pull from current node while current node is in PULLING status.'};
            }
            if (elu.isCrazy && !this._dbMeta.isWorkOnBusy()) {
                return {status: false, message: 'Database is busy.'};
            }
            let data: any = [];
            const peerInfo = this._pNet._nodeStateHandler.getPeerState(peerId)?.peer.getPeerMeta()
            if (!peerInfo){  
                pullLogger.logInfo('Peer not found: ' + peerId, taskId)
                return {status: false, data, message: 'peer not find'}
            }
            const forcePullTask = this.workflow.generateTask({desc: 'force Pull process'}, () => {
                this._pNet.sendTask(peerInfo, CHANNELS.FORCEPULL_GO, forcePullTask.id, {}) 
            }, taskId)
            forcePullTask.execute()
            await forcePullTask.getPromise()
            await this.checkinAll();
            return {status: true, data, message: 'force Pull done'}
        } catch(e) {
            pullLogger.logError(e)
            return {status: false, message: e.message}
        }
    }

    async pushToPeers(entities?:Array<string>, checkInFlag?:boolean,toPeerIds?: Array<string>){
        const taskId = this.genTaskIdShort()
        const fromPeer =this._pNet._node._meta //self
        pullLogger.logInfo(`Push entities ${!entities ? 'all' : (entities || []).join()} to peers ${toPeerIds||[].join()}`, taskId)
        const pushEntitiesTask = this.workflow.generateTask({desc: 'push entity to peers'}, () => {
            this._pNet.broadcast(pushEntitiesTask, CHANNELS.PUSH_GO, { peer:fromPeer, entities}, undefined, 120000,toPeerIds) //more time to wait.
        }, taskId)

        pushEntitiesTask.onAsync('subTasksClose', async() => {
            if(checkInFlag){
                checkinLogger.logInfo(`Push task finished, do all check in `, taskId)
                await this.checkinAll();
            }
            pushEntitiesTask.resolve({status: true, message: `push done to peers`})
        })

        await pushEntitiesTask.execute()
        await pushEntitiesTask.getPromise()

    }

    async resolvePeers(correctPeerid: string) {
        pullLogger.logInfo(`Resolve all peers conflict`)
        const preferPeerId = this._pNet.getPreferPeerId({ correctPeerid })
        const taskId = this.genTaskIdShort()
        const preferPeer = this._pNet._nodeStateHandler.getPeerState(preferPeerId)?.peer.getPeerMeta()
        if (preferPeerId && preferPeer) {
            if (preferPeerId === this._pNet._node._nodeMeta.id) { //correct is self, do nothing
                await this.pushToPeers(undefined,true)
            } else {
                const resolveTask = this.workflow.generateTask({ desc: 'Resolve conflict peers' }, async () => {
                    pullLogger.logInfo(`Send resolve conflict task to perfer peer ${preferPeerId}`)
                    //await this._pNet.sendTask(preferPeer, CHANNELS.AUTO_RESOLVE, resolveTask.id, pullPayload) // provide local chain stats to do partially pull
                    let pullPayload = {checkInFlag: true}
                    this._pNet.broadcast(resolveTask, CHANNELS.AUTO_RESOLVE, pullPayload , undefined, undefined,[preferPeerId]) //more time to wait.
                }, taskId)
                resolveTask.on('subTasksClose', () => {
                    resolveTask.resolve({ status: true, message: `Resolve conflict task peers done` })
                })
                await resolveTask.execute()
            }
        }
    }



    async checkinAll(){
        const taskId = this.genTaskIdShort()
        checkinLogger.logInfo('All nodes checkin',taskId)
        const reqCheckinTask = this.workflow.generateTask({desc: 'require checkin process'}, () => {
            this._pNet.broadcast(reqCheckinTask, CHANNELS.REQ_CHECKIN, {})
        },taskId)
        reqCheckinTask.on('subTasksClose', () => {
            reqCheckinTask.resolve({status: true, message: 'require checkin done'})
        })
        await reqCheckinTask.execute()
        await reqCheckinTask.getPromise()
        setTimeout(() => { // It seems, the file writing on the other peer might have not been finished yet at this time.
            this._pNet.checkin()
        }, 10000)
    }

    async changeLogger(config: ILoggerConfig){
        return this._internalDb.changeLogger(config)
    }

    public isLocalUpdate(dataEntityName: string, action: string = ''): any {
        if (this._dbMeta.cluster.enabled === false) {
            return true
        }
        const deMeta: any = this._dbMeta.getDataEntities().get(dataEntityName) || {}
        const inMemory = (deMeta.persistMethod as PersistMethod) === PersistMethod.PERSIST_METHOD_ONLY_MEMORY
        const inLocal = (deMeta.syncMode as SyncMode) === SyncMode.SYNC_LOCAL

        if(inMemory && action !=='select' || inLocal) {
            return true
        }
        
        return false // default should sync to other peer node.
    }

    public selectInMemory(dataEntityName: string, action: string = ''): any {
        const deMeta: any = this._dbMeta.getDataEntities().get(dataEntityName) || {}
        const inMemory = (deMeta.persistMethod as PersistMethod) === PersistMethod.PERSIST_METHOD_ONLY_MEMORY

        if(inMemory && action ==='select') {
            return true
        }
        
        return false // default should sync to other peer node.
    }

    private async doCreateEntity(type:string,dMetaData:any){  
        const txnId = this.genTaskId()
        txnLogger.logInfo('Starts creating entity: ' + JSON.stringify(dMetaData)+ "SchemaType:"+type, txnId)
        const aMeta=new DataEntityMetadata(dMetaData)
        if(aMeta){
            if (!aMeta.owner) dMetaData.owner=this._pNet._node._meta.id //set the owner when create table if no owner.
            const rtn=await this._internalDb.create(type, dMetaData, {txnId})
            if(rtn.status){
                const meta=this._dbMeta.getMetaData().dataEntities.filter((obj:any)=>{return obj.name===dMetaData.name})[0];
                if(!meta){ //not exist
                    this._dbMeta.getMetaData().dataEntities.push(dMetaData)
                }
                
                if(!this._dbMeta.getDataEntities().get(dMetaData.name)){ //not exist
                    await this._internalDb._loadEntity(dMetaData);  //append new entity
                }
                await this._setBlockEntity(dMetaData)
                await this.loadChain(aMeta.name)  //append china
            }
            txnLogger.logInfo('create entity result:' + JSON.stringify(rtn), txnId)
            return rtn;
        }else{
            return Promise.resolve({status:false,message:'entity meta data is not correct'})
        }

    }
    async createEntities(dMetaDatas:Array<DataEntityMetadata>){  //for transaction 
        let promissAll: Promise<any>[] = [];
        let pushEntitiesName:Array<string>=[];
        if (Array.isArray(dMetaDatas)) {
            for await (let dMetaData of dMetaDatas) {
                promissAll.push(await this.doCreateEntity(SchemaType.PERSIST_ENTITY,dMetaData));
            }
        }

        for await (const res of promissAll){
            if (res.status && res.dataEntity) { //new created
                const existData = await this._internalDb.select(res.dataEntity)
                if (existData.rowCount || existData.length) { //exist data which may from upgrade
                    pushEntitiesName.push(res.dataEntity)
                }
            }
        }

        if(pushEntitiesName.length>0){
            pullLogger.logInfo(`Push Entities ${pushEntitiesName.join(',')} to peers since eixst data`)
            setTimeout(() => {
                this.pushToPeers(pushEntitiesName)
            }, 1000); //wait the peer create entities
        }
        return pushEntitiesName;
    }

    async create(type: string, dMetaData: any) {  //open function for outer invoke 
        if (this._pNet.isInCluster()) {
            //await this.handleTxn(CONFIG.EXTEND_ENTITY, ACTION.DB_CREATE,dMetaData)
            return Promise.resolve({ status: false, message: "create have not support in cluster mode" })
        } else {
            return this.doCreateEntity(type, dMetaData)
        }
    }
    async drop(type: string,dataEntityName: string) {  //open function for outer invoke 
        if (this._pNet.isInCluster()) { //
            //await this.handleTxn(CONFIG.EXTEND_ENTITY, ACTION.DB_CREATE,dMetaData)
            return Promise.resolve({ status: false, message: "drop have not support in cluster mode" })
        } else {
            const rtn = await this._internalDb.drop(type, dataEntityName)
            if (rtn.status) {
                //entity
                const ind1 = this._dbMeta.getMetaData().dataEntities.findIndex((obj: any) => { return obj.name === dataEntityName })
                if (ind1>-1) { //not exist
                    this._dbMeta.getMetaData().dataEntities.splice(ind1,1);
                }
                if(this._dbMeta.getDataEntities().has(dataEntityName)){
                    this._dbMeta.getDataEntities().delete(dataEntityName)
                }

                //block
                const blockDEName = 'meta_block_' + dataEntityName
        
                const ind3 =this._metaNames.findIndex((obj: any) => {return obj === blockDEName })
                if (ind3>-1){
                    this._metaNames.splice(ind3,1);
                }
                const ind4 = this._dbMeta.getMetaData().dataEntities.findIndex((obj: any) => { return obj.name === blockDEName })
                if (ind4>-1) { //not exist
                    this._dbMeta.getMetaData().dataEntities.splice(ind4,1);
                }
                if(this._dbMeta.getDataEntities().has(blockDEName)){
                    this._dbMeta.getDataEntities().delete(blockDEName)
                }

                await this._internalDb._releaseEntity(blockDEName)

                //chain
                if(this._chains.has(dataEntityName)){
                    this._chains.delete(dataEntityName)
                }
                return Promise.resolve({status:true,message:`Entity ${dataEntityName} have been drop`})
            }else{
                return rtn
            }

          
        }
    }

    async analyzeDataEntity(de: string, isIncludeRows: boolean, isDoHash: boolean) {
        return await this._internalDb.analyzeDataEntity(de, isIncludeRows, isDoHash)
    }

    async isBusy() {
        return elu.isBusy
    }
    
    async isCrazyBusy() {
        return elu.isCrazy
    }

    async checkin() {
        return this._pNet.checkin()
    }

    async getNodeAddrs() {
        return await this._pNet._node?.getAddrs()
    }

    /**
     * Expose the function yamlStrToJson
     * @param yamlStr 
     * @returns 
     */
    async yamlStrToJson(yamlStr: string) {
        if (!yamlStr || yamlStr.length === 0) {
            return {}
        }
        return yaml.yamlStrToJson(yamlStr)
    }
    
    /**
     * Expose the function jsonToYAMLStr
     * @param obj 
     * @returns 
     */
    async jsonToYAMLStr(obj: object) {
        if (!obj) {
            return ''
        }
        return yaml.jsonToYAMLStr(obj)
    }

    /**
     * Expose the parse function of yaml package. It doesn't keep comments in object.
     * @param yamlStr 
     * @returns 
     */
    async yamlParse(yamlStr: string) {
        return yaml.yamlParse(yamlStr)
    }

    /**
     * Expose the stringify function of yaml package. It doesn't convert the obj.__bzdb_yaml_metadata.comments to yaml comments
     * @param obj 
     * @returns 
     */
    async yamlStringify(obj: object) {
        return yaml.yamlStringify(obj)
    }

    /**
     * Just for testing purpose. Provide some settings through stdin.
     * @param testType 
     */
    async setTestStatus(testType: string) {
        switch (testType) {
            case 'enable-duplicated-block': 
                process.env.DBTEST_DUPLICATED_BLOCK = 'true'
                break
            case 'disable-duplicated-block': 
                process.env.DBTEST_DUPLICATED_BLOCK = ''
                break
        }
    }

    /**
     * Just for testing purpose. Invoke this to hangup (disconnect) the connection with a peer
     * @param peerId 
     */
    async hangup(peerId: string) {
        const peer = this._pNet._nodeStateHandler.getPeerState(peerId)?.peer.peerIdInst
        if (peer) {
            this._pNet._node._node.hangUp(peer)
        }
    }
}


async function startWorker(dbMeta: any): Promise<DbWorker>{
    // if (!worker) {
    //     worker = new DbWorker()
    // }
    const worker = new DbWorker()
    worker.start(dbMeta) // start the worker asynchronized.
    return worker
}

// In case working in worker thread

let worker: DbWorker

class ConflictManager {
    worker: DbWorker
    _conflictObserver: Promise<any>
    _conflictObserverResolve: Function
    constructor(worker: DbWorker) {
        this.worker = worker
        this._conflictObserver = new Promise((resolve) => {
            this._conflictObserverResolve = resolve;
        });
    }

    async waitForconflictObserverSetting() {
        await this._conflictObserver;
    }

    //save conflict data into entity data_conflict
    async saveConflictData(data: any) {
        await this.waitForconflictObserverSetting();
        const peers = this.worker._internalDb.selectSync('meta_peers').data
        const peerMap: any = {};
        peers.forEach((peer: any) => {
            peerMap[peer.id] = Utils.getServerNameFromUrl(peer.serverURL);
        })
        const conflictData = JSON.parse(JSON.stringify(data.parames));
        conflictData.date = Utils.transferTimestampToDate(conflictData.localLastBlockTime || conflictData.peerLastBlockTime);
        conflictData.remotePeer = peerMap[conflictData.remotePeerId] || ''
        if(conflictData.correctPeerid){
            conflictData.correctPeer = peerMap[conflictData.correctPeerid] || ''
        }
        conflictData.status = CONFLICT_STATUS.OPEN;
        await this.worker.updateOrInsert('data_conflict', conflictData);
        const metaDataConflict = await this.worker._internalDb.select('data_conflict','',{orderBy: {fields: ['date'], orders: ['asc']}});
        if (metaDataConflict.rowCount > CONFIG.MAXIMUM_DATA_CONFLICT_AMOUNT){
            const conflict = metaDataConflict.data[0];
            await this.worker._internalDb.delete('data_conflict', {'dataEntity':conflict.dataEntity, 'type':conflict.type, 'remotePeerId':conflict.remotePeerId, 'date':conflict.date });
        }
        
    }

    async updateDataConflictStatus(){
        const metaDataConflict = await this.worker._internalDb.select('data_conflict',{status: CONFLICT_STATUS.OPEN})
        if (metaDataConflict.rowCount > 0){
            const batchTxnData: Array<any> = [];
            metaDataConflict.data.forEach((item: any) => {
                item.status = CONFLICT_STATUS.RESOLVED;
                batchTxnData.push({dataEntityName: 'data_conflict', action: 'UPDATEORINSERT', value: item})
            });
            try {
                await this.worker.batchTxn(batchTxnData)
            }
            catch (e) {
                Logger.logError(e)
                throw e
            }
        }
    }

    async clearDataConflict(){
        await this.worker._internalDb.delete('data_conflict', {'status':CONFLICT_STATUS.OPEN});
        this._conflictObserverResolve()
    }

    getCorrectId(conflictData:any){
        let correctPeers=new Map()
        let correctPeerId=''
        if (conflictData.length > 0){
            conflictData.forEach((item: any) => {
                if(item.correctPeerid){
                    if(correctPeers.get(item.correctPeerid)){ //exist
                        correctPeers.set(item.correctPeerid,((correctPeers.get(item.correctPeerid)||0)+1)) 
                    }else{
                        correctPeers.set(item.correctPeerid,1)
                    }
                }
            });

            const maxValue = Math.max(...correctPeers.values());
            const keyArry = new Array<string>;
            correctPeers.forEach((value, key) => {
                if(value === maxValue){
                    keyArry.push(key)
                }
            }); 
            correctPeerId=keyArry.sort()[0]  
        }
        return correctPeerId;
    }
}

class TaskSender {

    observer: any
    worker: DbWorker
    tasks: Map<string, any>
    _observable: Observable<any>
    _observerProm: Promise<any>
    _observerResolve: Function

    constructor(worker: DbWorker) {
        this.worker = worker
        this.tasks = new Map()
        this._observerProm = new Promise((resolve) => {
            this._observerResolve = resolve
        })
    }

    getObservable() {
        if (this._observable) {
            return this._observable
        }
        this._observable = new Observable((observer: any) => {
            this.setObserver(observer)
        })
        return this._observable
    }

    setObserver(observer: any) {
        this.observer = observer
        this._observerResolve()
    }

    async waitForObserverSetting() {
        await this._observerProm
    }

    async send(data: any) {
        await this.waitForObserverSetting()
        const id = await this.worker.getUID()
        return new Promise((resolve) => {
            this.tasks.set(id, resolve)
            this.observer.next({
                id, data
            })
            setTimeout(() => {
                this.replyToWorker(id, {status: false, message: 'timeout'})
            }, 180000)
        })
    }

    async sendEvent(data: any, sendFlag = true){
        await this.waitForObserverSetting()
        const id = await this.worker.getUID();
        if(data.cmd === STATUS.DATA_CONFLICT){
            this.worker._conflictManager.saveConflictData(data);
        }
        if(sendFlag){
            this.observer.next({
               id, data
            })
        }
    }

    async replyToWorker(id: string, data: any) {
        const taskResove = this.tasks.get(id);
        if (taskResove) {
            this.tasks.delete(id)
            taskResove(data)
        }
    }
}

if (isWorkerRuntime()) { // && process.env.THREADS_WORKER_INIT_TIMEOUT === undefined
    console.log('Worker in multi threads mode')
    worker = new DbWorker()
    const taskSender = worker.taskSender

    function getWorkerObservable() {
        return taskSender.getObservable()
    }

    function exposeWorker () {
        const workerInst:DatabaseInterface = {
            start: worker.start.bind(worker),
            stop: worker.stop.bind(worker),
            clearChainsHist: worker.clearChainsHist.bind(worker),
            batchTxn: worker.batchTxn.bind(worker),
            refreshDataEntity: worker.refreshDataEntity.bind(worker),
            insert: worker.insert.bind(worker),
            update: worker.update.bind(worker),
            delete: worker.delete.bind(worker),
            updateOrInsert: worker.updateOrInsert.bind(worker),
            bulkLoad: worker.bulkLoad.bind(worker),
            // selectSync: worker.selectSync.bind(worker), //Deprecated
            select: worker.select.bind(worker),
            selectNoPKData: worker.select.bind(worker),
            partitions: worker.partitions.bind(worker),
            writePartitions: worker.writePartitions.bind(worker),
            count: worker.count.bind(worker),
            waitLoadReady: worker.waitLoadReady.bind(worker),
            waitStatusReady: worker.waitStatusReady.bind(worker),
            getUID: worker.getUID.bind(worker),
            registerCommand:worker.registerCommand.bind(worker),
            exec: worker.exec.bind(worker),
            getDbSnapshot: worker.getDbSnapshot.bind(worker),
            kickNode: worker.kickNode.bind(worker),
            introduceNode: worker.introduceNode.bind(worker),
            checkStatus: worker.checkStatus.bind(worker),
            forcePull: worker.forcePull.bind(worker),
            changeLogger: worker.changeLogger.bind(worker),
            create: worker.create.bind(worker),
            drop: worker.drop.bind(worker),
            getStatus: worker.getStatus.bind(worker),
            analyzeDataEntity: worker.analyzeDataEntity.bind(worker),
            isBusy: worker.isBusy.bind(worker),
            isCrazyBusy: worker.isCrazyBusy.bind(worker),
            checkin: worker.checkin.bind(worker),
            checkinAll: worker.checkinAll.bind(worker),
            pushToPeers: worker.pushToPeers.bind(worker),
            resolvePeers: worker.resolvePeers.bind(worker),
            getWorkerObservable: getWorkerObservable,
            replyToWorker: taskSender!.replyToWorker.bind(taskSender),
            testSyncArbitraryFile: worker.testSyncArbitraryFile.bind(worker),
            getFileSyncInfo: worker.getFileSyncInfo.bind(worker),
            printClusterStatus: worker.printClusterStatus.bind(worker),
            getNodeAddrs: worker.getNodeAddrs.bind(worker),
            yamlStrToJson: worker.yamlStrToJson.bind(worker),
            jsonToYAMLStr: worker.jsonToYAMLStr.bind(worker),
            yamlParse: worker.yamlParse.bind(worker),
            yamlStringify: worker.yamlStringify.bind(worker),
            //setTestStatus: worker.setTestStatus.bind(worker),
            //hangup: worker.hangup.bind(worker)
        }
        expose(workerInst)
    }
    exposeWorker()
}


export {
    DbWorker,
    startWorker
}
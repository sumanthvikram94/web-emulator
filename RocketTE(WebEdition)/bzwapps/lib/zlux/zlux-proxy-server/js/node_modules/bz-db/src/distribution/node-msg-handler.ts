import { Libp2p } from 'libp2p'
import { pipe } from 'it-pipe'
import { ITaskMessage, IPeerState, NodeMetadata, IPeerMeta } from './node-metadata.js'
import { NET_STATUS } from '../constants/actions.js'
import { CHANNELS } from '../constants/channels.js'
import { STATUS } from '../constants/status.js'
import { CONFIG } from '../constants/config.js'
import { VERSION } from '../constants/version.js'
import { AUTO_SCALE_STATUS, PrivateNet } from './private-net.js'
import { PrivateNetNode } from './private-net-node.js'
import { DbWorker } from '../main/db-worker.js'
import { Utils, loggers, timeTracer } from '../services/index.js'
import { Block } from '../blockchain/block.js'
import { MessageSender } from './message-sender.js'
import gzip_pkg from 'node-gzip' // work around for commonjs package
const { gzip, ungzip } = gzip_pkg
import { FileSyncInfo, FileSyncTask, FileSyncMsg, FileSync } from '../main/file-sync.js'  // BZ-15355, sync file for cluster
import { ClusterNodeType,DataEntityMetadata } from '../main/metadata.js'

const txnLogger = loggers.txn
const checkinLogger = loggers.checkin
const clusterLogger = loggers.cluster
const pullLogger = loggers.pull

interface BatchTxnValidation {
    isValid: boolean
    inValidSubjects: any[]
}

export interface IAutoScaleResult {
    status: boolean
    peerMeta?: IPeerMeta
    message?: string
}

/**
 * Handles tcp msg and node events
 */
class NodeMsgHandler{

    _node: Libp2p
    _pnNode: PrivateNetNode
    _worker: DbWorker
    _net: PrivateNet
    _privateChannels: string[]
    _strangerChannels: string[]
    _publicChannels: string[]
    _msgSender: MessageSender
    _textDecoder: TextDecoder

    constructor(node: PrivateNetNode, worker: DbWorker, net: PrivateNet){
        this._pnNode = node
        this._node = node._node
        this._worker = worker
        this._net = net
        this._msgSender = new MessageSender(this._node)
        this._textDecoder = new TextDecoder()
        this.ensureHandlers()
        this.listenToStdin() // for test only
    }

    private ensureHandlers(){
        this.setReceiveHandlers()
        this.setNodeEventHandlers()
    }

    get worker(){
        return this._worker
    }

    /**
     * For manually UT only. Take input into terminal, and do the functions.
     * Use TAB to seperate the commands, e.g.: 
     * select   testDE
     */
    private listenToStdin(){
        //const db = this.worker
        // process.stdin.on('data', async (data:Buffer) => {
        //     Logger.logInfo('Handle console input')
        //     const cmd = (data.toString().replace('\r\n', ''))
        //     const tokens = cmd.split('\t')
        //     Logger.logInfo(JSON.stringify(tokens))
        //     const action = tokens[0]
        //     try{
        //         if (action === 'select'){
        //             const de = tokens[1]
        //             const filter = tokens[2]
        //             const options = JSON.parse(tokens[3] || '{}')
        //             console.log(await db.select(de, filter? JSON.parse(filter): undefined, options))
        //         } else if (action === 'selectJSON'){
        //             const de = tokens[1]
        //             const filter = tokens[2]
        //             console.log(JSON.stringify(await db.select(de, filter? JSON.parse(filter): undefined)))
        //         } else if (action === 'uid'){
        //             const radix = tokens[1]? Number.parseInt(tokens[1]): undefined
        //             const level = tokens[2]? Number.parseInt(tokens[2]): undefined
        //             console.log(await db.getUID(radix, level))
        //         } else if (action === 'exec'){
        //             const cmd = tokens[1]? tokens[1]: ''
        //             const peerId = tokens[2]? tokens[2]: undefined
        //             console.log(await db.exec(cmd, [], peerId))
        //         } else if (action === 'count') {
        //             const de = tokens[1]
        //             console.log(await db.count(de))
        //         } else if (action === 'insert') {
        //             const de = tokens[1]
        //             const val = JSON.parse(tokens[2])
        //             console.log(await db.insert(de, val))
        //         } else if (action === 'update') {
        //             const de = tokens[1]
        //             const filter = JSON.parse(tokens[2])
        //             const val = JSON.parse(tokens[3])
        //             const cons = tokens.length >= 5? JSON.parse(tokens[4]): {}
        //             console.log(await db.update(de, filter, val, cons))
        //         } else if (action === 'delete') {
        //             const de = tokens[1]
        //             const filter = tokens.length >= 3?JSON.parse(tokens[2]): {}
        //             const cons = tokens.length >= 4? JSON.parse(tokens[3]): {}
        //             console.log(await db.delete(de, filter, cons))
        //         } else if (action === 'updateOrInsert') {
        //             const de = tokens[1]
        //             const val = JSON.parse(tokens[2])
        //             console.log(await db.updateOrInsert(de, val))
        //         } else if (action === 'bulkLoad') {
        //             const de = tokens[1]
        //             const val = JSON.parse(tokens[2])
        //             const cons = tokens.length >= 4? JSON.parse(tokens[3]): undefined
        //             console.log(await db.bulkLoad(de, val, cons))
        //         } else if ('fsadd' === action.toLowerCase()) {
        //             const abs_path = tokens[1].replace(/['"](.*)['"]/, '$1');
        //             // 'fileSync' is defined in test.js
        //             console.log(await this.worker.testSyncArbitraryFile(abs_path, 'fileSync', true));                    
        //         } else if ('fsdel' === action.toLowerCase()) {
        //             const abs_path = tokens[1].replace(/['"](.*)['"]/, '$1');
        //             // 'fileSync' is defined in test.js
        //             console.log(await this.worker.testSyncArbitraryFile(abs_path, 'fileSync', false));                    
        //         } else if (action === 'introduceNode'){
        //             const nodeInfo: IPeerMeta = JSON.parse(tokens[1])
        //             console.log(await this.worker.introduceNode(nodeInfo))
        //         } else if (action === 'kickNode'){
        //             const nodeInfo: IPeerMeta = JSON.parse(tokens[1])
        //             console.log(await this.worker.kickNode(nodeInfo))
        //         } else if (action === 'checkStatus'){
        //             console.log(await this.worker.checkStatus())
        //         } else if (action === 'batchTxn'){
        //             const btd:BatchTxnData[] = JSON.parse(tokens[1])
        //             const result = await this.worker.batchTxn(btd)
        //             console.log(result)
        //         }else if (action === 'snapshot'){
        //             console.log(await this.worker.getDbSnapshot()) 
        //         }else if (action === 'forcePull'){
        //             const peerId = tokens[1]
        //             console.log(await this.worker.forcePull(peerId)) 
        //         }else if (action === 'pushToPeers'){
        //             console.log(await this.worker.pushToPeers()) 
        //         }
        //         else if (action === 'status'){
        //             await this._net.showStatus()
        //         }else if (action === 'senderStatus'){
        //             this._msgSender.printSenders()
        //         }else if (action === 'clear'){
        //             await this.worker.clearChainsHist()
        //         }else if (action === 'checkin'){
        //             this._net.checkin()
        //         }else if (action === 'checkinAll'){
        //             this.worker.checkinAll()
        //         }else if (action === 'cancelAutoCheckin'){
        //             this._net.cancelAutoCheckin()
        //         }else if (action === 'analyzeDataEntity'){
        //             const de = tokens[1]
        //             const isIncludeRows = tokens[2] !== undefined
        //             const isDoHash = tokens[3] !== undefined
        //             console.log(await this.worker._internalDb.analyzeDataEntity(de, isIncludeRows, isDoHash))
        //         }else if (action === 'stop'){
        //             await this._net.showStatus()
        //             this.worker.stop()
        //             await this._net.showStatus()
        //         } else if (action === 'perf'){
        //             console.log(db._workflow.getPerfRecords())
        //         } else if (action === 'version'){
        //             console.log(VERSION)
        //         } else if (action === 'create') {
        //             const type = tokens[1]
        //             const metaData = JSON.parse(tokens[2] || '{}')
        //             const de=metaData
        //             console.log(await this.worker.create(type,de))
        //         }else if (action === 'drop') {
        //             const type = tokens[1] 
        //             const entityName = tokens[2] 
        //             console.log(await this.worker.drop(type,entityName))
        //         }
        //         else {
        //             Logger.logWarn('Unknown command, validating commands are [select,insert,update,delete,updateOrInsert,bulkLoad,introduceNode, kickNode, checkStatus,snapshot,forcePull] ')
        //         }
        //         this._worker.workflow.describeTasks()
        //         Logger.logInfo('Queue stats: ' + JSON.stringify(this._worker.workflow.getQueueStats()))
        //     }catch(e){
        //         Logger.logError(e)
        //     }
        // })
    }

    /**
     * Handlers for events of libp2p
     */
    private setNodeEventHandlers(){
        this._node.addEventListener('peer:discovery', async (evt) => { // evt.detail is PeerInfo
            const id = evt.detail.id.toString()
            clusterLogger.logInfo('Discovered:' + id)
            const peerState = this._net._nodeStateHandler.getPeerState(id)
            if (peerState){
                const peerMeta = peerState.peer
                peerMeta.updateMultiaddrs(evt.detail.multiaddrs)
                await this._worker._internalDb.updateOrInsert('meta_peers', peerMeta._peerMeta) // Overrides peerMeta with possible new ip addr and ts
            }
        })
        this._node.addEventListener('peer:connect', async (evt) => { // evt.detail is Connection
            const conn = evt.detail
            const id = conn.remotePeer.toString()
            if(!this._net._nodeStateHandler.connectedPeers.includes(id)){
                this._net._nodeStateHandler.connectedPeers.push(id);
            }
            clusterLogger.logInfo('peer:connect event, ID: ' + id + ', addr: ' + conn.remoteAddr.toString())
            // this._net.setAddressBook()
            const peersRecord = this._worker._internalDb.selectSync('meta_peers', {id})
            if (peersRecord?.rowCount > 0){
                let currentPeerState = this._net._nodeStateHandler.getPeerState(id)
                if (!currentPeerState){
                    await this._net._nodeStateHandler.setAddressBook()
                    currentPeerState = this._net._nodeStateHandler.getPeerState(id)
                    if (!currentPeerState){ // Should never happen
                        clusterLogger.logWarn('Peer state not exist for ' + id)
                        return 
                    } 
                    currentPeerState.status = NET_STATUS.PEER_CONNECTED
                }else{
                    currentPeerState.status = NET_STATUS.PEER_CONNECTED
                }
                clusterLogger.logInfo('Local node status: ' + this._worker.statusService.status)
                if ([STATUS.LONELY_ISLAND,STATUS.CHECKIN].includes(this._worker.statusService.status) ){
                    // if (!this.worker.workflow.existsTaskDesc('Checkin process')){
                    //     this._net.checkin() // A peer connected in lonely island mode, do checkin
                    // }else{
                    //     this._worker.statusService.status === STATUS.READY
                    // }
                    clusterLogger.logInfo('Trigger checkin for newly connected peer:'+id);
                    this._net.checkin(true) 
                }else{
                    clusterLogger.logInfo('Bypass the checkin since the status is not LONELY_ISLAND or CHECKIN, peer id:'+id);
                }
            } else if (this._net.isAutoScalingEnabled() && !this._net.isScalableNode()) { // The persistent node does this
                this._net.pushAutoScale(conn.remotePeer)
            } else {
                setTimeout(async () => { // Jerry: here is why Jest won't stop even all cases are executed. No big impact though.
                    const peerData = this._worker._internalDb.selectSync('meta_peers', {id})
                    if (peerData && peerData.rowCount > 0){
                        // The connection peer is legal.
                        let peerState = this._net._nodeStateHandler.getPeerState(id)
                        if (!peerState){
                            await this._net._nodeStateHandler.setAddressBook()
                            peerState = this._net._nodeStateHandler.getPeerState(id)
                            if (!peerState){ // Should never happen
                                clusterLogger.logWarn('Peer state not exist for ' + id)
                                return 
                            } 
                            peerState.status = NET_STATUS.PEER_CONNECTED
                        }
                    } else {
                        clusterLogger.logWarn(`Node ${id} is not a recognized peer of private net, breaking the connection.`)
                        await this._net.deletePeer(await NodeMetadata.fromJSON({id, multiaddrs: [conn.remoteAddr.toString()]}))
                    }
                }, CONFIG.ILLEGAL_CONNECTION_TOLERANCE)
            }
        })
        this._node.addEventListener('peer:disconnect', async (evt) => {
            const conn = evt.detail
            const id = conn.remotePeer.toString()
            const index = this._net._nodeStateHandler.connectedPeers.indexOf(id);
            if(index > -1){
                this._net._nodeStateHandler.connectedPeers.splice(index, 1);
            }
            clusterLogger.logInfo('peer:disconnect event, ID: ' + id)
            try{
                const peerRecord = this.worker._internalDb.selectSync('meta_peers', {id})
                if (peerRecord.rowCount === 0) { // the peer is kicked. delete it from peers state record
                    if (this._net._nodeStateHandler.peers.has(id)) {
                        this._net._nodeStateHandler.peers.delete(id)
                    }
                    return
                } else if (this._net.isAutoScalingEnabled() && peerRecord.data[0].nodeType === ClusterNodeType.SCALABLE){ // Kick the peer when it's offline and it's scalable node.
                    const peerMetadata = await this._net.getPeerMeta(conn.remotePeer)
                    return await this.worker.kickNode(peerMetadata.getPeerMeta()!)
                }
                const peerState = this._net._nodeStateHandler.getPeerState(id)
                if (peerState){
                    peerState.status = NET_STATUS.PEER_CONNECT_FAILED
                }
                //TODO dial
                if(!this._net._nodeStateHandler._interval){
                    this._net._nodeStateHandler._interval = setInterval(async () => {
                        if(this._net._nodeStateHandler._debounceId){
                            clearTimeout(this._net._nodeStateHandler._debounceId)
                        }
                        const isAllReady = await this._net._nodeStateHandler.checkPeerState()
                        if( isAllReady){
                            clearInterval(this._net._nodeStateHandler._interval);
                        }
                    }, CONFIG.CHECK_NODE_STATE_INTERVAL_TIME);
                }
                const isLonely = await this._net.isLonelyIsland() // When kickNode, it should not go in here.
                if (isLonely){ // In case all nodes are disconnected
                    this._worker.statusService.status = STATUS.LONELY_ISLAND
                    clusterLogger.logWarn('peer:disconnect caused local status become LONELY ISLAND, peer:'+ id)
                }
            }catch(e){
                const peer = await NodeMetadata.fromJSON({
                    id: conn.remotePeer.toString(),
                    multiaddrs: [conn.remoteAddr.toString()]
                })
                const peerState: IPeerState = {
                    id,
                    peer,
                    status: NET_STATUS.PEER_CONNECT_FAILED
                }
                this._net._nodeStateHandler.setPeerState(id, peerState)
            }
        })
        this._node.peerStore.addEventListener('peer', (evt) => {
            clusterLogger.logInfo('peer event, peerId: ' + evt.detail.id.toString())
        })
        this._node.peerStore.addEventListener('change:multiaddrs', async (evt) => {
            clusterLogger.logInfo('change:multiaddrs event, peerId: ' + evt.detail.peerId.toString() + ', multiaddrs ' + evt.detail.multiaddrs + ', oldMultiaddrs ' + evt.detail.oldMultiaddrs)
            return true
        })
        this._node.peerStore.addEventListener('change:protocols', (evt) => {
            clusterLogger.logDebug('change:protocols event, peerId: ' + evt.detail.peerId.toString() + ', protocols ' + evt.detail.protocols)
        })
    }

    /**
     * Set a handler for messages received on each channel (or called protocol by libp2p)
     */
    private setReceiveHandlers(){
        this._privateChannels = [ // Only for the nodes inside the private net
            CHANNELS.HAPPY_TO_JOIN, CHANNELS.LEAVE, CHANNELS.CHECKIN, CHANNELS.CHECKIN_RES,
            CHANNELS.TXN, CHANNELS.TXN_CONFIRM, CHANNELS.TXN_GO, CHANNELS.TXN_DONE,
            CHANNELS.BATCH_TXN, CHANNELS.BATCH_TXN_CONFIRM, CHANNELS.BATCH_TXN_GO, CHANNELS.BATCH_TXN_DONE,
            CHANNELS.FILE_TRANSFER_REQ, CHANNELS.FILE_TRANSFER_RES, CHANNELS.FILE_TRANSFER_PULL_DONE,
            CHANNELS.PULL, CHANNELS.PULL_RES, CHANNELS.KICK, CHANNELS.SELECT_MEMORY, CHANNELS.SELECT_MEMORY_DONE,
            CHANNELS.AUTO_RESOLVE, CHANNELS.AUTO_RESOLVE_DONE          
        ]
        this._strangerChannels = [ // Only for the nodes outside the private net
            CHANNELS.INTRODUCE
        ]
        this._publicChannels = [ // Callable by any other nodes
            CHANNELS.STATUS, CHANNELS.STATUS_CONFIRM, CHANNELS.PING, CHANNELS.PONG, CHANNELS.REJECT, CHANNELS.AUTO_SCALE, CHANNELS.AUTO_SCALE_RES
        ]
        this.setMsgHandler(CHANNELS.CMD, this.handleCmd)
        this.setMsgHandler(CHANNELS.CMD_RES, this.handleCmdRes)
        this.setMsgHandler(CHANNELS.INTRODUCE, this.handleIntroduce)
        this.setMsgHandler(CHANNELS.HAPPY_TO_JOIN, this.handleHTJ)
        this.setMsgHandler(CHANNELS.STATUS, this.handleStatus)
        this.setMsgHandler(CHANNELS.STATUS_CONFIRM, this.handleStatusConfirm)
        this.setMsgHandler(CHANNELS.CHECKIN, this.handleCheckin)
        this.setMsgHandler(CHANNELS.CHECKIN_RES, this.handleCheckinRes)
        this.setMsgHandler(CHANNELS.TXN, this.handleTxn)
        this.setMsgHandler(CHANNELS.TXN_CONFIRM, this.handleTxnConfirm)
        this.setMsgHandler(CHANNELS.TXN_GO, this.handleTxnGo)
        this.setMsgHandler(CHANNELS.TXN_DONE, this.handleTxnDone)
        this.setMsgHandler(CHANNELS.BATCH_TXN, this.handleBatchTxn)
        this.setMsgHandler(CHANNELS.BATCH_TXN_CONFIRM, this.handleBatchTxnConfirm)
        this.setMsgHandler(CHANNELS.BATCH_TXN_GO, this.handleBatchTxnGo)
        this.setMsgHandler(CHANNELS.BATCH_TXN_DONE, this.handleBatchTxnDone)
        this.setMsgHandler(CHANNELS.SELECT_MEMORY, this.handleSelectMemory)
        this.setMsgHandler(CHANNELS.SELECT_MEMORY_DONE, this.handleSelectMemoryDone)
        this.setMsgHandler(CHANNELS.PULL, this.handlePull)
        this.setMsgHandler(CHANNELS.PULL_RES, this.handlePullRes)
        this.setMsgHandler(CHANNELS.PUSH_GO, this.handlePushGo)
        this.setMsgHandler(CHANNELS.PUSH_DONE, this.handlePushDone)
        this.setMsgHandler(CHANNELS.FORCEPULL_GO, this.handleForcePullGo)
        this.setMsgHandler(CHANNELS.FORCEPULL_DONE, this.handleForcePullDone)
        this.setMsgHandler(CHANNELS.PING, this.handlePing)
        this.setMsgHandler(CHANNELS.PONG, this.handlePong)
        this.setMsgHandler(CHANNELS.KICK, this.handleKick)
        this.setMsgHandler(CHANNELS.LEAVE, this.handleLeave)
        this.setMsgHandler(CHANNELS.REQ_CHECKIN, this.handleReqCheckin)
        this.setMsgHandler(CHANNELS.REQ_CHECKIN_ACK, this.handleReqCheckinAck)
        this.setMsgHandler(CHANNELS.FILE_TRANSFER_REQ, this.handleFileTransferReq)
        this.setMsgHandler(CHANNELS.FILE_TRANSFER_PULL_DONE, this.handleFileSyncPullDone)
        this.setMsgHandler(CHANNELS.REJECT, this.handleReject)
        this.setMsgHandler(CHANNELS.AUTO_SCALE, this.handleAutoScale)
        this.setMsgHandler(CHANNELS.AUTO_SCALE_RES, this.handleAutoScaleRes)
        this.setMsgHandler(CHANNELS.AUTO_RESOLVE, this.handleConflictGo)
        this.setMsgHandler(CHANNELS.AUTO_RESOLVE_DONE, this.handleConflictDone)
        this.handleFileTransferRes(CHANNELS.FILE_TRANSFER_RES)
    }
    
    /**
     * Parse the message received, and return the parsed TaskMessage object.
     * It will handle message fragmenting as well
     * @param buffList The message received is put into list of buffers
     * @param connection 
     * @param protocol 
     * @returns TaskMessage: {peer: NodeInfo, taskid: string, payload: any}
     */
    private async parseMsg(buffList:any, connection: any, protocol: string): Promise<ITaskMessage | null>{
        const hasFragments = connection['frag'+protocol] !== undefined
        const b64 = this._textDecoder.decode(buffList.slice())
        clusterLogger.logDebug('Parsing msg from peer' + connection.remotePeer.toString() + ', channel ' + protocol + ', length: ' + b64.length)
        if (b64 && b64.endsWith(CONFIG.PACKET_END_ESC)){
            try{
                let msg: Buffer | string = Buffer.from(b64.substring(0, b64.length - CONFIG.PACKET_END_ESC.length), 'base64')
                msg = hasFragments? Buffer.concat([connection['frag'+protocol], msg]) : msg
                let str: string = ''
                let isNotZipped = false
                if (CONFIG.MESSAGE_COMPRESS === 'GZIP'){
                    try{
                        const temp = await ungzip(msg)
                        str = this._textDecoder.decode(temp)
                    } catch (err) {
                        if (err.message === 'Error: incorrect header check'){ // Error while unzip the message. 
                            str = msg.toString('utf8') // In case the message is from old version that message was not zipped
                            isNotZipped = true
                        } else {
                            throw err
                        }
                    }
                } else {
                    str = msg.toString('utf8')
                }
                if (hasFragments){
                    clusterLogger.logDebug('Rebuilt fragmented JSON string length: ' + connection['frag'+protocol].length)
                    delete connection['frag'+protocol]
                }
                const data = JSON.parse(str);
                const peer = data['peer']
                const taskid = data['taskid']
                const payload = data['payload']
                if (!this.channelAuthCheck(protocol, connection)){ // Do nothing if the message is from illegal node.
                    peer.multiaddrs = [connection.remoteAddr.toJSON()]
                    peer.peerId = connection.remotePeer.toJSON()
                    await this.rejectUnauthorized({peer, taskid})
                    return null
                }
                return {
                    peer, taskid, payload, isNotZipped
                }
            } catch(e) { // the buff doesn't contain complete JSON string. 
                clusterLogger.logError(e)
                return null
            }
        } else {
            const msg = Buffer.from(b64, 'base64')
            const buf = hasFragments? Buffer.concat([connection['frag'+protocol], msg]): msg
            // const str = hasFragments? connection['frag'+protocol] + msg : msg
            clusterLogger.logDebug('Receiving data fragment for peer: ' + connection.remotePeer.toString() + ', channel: ' + protocol + ', size: ' + buf.length)
            connection['frag'+protocol] = buf
            return null
        }
    }

    channelAuthCheck(channel: string, connection: any): boolean {
        // const isPeer = (this._worker._internalDb.selectSync('meta_peer', {id: connection.remotePeer._idB58String})).rowCount > 0
        const isPeer = this._net._nodeStateHandler.peers.has(connection.remotePeer.toString())
        if (this._privateChannels.includes(channel) && !isPeer){
            clusterLogger.logWarn(`Received message from illegal node: ${connection.remoteAddr.toString()} on channel: ${channel}`)
            return false
        }
        if (this._strangerChannels.includes(channel) && isPeer){
            // Do something?
        }
        return true
    }

    /**
     * Send a reject message to the unauthorized node.
     * @param data 
     */
    async rejectUnauthorized(data:any){
        try {
            const {peer, taskid} = data
            clusterLogger.logInfo('Sending reject to unauthorized node: ' + peer.id, taskid)
            await this._net.sendTask(peer, CHANNELS.REJECT, taskid, {message: 'Not authorized'})
        } catch (e) {
            clusterLogger.logError(e)
        }
    }

    /**
     * Invokes the message handling of libp2p
     * @param channel 
     * @param handlerFunc 
     */
    private setMsgHandler(channel: string, handlerFunc: Function){
        const _this = this
        this._node.handle(channel, ({ stream, connection }) => { // libp2p event handling.
            // if (!this.channelAuthCheck(channel, connection)){ // Do nothing if the message is from illegal node.
            //     return
            // }
            clusterLogger.logDebug(`Received message from peer ${connection.remotePeer.toString()}, channel: ${channel}`)
            pipe(
              stream,
              async function (source: any) {
                for await (const msg of source) {
                    timeTracer.trace('parseMsg ' + channel)
                    const data = await _this.parseMsg(msg, connection, channel)
                    timeTracer.trace('parseMsg ' + channel)
                    if (data){
                        handlerFunc.call(_this, data) // call the handler function assigned to the channel
                    }
                }
              }
            )
        })
    }

    /**
     * Handle the auto-scale request. For the 1st request, it sends the peer metadata, for further requests, it doesn't send metadata, but send a msg.
     * @param data 
     */
    private async handleAutoScale(data:any){
        const {peer, taskid, payload} = data

        if (!this._net.isAutoScalingEnabled() || this._net.isInCluster()) {
            await this._net.sendTask(peer, CHANNELS.AUTO_SCALE_RES, taskid, {
                status: false, peerMeta: undefined, message: 'the same node is already in current cluster'
            })
            return // Auto scaling is not enabled. Do nothing.
        }

        try {
            clusterLogger.logInfo('Received auto scale request from peer: ' + peer.id + ', payload: ' + payload, taskid)
            // Only accept the 1st autoscaling request
            const status = this._net.autoScaleStatus === AUTO_SCALE_STATUS.NONE
            const resp: IAutoScaleResult = {
                status,
                peerMeta: status? this._pnNode.getPeerInfo(): undefined,  // Provide the peer metadata to 1st requester
                message: status? undefined: 'In another auto-scaling proposal' // Send message to 2nd requester
            }

            this._net.autoScaleStatus = AUTO_SCALE_STATUS.ACCEPTING
            const peerId = (await NodeMetadata.fromJSON(peer)).peerIdInst
            const peerMeta = await this._net.getPeerMeta(peerId)

            await this._net.sendTask(peerMeta.getPeerMeta()!, CHANNELS.AUTO_SCALE_RES, taskid, resp)
        } catch (e) {
            await this._net.sendTask(peer, CHANNELS.AUTO_SCALE_RES, taskid, {
                status: false, peerMeta: undefined, message: 'failed to handle auto scale'
            })
            clusterLogger.logError(e)
        }
    }
    
    /**
     * Receives the peer metadata and continue the auto-scale task processing.
     * @param data 
     */
    private async handleAutoScaleRes(data:any){
        try {
            const {peer, taskid, payload} = data
            clusterLogger.logInfo('Received auto scale response from peer: ' + peer.id + ', payload: ' + JSON.stringify(payload), taskid)
            if (taskid){
                const asTask = this.worker.workflow.getTask(taskid)
                if (asTask){
                    asTask.resolve(payload)
                }
            }
        } catch (e) {
            clusterLogger.logError(e)
        }
    }

    /**
     * Executes a command
     */
    private async handleCmd(data:any){
        try {
            const {peer, taskid, payload} = data
            txnLogger.logInfo('Handle cmd from peer: ' + peer.id + ', payload: ' + JSON.stringify(payload), taskid)
            const {cmd, parames} = payload
            const result = await this.worker.exec(cmd, parames)
            await this._net.sendTask(peer, CHANNELS.CMD_RES, taskid, result)
        } catch (e) {
            txnLogger.logError(e)
        }
    }


    /**
     * Response of command execution
     */
     private async handleReject(data:any){
        try {
            const { peer, taskid, payload } = data
            const { message } = payload
            const rejectedTask = this.worker.workflow.getTask(taskid)
            txnLogger.logWarn('Task ' + (rejectedTask?.description || '') + ' rejected by node: ' + peer.id + ', message: ' + message, taskid)
            rejectedTask?.resolve({status: false, message})
        } catch (e) {
            txnLogger.logError(e)
        }
    }


    /**
     * Response of command execution
     */
     private async handleCmdRes(data:any){
        try {
            const {peer, taskid, payload} = data
            txnLogger.logInfo('Handle cmd res from peer: ' + peer.id + ', payload: ' + JSON.stringify(payload), taskid)
            const execTask = this.worker.workflow.getTask(taskid)
            execTask?.resolve(payload)
        } catch (e) {
            txnLogger.logError(e)
        }
    }

    /**
     * Msg handler for channel: introduce
     * @param data 
     */
    private async handleIntroduce(data:any){
        try {
            const {peer, taskid, payload} = data
            clusterLogger.logInfo('Handle introduce from peer: ' + peer.id + ', payload: ' + payload, taskid)
            const {peers, newbie,dbExtend} = payload
            // No need to check same server and port, but different peer id. libp2p already done validation for this kind of request.
            if (newbie && newbie.id === this._pnNode.getPeerInfo().id){ // local node is the newbie
                const peerInfos = peers.data
                clusterLogger.logDebug('Being introduced. Existing peers: ' + JSON.stringify(peerInfos), taskid)
                await this.worker._internalDb.bulkLoad('meta_peers', peerInfos)
                await this.worker.createEntities(dbExtend.entity)  //newbie node create extend entity
                await this._net._nodeStateHandler.setAddressBook()
                await this._net.dialAll()
                let peerState
                peerInfos.forEach((peer: { id: any }) => {
                    if (this._net._nodeStateHandler.connectedPeers.includes(peer.id)) {
                        peerState = this._net._nodeStateHandler.getPeerState(peer.id)
                        if(peerState){
                            peerState.status = NET_STATUS.PEER_CONNECTED
                        }
                    }
                })
                await this._net.pullAll(peer.id)
                await this.worker.clearLocalDE() // memory or local data can't be cleared by pull, so clear them here.
                await this._net.sendTask(peer, CHANNELS.HAPPY_TO_JOIN, taskid, {}, true)
                this._net.bookAutoCheckin()
                this.worker._txnRedundent =  peerInfos.length// when being introduced, it need set the txn redundent correctly
                this.worker.updateBCRedundents()
                if (this._net.autoScaleStatus === AUTO_SCALE_STATUS.ACCEPTING) { // Finish the auto-scaling process when it joined the clsuter.
                    this._net.autoScaleStatus = AUTO_SCALE_STATUS.ACCEPTED
                }
            } else if (peer) {
                await this._net._nodeStateHandler.setAddressBook()
                await this.worker.createEntities(dbExtend.entity)  //peer node create extend entity
                await this._net._node.dial(await NodeMetadata.fromJSON(newbie))
                const peerState = this._net._nodeStateHandler.getPeerState(newbie.id)
                if (peerState){
                    peerState.status = NET_STATUS.PEER_CONNECTED
                }
                await this._net.sendTask(peer, CHANNELS.HAPPY_TO_JOIN, taskid, {})
                this.worker._txnRedundent ++ // when being introduced, it need set the txn redundent correctly
                this.worker.updateBCRedundents()
            } else {
                throw Error('Welcome from nobody...')
            }
            await this._net.showStatus()
 
            if(this._net._clusterResolve) {
                this._net._clusterResolve()
            }

            // Logger.logInfo(`Done welcome task from peer: ${peer.id}`)
        } catch (e) {
            clusterLogger.logError(e)
        }
    }

    /**
     * Msg handler for channel: kick
     * @param data 
     */
    private async handleKick(data:any){
        try {
            const {peer, taskid, payload} = data
            const {peers, peerInfo} = payload
            clusterLogger.logInfo('Received KICK from peer: ' + peer.id + ', payload: ' + payload, taskid)
            if(peerInfo.id === this._pnNode.getPeerInfo().id) {
                // For the deleted peer of itself should delete other peers, 
                // for example A delete C, for C, it should delete ABD, and generate peer.json
                this._net.cancelAutoCheckin()
                await this._net.sendTask(peer, CHANNELS.LEAVE, taskid, {})
                await this._worker._internalDb.delete('meta_peers')
                await this._worker._internalDb.insert('meta_peers', peerInfo)
                const others = peers.data.filter((d: any) => d.id !== peerInfo.id);
                for await (const node of others){
                    const peerInst = await NodeMetadata.fromJSON(node)
                    await this._net.deletePeer(peerInst)
                    clusterLogger.logInfo('delete addressBook: ' + node.id, taskid)
                }
                this._worker.statusService.status = STATUS.READY // Set status to ready after it's kicked.
                this.worker._txnRedundent = 0 // when being kicked, it need set the txn redundent correctly
                this.worker.updateBCRedundents()
            } else {
                // B, D delete addressbook
                const peerInst = await NodeMetadata.fromJSON(peerInfo)
                await this._net._internalDb.delete('meta_peers', {id: peerInfo.id}) // remove the peer from internalDb
                await this._net.deletePeer(peerInst) // disconnect the peer
                await this._net.sendTask(peer, CHANNELS.LEAVE, taskid, {})
                clusterLogger.logInfo('delete addressBook: ' + peerInfo.id, taskid)
                this.worker._txnRedundent -- // when being kicked, it need set the txn redundent correctly
                this.worker.updateBCRedundents()
            }
            await this._net.showStatus()
        } catch (e) {
            clusterLogger.logError(e)
        }
    }

    /**
     * Msg handler for channel: happyToJoin
     * @param data 
     */
    private handleHTJ(data:any){
        try {
            const {peer, taskid, payload} = data
            clusterLogger.logInfo('Handle happy to join from peer: ' + peer.id + ', payload: ' + payload, taskid)
            const introduceBroadcastTask = this.worker.workflow.getTask(taskid)
            introduceBroadcastTask?.resolve({status: true})
        } catch (e) {
            clusterLogger.logError(e)
        }
    }

    /**
     * Msg handler for channel: leave
     * @param data 
     */
    private handleLeave(data:any){
        try {
            const {peer, taskid, payload} = data
            clusterLogger.logInfo('Handle delete peer: ' + peer.id + ', payload: ' + payload, taskid)
            const kickBroadcastTask = this.worker.workflow.getTask(taskid)
            kickBroadcastTask?.resolve({status: true})
        } catch (e) {
            clusterLogger.logError(e)
        }
    }

    /**
     * Msg handler for channel: status
     * @param buff 
     */
    private async handleForcePullGo(data:any){
        // this._worker.statusService.status
        try {
            const {peer, taskid, payload} = data
            pullLogger.logInfo('Handle ForcePull from peer: '  + peer.id + ', payload: ' + payload, taskid)
            await this._net.pullAll(peer.id)
            await this._net.sendTask(peer, CHANNELS.FORCEPULL_DONE, taskid,{})
        } catch (e) {
            pullLogger.logError(e)
        }
    }

    /**
     * BZ-15355, sync file for cluster
     * @param data 
     */
    private async handleFileSyncPullDone(data:any){
        try {
            const {peer, taskid, payload} = data
            clusterLogger.logInfo(`FileSync::handlePullDone(FST_PULL_DONE), from peer: ${peer.id}, payload: ${payload}`, taskid)
            const task = this.worker.workflow.getTask(taskid)
            task?.resolve({status: true})
        } catch (e) {
            clusterLogger.logError(e)
        }
    }



    /**
     * Msg handler for channel: status
     * @param data 
     */
    private async handleForcePullDone(data:any){
        try {
            const {peer, taskid, payload} = data
            pullLogger.logInfo('Received ForcePull DONE from peer:'  + peer.id + ', payload: ' + payload, taskid)
            const andleForcePull = this.worker.workflow.getTask(taskid)
            andleForcePull?.resolve({status: true})

        } catch (e) {
            pullLogger.logError(e)
        }
    }

    /**
     * Msg handler for channel: status
     * @param data 
     */
     private async handleReqCheckin(data:any){
        try {
            const {peer, taskid, payload} = data
            checkinLogger.logInfo('Handle require checkin from peer: ' + peer.id + ', payload: ' + payload, taskid)
            this._net.checkin(false, taskid)
            await this._net.sendTask(peer, CHANNELS.REQ_CHECKIN_ACK, taskid, {})
        } catch (e) {
            checkinLogger.logError(e)
        }
    }

    
    /**
     * Msg handler for channel: status
     * @param data 
     */
     private async handleReqCheckinAck(data:any){
        try {
            const {peer, taskid, payload} = data
            checkinLogger.logDebug('Handle require checkin acknowlege from peer: ' + peer.id + ', payload: ' + payload, taskid)
            const broadcastTask = this.worker.workflow.getTask(taskid)
            broadcastTask?.resolve({status: true})
        } catch (e) {
            checkinLogger.logError(e)
        }
    }


    /**
     * Msg handler for channel: status
     * @param data 
     */
    private async handleStatus(data:any){
        // this._worker.statusService.status
        try {
            const {peer, taskid, payload, isNotZipped} = data

            const {type} = payload
            const extendRequired = payload.extendRequired
            clusterLogger.logDebug('Handle check status from peer: ' + peer.id + ', payload: ' + payload, taskid)

            const peers = await this.worker._internalDb.select('meta_peers')
            const inNet = isNotZipped? true: peers.data.length > 1 // In case the status check is from old version, return a true for inNet, so the introduce could fail.
            let result = {
                status: this.worker.statusService.status,
                inNet,
                peerId: this._pnNode.getPeerInfo().id,
                type,
                version: VERSION,
                appName: this.worker.appName
            }
            if(extendRequired){
                const extendEntity = await this.worker._internalDb.select(CONFIG.EXTEND_ENTITY);
                //extendEntity.data=extendEntity.data.filter((e:any)=>{return e.owner===this._net._node._nodeMeta.id})
                result=Object.assign(result,{"dbExtend":{entity:extendEntity}});
            }
            await this._net.sendTask(peer, CHANNELS.STATUS_CONFIRM, taskid, result, true, isNotZipped)
        } catch (e) {
            clusterLogger.logError(e)
        }
    }

    /**
     * Msg handler for channel: status
     * @param data 
     */
     private async handleSelectMemory(data:any){
        // this._worker.statusService.status
        try {
            const {peer, taskid, payload} = data
            const {type, dataEntityName, filter, options} = payload
            clusterLogger.logDebug('Handle select memory from peer: ' + peer.id + ', payload: ' + JSON.stringify(payload), taskid)

            const peers = await this.worker._internalDb.select('meta_peers')
            const id = this._pnNode.getPeerInfo().id
            const result = {
                inNet: peers.data.length > 1,
                status: this._worker.statusService.status,
                peerId: id,
                serverName: (peers.data.find((d: any) => d.id === id) || {}).serverURL,
                type,
                data: await this.worker._internalDb.select(dataEntityName, filter, options)
            }

            await this._net.sendTask(peer, CHANNELS.SELECT_MEMORY_DONE, taskid, result)
        } catch (e) {
            clusterLogger.logError(e)
        }
    }

      /**
     * Msg handler for channel: pong
     * @param data 
     */
    private async handleSelectMemoryDone(values:any){
        const {peer, taskid, payload} = values
        const {inNet, type, peerId, serverName, status, data} = payload
        clusterLogger.logDebug(`Received select memory done from ${peer.id}, inNet: ${inNet}`, taskid)
        const workflow = this.worker.workflow
        const resTask = workflow.getTask(taskid);
       
        const setPro = (task: any, rsTask: any) => {             
            task.setProperty('status', {data, status, inNet})
            rsTask.resolve({data, inNet, status})
        }
    
        let statusProcess = workflow.getTask(taskid)
        if (!statusProcess) {
            throw Error('Task not found: ' + taskid)
        }

        if(type === 'broadcast') {
            statusProcess = workflow.getTask(statusProcess.parentId)
            if (!statusProcess) {
                throw Error('Task not found: ' + statusProcess)
            }
            let statuses = statusProcess.getProperty('statuses')
            if (!statuses) {
                statuses = new Map<string, any>()
                statuses.set('statuses', [{id: peerId, serverName, data, status, inNet}])
            } else {
                const arr = statuses.get('statuses');
                arr.push({id: peerId, serverName, data, status, inNet})
                statuses.set('statuses', arr)
            }
            statusProcess.setProperty('statuses', statuses)
        }
        
        setPro(statusProcess, resTask)
    }

    /**
     * Msg handler for channel: ping
     * @param data 
     */
    private async handlePing(data:any){
        const {peer, taskid, payload} = data
        clusterLogger.logInfo('Received PING from peer: ' + peer? peer.id: peer + ', payload: ' + payload, taskid)
        await this._net.sendTask(peer, CHANNELS.PONG, taskid, {})
    }

    /**
     * Msg handler for channel: pong
     * @param data 
     */
    private async handlePong(data:any){
        const {peer, taskid, payload} = data
        clusterLogger.logInfo('Received PONG from peer: ' + peer + ', payload: ' + payload, taskid)
        if (taskid){
            const pingTask = this.worker.workflow.getTask(taskid)
            if (pingTask){
                pingTask.resolve({status: true, message: 'pong'})
            }
        }
    }

    /**
     * Msg handler for channel: checkin
     * @param data 
     */
    private async handleCheckin(data:any){
        try {
            const { peer, taskid, payload } = data
            const { chainsStat } = payload
            // await this.worker.waitLoadReady() // Wait till load ready before respond the checkin, otherwise the data might be incomplete.
            await this._net.waitForPull()
            const peerChainsStat = Utils.arrayToBCSMap(chainsStat)
            // getStatistics is queued up with appendBlock, appendBlocks
            const statistics = await this._worker.getStatistics('both', taskid, peerChainsStat, peer?.id)
            const { stats, isConflict } = statistics.chain
            const dbStat = statistics.db
            // const dbStat = await this._worker.getDBStat()
            if (isConflict){
                checkinLogger.logSevere('Conflict peer: ' + peer.id, taskid)
            }
            const isInConflictStat = this._worker.statusService.status === STATUS.DATA_CONFLICT
            checkinLogger.logDebug('Handle checkin from peer: ' + peer.id + ', payload: ' + payload, taskid)
            const checkinResTask = this.worker.workflow.generateTask({timeoutMs:0, desc: 'Checkin-res task'}, async () => {
                try{
                    const res = {
                        chainsStat: Utils.mapToArray(stats),
                        dbStat: Utils.mapToArray(dbStat),
                        isConflict, // Whether chain or data contains conflict
                        isInConflictStat // Whether the local node is in data conflict status
                    }
                    await this._net.sendTask(peer, CHANNELS.CHECKIN_RES, taskid, res)
                }catch(e){
                    checkinLogger.logError(e, taskid)
                }
            })
            await checkinResTask.execute()
            checkinLogger.logDebug(`Done checkin-res task for peer: ${peer.id}`, taskid)
            // this.worker.workflow.enqueueTask(checkinResTask).then(() => {
            //     console.log(`Done checkin-res task for peer: ${peer.id}`)
            // }).catch((err) => {
            //     console.error(err)
            // })
            checkinResTask.resolve('done')
        } catch (e) {
            checkinLogger.logError(e)
        }
    }
    
    /**
     * Msg handler for channel: checkin-res
     * @param data 
     */
    private handleCheckinRes(data:any){
        try{
            const {peer, taskid, payload} = data
            checkinLogger.logDebug('Handle checkin-res from peer: ' + peer.id, taskid)
            if (taskid && payload){
                const task = this.worker.workflow.getTask(taskid)
                if (!task) {
                    throw Error('Task not found: ' + taskid)
                }
                const checkinProcess = this.worker.workflow.getTask(task.parentId)
                if (!checkinProcess) {
                    throw Error('Checkin process not found: ' + task.parentId)
                }

                const { chainsStat, dbStat, isConflict, isInConflictStat } = payload
                const nodeStat = {
                    chainsStat: Utils.arrayToBCSMap(chainsStat),
                    dbStat: Utils.arrayToBCSMap(dbStat),
                    isConflict,
                    isInConflictStat
                }
                let nodeData = checkinProcess.getProperty('nodeData')
                if (!nodeData) nodeData = new Map<string, any>()
                nodeData.set(peer.id, nodeStat)
                checkinProcess.setProperty('nodeData', nodeData) // record the chain from a peer to process
                // checkinProcess.setProperty(peer.id, Utils.arrayToMap(payload)) 

                task.resolve({status: true, message: `Checkin response from peer: ${peer.id}`})
                checkinLogger.logDebug('Resolved checkin broadcast from peer: ' + peer.id, taskid)
                const peerState = this._net._nodeStateHandler.getPeerState(peer.id)
                if (peerState){
                    peerState.status = NET_STATUS.PEER_CONNECTED
                }
            }
        }catch(e){
            checkinLogger.logError(e)
        }
    }

    /**
     * Msg handler for channel: pull
     * @param data 
     * @returns 
     */
    private async handlePull(data: any){
        const {peer, taskid, payload} = data
        pullLogger.logInfo(`Received PULL request from ${peer.id}`, taskid)
        const getData= async(deMeta:DataEntityMetadata,subject:string)=>{
            let data
            pullLogger.logInfo(`Respond full data for ${subject}`, taskid)
            if (this.worker.isApplyToPartition(deMeta)){ // Use partitions as much as possible, it has better performance.
                data = await this.worker._internalDb.partitions(subject)
            } else if (deMeta && (deMeta.fileName || (Array.isArray(deMeta.primaryKeys) && deMeta.primaryKeys.length>0))){
                data = await this.worker.select(subject)
            } else {
                data = await this.worker.selectNoPKData(subject)
            }
            return data
        }
        let payloadPull: object
        const blocksMap = new Map<string, any>()
        if(payload.entities && Array.isArray(payload.entities)){
            pullLogger.logInfo(`Respond entities ${payload.entities.join()}`, taskid)
            for await(let subject of payload.entities){
                try{
                    const options = {
                        groups: ['snapshot-' + subject], 
                        desc: 'Generates DE snapshot for ' + subject
                    }
                    // Here returns both chain and DB data to peer. Make sure the append block is complete (txn executed) before gether the response data.
                    const snapshotTask = this.worker.workflow.generateTask(options, async () => {
                        const chain = this.worker.findChain(subject)
                        const deMeta =chain? this.worker._dbMeta.getDataEntities().get(subject):undefined
                        if(deMeta){
                            let data=await getData(deMeta,subject)
                            blocksMap.set(subject, {isFullPull: true, chain, data})
                        }
                    })
                    await this.worker.workflow.waitForTaskGroups(['append-block-' + subject])
                    await snapshotTask.executeAndResolve()
                }catch(err){
                    pullLogger.logWarn(err, taskid)
                }
            }
            payloadPull = {
                chains: Utils.mapToArray(blocksMap)
            }
            this._net.sendTask(peer, CHANNELS.PULL_RES, taskid, payloadPull) // only respond the necessary sub chains.
            pullLogger.logInfo(`Sent PULL response to peer ${peer.id}`, taskid)
            return
        }
        
        const chainsStats = Array.isArray(payload)? Utils.arrayToBCSMap(payload): null

        if (chainsStats && chainsStats.size > 0){ // if chainsStats provided, check whether should do partially pull or pull all
            await this.worker.waitLoadReady() // Avoid reply incomplete data while pull
            const localBCS = (await this.worker.getStatistics('chain', taskid, undefined, peer?.id)).chain.stats
            for (const chainStat of Array.from(chainsStats.values())){ // compare each chain with the stats
                const subject = chainStat.subject
                const localChainStat = localBCS.get(subject)
                if (!localChainStat){ // This should never happen
                    continue
                }
                if (localChainStat.size > chainStat.size){ // In case local chain is out of date
                    const chain = this.worker.findChain(subject)
                    const deMeta = this.worker._dbMeta.getDataEntities().get(subject)
                    if (deMeta && deMeta.primaryKeys && deMeta.primaryKeys.length > 0
                        && ( deMeta.fileName !== undefined // data entity with fixed filename, there is only 1 record 
                             || localChainStat.size - chainStat.size > localChainStat.keptSize // the missed block is already cleared on local 
                             // remote chain is lacking more than CONFIG.PULL_TOLERANCE_PERCENT (%)
                             || ( localChainStat.size * (1 - CONFIG.PULL_TOLERANCE_PERCENT) > chainStat.size
                                  // remote chain is lacking more than CONFIG.PULL_TOLERANCE_BLOCKS blocks 
                                  && localChainStat.size > chainStat.size + CONFIG.PULL_TOLERANCE_BLOCKS
                                  // missed txn count exceeds CONFIG.PULL_TOLERANCE_PERCENT (%) of data row count
                                  && (localChainStat.txnSize - chainStat.txnSize) > (await this.worker.count(subject)).rowCount * CONFIG.PULL_TOLERANCE_PERCENT
                                )
                           )
                    ) { // Do Full pull for the data entity
                        let data=await getData(deMeta,subject)
                        blocksMap.set(subject, {isFullPull: true, chain, data})
                        continue  //here should do fully pull for this subject only, but not do fully pull for all subjects
                    }
                    pullLogger.logInfo(`Respond missed blocks for ${subject}`, taskid)
                    blocksMap.set(subject, chain.subChain(chainStat.size)) // Only respond the missed blocks
                }
            }
            payloadPull = {
                chains: Utils.mapToArray(blocksMap)
            }
            this._net.sendTask(peer, CHANNELS.PULL_RES, taskid, payloadPull) // only respond the necessary sub chains.
            pullLogger.logInfo(`Sent PULL response to peer ${peer.id}`, taskid)
            return
        }
        // Pull all chains and data
        const snapshot = await this.worker.getDbSnapshot()
        payloadPull = {
            chains: Utils.mapToArray(this.worker.chains),
            data: snapshot,
            isFullyPull: true
        }
        pullLogger.logInfo(`Respond for fully pull`, taskid)
        this._net.sendTask(peer, CHANNELS.PULL_RES, taskid, payloadPull)
        pullLogger.logInfo(`Sent PULL response to peer ${peer.id}`, taskid)
    }

    /**
     * Msg handler for channel: pull-res
     * @param data 
     */
    private async handlePullRes(data: any){
        const {peer, taskid, payload} = data
        pullLogger.logInfo(`Received PULL_RES from peer: ${peer.id}`, taskid)
        const pullTask = this.worker.workflowExec.getTask(taskid)
        if (!pullTask){
            throw Error('Pull task not found: ' + taskid)
        }
        if (!payload || !payload['chains'] || (!payload['data'] && payload['isFullyPull'])){
            pullLogger.logDebug(payload, taskid)
            throw Error('Payload is wrong for pull task')
        }
        try{
            if (payload.isFullyPull){ // replace all chains and data if full pull
                pullLogger.logInfo('Doing fully pull', taskid)
                await this.worker.replaceData(peer, payload.data, taskid)
                await this.worker.replaceChains(payload.chains, taskid)
                pullLogger.logInfo(`Done with fully pull`, taskid)
            }else{ // otherwise append the missed block or pull entiries
                pullLogger.logInfo('Doing increasing pull', taskid)
                const chains = payload.chains
                await this.worker.appendChains(peer, chains, taskid)
                pullLogger.logInfo(`Done with increasing pull`, taskid)
            }
            pullTask.resolve({status: true})
        } catch(e) {
            pullLogger.logSevere('Error while pull process', taskid)
            pullLogger.logError(e, taskid)
            pullTask.reject({status: false, message: e.message})
            throw e
        }
    }


    private async handlePushGo(data:any){
        try {
            const {peer, taskid, payload} = data
            //wait introduce finish to make sure the entities created.
            await this._worker.waitCluster()
            if(payload?.entities){
                pullLogger.logInfo(`Handle push entities ${(payload.entities || []).join()}  from peer: `  + peer.id + ', payload: ' + payload, taskid)
                await this._net.pull(peer.id,payload?.entities)
            }else{
                pullLogger.logInfo('Handle push all entities from peer: '  + peer.id + ', payload: ' + payload, taskid)
                await this._net.pullAll(peer.id)
            }
            await this._net.sendTask(peer, CHANNELS.PUSH_DONE, taskid,{})        
        } catch (e) {
            pullLogger.logError(e)
        }
    }

    private async handlePushDone(data:any){
        try {
            const {peer, taskid, payload} = data
            pullLogger.logInfo('Received push DONE from peer:'  + peer.id + ', payload: ' + payload, taskid)
            const pushTask = this.worker.workflow.getTask(taskid)
            pushTask?.resolve({status: true})
        } catch (e) {
            pullLogger.logError(e)
        }
    }


    private async handleConflictGo(data:any){
        try {
            const {peer, taskid, payload} = data
            pullLogger.logInfo('Received conflict resolve request from peer:'  + peer.id + ', payload: ' + payload, taskid)
            //wait introduce finish to make sure the entities created.
            await this._worker.waitCluster()
            //if(payload.entities && payload.toPeers){
            await this._worker.pushToPeers(payload.entities,payload.checkInFlag,payload.toPeers)
            //}
            await this._net.sendTask(peer, CHANNELS.AUTO_RESOLVE_DONE, taskid,{})
        } catch (e) {
            pullLogger.logError(e)
        }
    }

    private async handleConflictDone(data:any){
        try {
            const {peer, taskid, payload} = data
            pullLogger.logInfo('Received conflict resolve response from peer:'  + peer.id + ', payload: ' + payload, taskid)
            const pushTask = this.worker.workflow.getTask(taskid)
            pushTask?.resolve({status: true})
        } catch (e) {
            pullLogger.logError(e)
        }
    }

    /**
     * Msg handler for channel: txn
     * @param data 
     */
    private async handleTxn(data:any){
        timeTracer.trace('peer handleTxn')
        const {peer, taskid, payload} = data
        const subject = payload['subject']
        const txnBlock = new Block(payload['block'])
        const chain = this.worker.findChain(subject)
        // if (chain.isFurtureBlock(txnBlock)){
        //     await this.worker.appendLostBlocks(chain, txnBlock)
        // }
        timeTracer.trace('peer claimBlock')
        const valid = chain.claimBlock(txnBlock, peer.id, taskid)
        timeTracer.trace('peer claimBlock')
        const payloadRes = {
            subject,
            valid
        }
        if (!valid.isValid){
            txnLogger.logDebug(JSON.stringify(valid), taskid)
        }
        txnLogger.logDebug(`TXN from ${peer.id}, blockIdx: ${txnBlock.index}, local chain size: ${chain.size}, txnData: ${JSON.stringify(txnBlock.toJSON())}. \nResult: ${valid.isValid}`, taskid)
        timeTracer.trace('peer TXN_CONFIRM sent')
        // if (process.env.DBTEST_DUPLICATED_BLOCK === 'true') { // For testing only. Comment this out.
        //     await Utils.waitTime(20000)
        // }
        this._net.sendTask(peer, CHANNELS.TXN_CONFIRM, taskid, payloadRes)
        // if (chain.isFurtureBlock(txnBlock)){  // The "future block" might be rejected, so, it shouldn't be out of date here
        //     this.worker.status = STATUS.OUT_OF_SYNC
        //     this._net.checkin()
        // }
        timeTracer.trace('peer handleTxn')
    }

    
    /**
     * Msg handler for channel: txn
     * @param data 
     */
     private async handleBatchTxn(data:any){
        const {peer, taskid, payload} = data
        txnLogger.logDebug(`Received batch TXN from ${peer.id}, taskid: ${taskid}, payload: ${JSON.stringify(payload)}`, taskid)
        const inValidSubjects:any[] = []
        if (Array.isArray(payload)){
            const subjectMap: Map<string, any> = new Map()
            for (const data of payload){
                const subject = data['subject']
                if (subjectMap.has(subject)) continue // In case there are multiple blocks for one data entity, we should validate the 1st block only
                subjectMap.set(subject, data)
            }
            for (const data of Array.from(subjectMap.values())){
                const subject = data['subject']
                const txnBlock = new Block(data['block'])
                const chain = this.worker.findChain(subject)
                const valid = chain.claimBlock(txnBlock, peer.id, taskid)
                if (valid.isValid === false){
                    inValidSubjects.push(subject)
                }
                // if (chain.isFurtureBlock(txnBlock)){ // The "future block" might be rejected, so, it shouldn't be out of date here
                //     this.worker.status = STATUS.OUT_OF_SYNC
                //     this._net.checkin()
                // }
            }
        }
        let result:BatchTxnValidation = {isValid:true, inValidSubjects}
        if (inValidSubjects.length > 0){
            result.isValid = false
        }
        txnLogger.logDebug(`Batch TXN from ${peer.id}, taskid: ${taskid}. Result: ${JSON.stringify(result)}`, taskid)
        this._net.sendTask(peer, CHANNELS.BATCH_TXN_CONFIRM, taskid, result)
    }

    
    /**
     * Msg handler for channel: txn-confirm
     * @param data 
     */
     private async handleBatchTxnConfirm(data:any){ // TBD, resend the txn requests when any one peer rejects it??
        const {peer, taskid, payload} = data
        const batchApproval:BatchTxnValidation  = payload
        const isApproved = batchApproval.isValid
        // const inValidSubjects = payload.inValidSubjects
        txnLogger.logDebug(`Received batch txn confirmation from ${peer.id}, isApproved: ${isApproved}`, taskid)
        const workflow = this.worker.workflow
        const txnBroadcastTask = workflow.getTask(taskid)
        if (!txnBroadcastTask) {
            throw Error('Task not found: ' + taskid)
        }
        const txnProcess = workflow.getTask(txnBroadcastTask.parentId)
        if (!txnProcess) {
            throw Error('Task not found: ' + txnBroadcastTask.parentId)
        }
        
        let approvals = txnProcess.getProperty('approvals')
        if (!approvals) approvals = new Map<string, any>()
        approvals.set(peer.id, batchApproval) 
        txnProcess.setProperty('approvals', approvals)
        // txnProcess.setProperty(peer.id, isApproved)

        txnBroadcastTask.resolve(isApproved)
    }

    /**
     * Msg handler for channel: txn-confirm
     * @param data 
     */
    private async handleTxnConfirm(data:any){
        const {peer, taskid, payload} = data
        const valid = payload.valid
        const subject = payload.subject
        txnLogger.logDebug(`Received txn confirmation from ${peer.id}, dataEntity: ${subject}, taskid: ${taskid}, isApproved: ${valid.isValid}`, taskid)
        // const workflow = this.worker.getDataEntityWorkflow(subject)
        const workflow = this.worker.workflow
        const txnBroadcastTask = workflow.getTask(taskid)
        if (!txnBroadcastTask) {
            throw Error('Task not found: ' + taskid)
        }
        const txnProcess = workflow.getTask(txnBroadcastTask.parentId)
        if (!txnProcess) {
            throw Error('Task not found: ' + txnBroadcastTask.parentId)
        }
        
        let approvals = txnProcess.getProperty('approvals')
        if (!approvals) approvals = new Map<string, any>()
        approvals.set(peer.id, valid) 
        txnProcess.setProperty('approvals', approvals)
        // txnProcess.setProperty(peer.id, isApproved)

        txnBroadcastTask.resolve(valid)
    }

     /**
     * Msg handler for channel: status-confirm
     * @param data 
     */
    private async handleStatusConfirm(data:any) {
        const {peer, taskid, payload} = data
        const {status, inNet, type, peerId, version, appName} = payload
        const versionStr = version? version.major+'.'+version.minor+'.'+version.patch : 'unknown'
        clusterLogger.logDebug(`Received status confirmation from ${peer.id}, status: ${status}, inNet: ${inNet}, version: ${versionStr}, appName: ${appName}`, taskid)
        const workflow = this.worker.workflow
        const resTask = workflow.getTask(taskid);
       
        const setPro = (task: any, rsTask: any) => {             
            task.setProperty('status', {status, inNet, version, appName})
            const dbExtend=payload.dbExtend || {}
            rsTask.resolve({status, inNet, version, appName,dbExtend})
        }
    
        let statusProcess = workflow.getTask(taskid)
        if (!statusProcess) {
            throw Error('Task not found: ' + taskid)
        }

        if(type === 'broadcast') {
            statusProcess = workflow.getTask(statusProcess.parentId)
            if (!statusProcess) {
                throw Error('Task not found: ' + statusProcess)
            }
            let statuses = statusProcess.getProperty('statuses')
            if (!statuses) {
                statuses = new Map<string, any>()
                statuses.set('statuses', [{id: peerId, status, inNet}])
            } else {
                const arr = statuses.get('statuses');
                arr.push({id: peerId, status, inNet})
                statuses.set('statuses', arr)
            }
            statusProcess.setProperty('statuses', statuses)
        }
        
        setPro(statusProcess, resTask)
    }
    /**
     * Msg handler for channel: txn-go
     * @param data 
     */
     private async handleBatchTxnGo(data:any){
        const {peer, taskid, payload} = data
        txnLogger.logDebug(`Received batch txn GO from ${peer.id}, taskid: ${taskid}`, taskid)
        const results:any[] = []
        const blocksMap = new Map()
        if (Array.isArray(payload)){
            for await (const data of payload){
                const subject = data['subject']
                const txnBlock = new Block(data['block'])
                let blocksArr:Block[]
                if (blocksMap.has(subject)){
                    blocksArr = blocksMap.get(subject)
                } else {
                    blocksArr = []
                    blocksMap.set(subject, blocksArr)
                }
                blocksArr.push(txnBlock)
            }
        }
        await this._worker.appendBlocks(blocksMap, taskid)
        await this._net.sendTask(peer, CHANNELS.BATCH_TXN_DONE, taskid, results)
    }

    /**
     * Msg handler for channel: txn-go
     * @param data 
     */
    private async handleTxnGo(data:any){
        timeTracer.trace('peer handleTxnGo')
        const {peer, taskid, payload} = data
        const subject = payload['subject']
        txnLogger.logDebug(`Received txn GO from ${peer.id}, taskid: ${taskid}, subject: ${subject}`, taskid)
        const txnBlock = new Block(payload['block'])
        const chain = this.worker.findChain(subject)
        if (chain.isFurtureBlock(txnBlock)){
            txnLogger.logWarn(`Received future block for ${chain.subject}, ` +
                `\ncurrent chain size: ${chain.size}, \nreceived block: ${JSON.stringify(txnBlock.toJSON())}, ` +
                `\n last block is: ${JSON.stringify(chain.getBlock(-1))}`, taskid)
            await this.worker.appendLostBlocks(chain, txnBlock, taskid)
        }
        if (chain.isFurtureBlock(txnBlock)){
            txnLogger.logWarn(`Failed for chain supplementing`, taskid)
            this.worker.status = STATUS.OUT_OF_SYNC
            this._net.checkin()
            return await this._net.sendTask(peer, CHANNELS.TXN_DONE, taskid, {subject, result: {status: false, message: 'Local chain out of date'}})
        }
        timeTracer.trace('peer appendBlock')
        const result = await this.worker.appendBlock(chain, txnBlock, true, taskid)
        timeTracer.trace('peer appendBlock')
        timeTracer.trace('peer TXN_DONE sent')
        await this._net.sendTask(peer, CHANNELS.TXN_DONE, taskid, {subject, result})
        timeTracer.trace('peer handleTxnGo')
    }

    
    /**
     * Msg handler for channel: txn-done
     * @param data 
     */
     private async handleBatchTxnDone(data:any){ // TBD, What if there is no TXN-DONE returned?
        const {peer, taskid, payload} = data
        txnLogger.logDebug(`Received batch txn DONE from ${peer.id}, taskid: ${taskid}, payload: ${payload}`, taskid)
        const txnGoBroadCastTask = this.worker.workflow.getTask(taskid)
        txnGoBroadCastTask?.resolve(payload)
    }

    /**
     * Msg handler for channel: txn-done
     * @param data 
     */
    private async handleTxnDone(data:any){ // TBD, What if there is no TXN-DONE returned?
        const {peer, taskid, payload} = data
        txnLogger.logDebug(`Received txn DONE from ${peer.id}, taskid: ${taskid}, payload: ${payload}`, taskid)
        const {result} = payload
        // const {subject, result} = payload
        // const txnGoBroadCastTask = this.worker.getDataEntityWorkflow(subject).getTask(taskid)
        const txnGoBroadCastTask = this.worker.workflow.getTask(taskid)
        txnGoBroadCastTask?.resolve(result)
    }

    /**
     * BZ-15355, sync file for cluster
     * create read scream and sent file content 
     */
    public sendFileToPeer(peerId: string, taskId: string, fsi: FileSyncInfo): Promise<any> {
      return new Promise(async (resolve, reject) => {
        if (0) reject(`ignore error TS6133 ('reject' is declared but its value is never read)`);
        const channel = CHANNELS.FILE_TRANSFER_RES;
        try {
            const peerData = this.worker._internalDb.selectSync('meta_peers', {id: peerId}).data[0];
            clusterLogger.logInfo(`FileSync::sendFileToPeer >>> [SEND TO] '${peerId}'`, taskId);
            let sendStream = null;
            if (0) {  // keep the codes here if we want a new stream for each file transfer
                sendStream = await this.ensureSendStreamForPeer(peerData, channel);
            } else {
                sendStream = await this._msgSender.getInputStream(peerData, channel);
            }
            const fsiStream = this.worker._fileSync.getFileReadStream(taskId, fsi);
            fsiStream.on('error', (err: any) => {
              clusterLogger.logSevere(`FileSync::sendFileToPeer, ==FAIL== (fsiStream error), ${err}`, taskId);
              resolve(false);
            })
            fsiStream.on('close', () => {
              clusterLogger.logInfo(`FileSync::sendFileToPeer, ==CLOSE== (stream)`, taskId);
              resolve(false);  // might be caused by network disconnection..
            });
            fsiStream.on('end', () => {
              clusterLogger.logInfo(`FileSync::sendFileToPeer, ==END== (stream)`, taskId);
              clusterLogger.logInfo(`FileSync::sendFileToPeer' >>> [SEND DONE]`, taskId);
              resolve(true);
            });

            fsiStream.pipe(sendStream);
            clusterLogger.logInfo(`FileSync::sendFileToPeer >>> [SENDING]`, taskId);
        } catch (e) { // Error should be catched if the connection is broken while sending msg
            clusterLogger.logSevere('FileSync::sendFileToPeer ==EXCEPTION==');
            clusterLogger.logError(e);
            resolve(false)
        }
      });
    }
    
    /**
     * BZ-15355, sync file for cluster
     * handle file transfer request, add queue to send file 
     */
    private handleFileTransferReq(data: any) {
        const {peer, taskid, payload} = data;
        clusterLogger.logInfo(`FileSync::handleFileTransferReq, request from peer '${peer.id}'`, taskid);

        /*if (!FileSync.isFileSyncMsg(payload)) {
            Logger.logSevere(`FileSync::handleFileTransferReq, invalid data for protocol '${CHANNELS.FILE_TRANSFER_REQ}'`);
            return;
        }*/
        
        for (const item of payload) {
            if (!FileSync.isFileSyncMsg(item)) {
                continue;
            }
            const fsm: FileSyncMsg = item as FileSyncMsg;
            const task: FileSyncTask = {
                peerId: peer.id,
                fsm: fsm
            }
            this.worker._fileSync.pushSendTask(task);
        }
    }

    /**
     * BZ-15355, sync file for cluster
     * handle file transfer response, parse data header and save file content to file
     */
    private async handleFileTransferRes(channel: string) {
        const writeFileFromStream = this.worker._fileSync.writeFileFromStream.bind(this.worker._fileSync);
        this._node.handle(channel, async ({ stream, connection }) => { // libp2p event handling
            try {
                if (!this.channelAuthCheck(channel, connection)){ // Do nothing if the message is from illegal node.
                    return;
                }
                clusterLogger.logInfo(`FileSync::handleFileTransferRes, <<< [RECV FROM] '${connection.remotePeer.toString()}', '${channel}'`);
                await pipe(
                    stream,
                    writeFileFromStream
                );
                clusterLogger.logInfo(`FileSync::handleFileTransferRes, <<< [RECV DONE]`);
            } catch (e) {
                clusterLogger.logSevere('Error: handleFileTransferRes failed');
                clusterLogger.logError(e);
            }
        });
    }

    /**
     * 
     * @param peer 
     * @param channel 
     * @returns The stream to send msg to the peer and channel
     */
    private async ensureSendStreamForPeer(peer: IPeerMeta, channel: string ){
        clusterLogger.logDebug('Ensure send stream for peer: ' + peer.id)
        const peerInst = await NodeMetadata.fromJSON(peer)
        // if (!peerSendStrm.has(channel)){
            try{
                const ma = await this._node.peerStore.addressBook.getPeerRecord(peerInst.peerIdInst)
                if (!ma){
                    this._net._nodeStateHandler.addAddressBook(peerInst)
                    await this._pnNode.dial(peerInst) // connect to the peer
                }
            } catch (e) {
            //     const peerState: PeerState = this._net.getPeerState(peerInst.id)
            //     this._net.setPeerState(peer.id, {peer: peer, status: STATUS.PEER_CONNECT_FAILED})// record peer status
                clusterLogger.logDebug(e.stack)
                throw Error(`Peer is not online: ${peer.id}`)
            }
        // }
        // await this._pnNode.dial(peerInst) // connect to the peer
        const stream = await this._node.dialProtocol(peerInst.peerIdInst, channel) // Each message is a seperate stream.
        return stream
    }

    /**
     * Sends message to the given peer and channel
     * @param peer 
     * @param channel 
     * @param msg 
     */
    async sendMsgToPeer(peer: IPeerMeta, channel: string, msg: string, isNotZipped = false){
        try{
            timeTracer.trace('sendMsgToPeer' + channel)
            // const stream = await this.ensureSendStreamForPeer(peer, channel)
            let b64: string
            timeTracer.trace('gzip')
            if (CONFIG.MESSAGE_COMPRESS === 'GZIP' && !isNotZipped){ // In case isNotZipped = true, the message is sent to old version without zip.
                const bufTemp = await gzip(msg)
                b64 = bufTemp.toString('base64') + CONFIG.PACKET_END_ESC
            } else {
                b64 = Buffer.from(msg, 'utf8').toString('base64') + CONFIG.PACKET_END_ESC
            }
            timeTracer.trace('gzip')
            timeTracer.trace('base64')
            timeTracer.trace('base64')
            clusterLogger.logDebug('Sending message to peer: ' + peer.id + ', channel: ' + channel + ', length: ' + b64.length)
            timeTracer.trace('send msg' + channel)
            await this._msgSender.send(peer, channel, b64)
            timeTracer.trace('send msg' + channel)
            timeTracer.trace('sendMsgToPeer' + channel)
        } catch (e) { // Error should be catched if the connection is broken while sending msg
            throw e // Throw the error out to fail the task that sends the msg
        }
    }

}

export{
    NodeMsgHandler, BatchTxnValidation
}
// Dependencies to node.js packages
import fs from 'fs-extra'
import path from 'path'

// Dependencies to open source package: libp2p
import { Libp2p, createLibp2p } from 'libp2p'
import { tcp } from '@libp2p/tcp'
import { mplex } from '@libp2p/mplex'
import { noise } from '@chainsafe/libp2p-noise'
import { mdns } from '@libp2p/mdns'
import { bootstrap } from '@libp2p/bootstrap'
import { preSharedKey } from 'libp2p/pnet'

// Dependencies to local code.
import { CHANNELS } from '../constants/channels.js'
import { INodeMetaExtended, IPeerMeta, NodeMetadata } from './node-metadata.js'
import { NodeMsgHandler } from './node-msg-handler.js'
import { PrivateNet } from './private-net.js'
import { DbWorker } from '../main/db-worker.js'
import { Utils, loggers } from '../services/index.js'
import { CONFIG } from '../constants/config.js'
import { VERSION } from '../constants/version.js'
import { MetaCluster, PeerDiscovery } from '../main/metadata.js'

// Calculates the __dirname in ESM
import { fileURLToPath } from 'url';
import { dirname } from 'path';
const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

// Private net swarmKey.
const b64 = 'L2tleS9zd2FybS9wc2svMS4wLjAvCi9iYXNlMTYvCmMzNmNjZGY3OWJlMGQ4OThjMDVkNTU4ZTM0MThiNTQzZDUyZmJlYmM3NzY3Y2M4MDdiODI1OWJjZjg1ZTdmYjk='
const swarmKey = Uint8Array.from(Buffer.from(b64, 'base64'))

const clusterLogger = loggers.cluster

/**
 * Node of cluster in private net
 */
class PrivateNetNode {

    _node: Libp2p
    _meta: INodeMetaExtended // Json format of metadata. 
    _nodeMeta: NodeMetadata // Metadata includes instances for PeerId and Multiaddr
    _msgHandler: NodeMsgHandler
    _worker: DbWorker
    _net: PrivateNet
    _listening: boolean = false
    _port: number
    _doStart: boolean = true // Should start the node or not.
    _metaCluster: MetaCluster
    _startProm: Promise<any> // Resolves when node is started or no need to start.
    _startPromResolve: Function

    constructor(worker: DbWorker, net: PrivateNet){
        this._worker = worker
        this._net = net
        this._metaCluster = this._worker._dbMeta.cluster
        this._startProm = new Promise((resolve) => {
          this._startPromResolve = resolve
        })
    }

    /**
     * Selects the meta_node
     * @returns Stored node metadata if exist. Otherwise null.
     */
    private getNodeMeta() {
      const result = this._worker._internalDb.selectSync('meta_node');
      if (result.rowCount > 0){
        return result.data[0]
      }
      return null
    }

    /**
     * Selects the meta_peer
     * @returns Stored peer metadata of local node if exist. Otherwise null.
     */
    private getPeerMeta(id: string) {
      const result = this._worker._internalDb.selectSync('meta_peers', {id});
      if (result.rowCount > 0){
        return result.data[0]
      }
      return null
    }

    /**
     * Loads the stored meta_node and meta_peer
     * @returns true if meta_node data exists 
     */
    private async loadStoredMetadata() {
      let meta: INodeMetaExtended = this.getNodeMeta()
      if (meta){ // Not the 1st time of startup, so the metadata already exists.
        const peerMeta = this.getPeerMeta(meta.id)
        if (this._net.isAutoScalingEnabled() && this._net.isScalableNode()){ // In case of scaling node, clear any peer node that exists before
          await this._worker._internalDb.delete('meta_peers')
          await this._worker._internalDb.insert('meta_peers', peerMeta)
        }
        const privKey = meta.peerId?.privKey
        if (peerMeta) {
          meta = Object.assign(meta, peerMeta) // Merge the meta_peer properties into meta. E.g. multiaddrs.
          meta.peerId!.privKey = privKey
        }
        try{
          if (this._metaCluster.proxy?.port) {
            meta.port = parseInt(this._metaCluster.proxy?.port)
            meta.localAddrs = meta.localAddrs?.split('/tcp/')[0] + '/tcp/' + meta.port
          } else {
            meta.port = meta.port ? meta.port : Number(meta.localAddrs?.split('/')[4]);
          }
        } catch (e) {
          clusterLogger.logWarn('Error encountered while overwriting port with cluster.proxy.port')
          clusterLogger.logError(e)
          meta.port = meta.port ? meta.port : Number(meta.localAddrs?.split('/')[4]);
        }
        this._meta = meta
        if (!await Utils.isFreePort(meta.port)){
          const msg = `The cluster port ${meta.port} is in use. Failed to start cluster listener.`
          if (this._net.isInCluster()){ // In case the port is in use, and it's already in cluster, throw Error, stop server.
            setTimeout(() => { // Ends the process on 3 seconds
              process.exit(1)
            }, 3000);
            throw Error(msg) // Here if let it go, it will grab the port used by other applicaiton..., so throwing the error here.
          } else { // In case the port is in use, and it's not in cluster, log the error and continue to start without clustering.
            clusterLogger.logWarn(msg)
            this._doStart = false
          }
          clusterLogger.logSevere(msg)
        }
        return true
      } else {
        clusterLogger.logInfo('Node metadata not exist. Will generate new metadata')
        let port;
        try {
          port = (this._metaCluster.proxy?.port) ? parseInt(this._metaCluster.proxy.port) : CONFIG.NODE_DEFAULT_PORT
        } catch (e) {
          clusterLogger.logWarn('Error encountered while overwriting port with cluster.proxy.port')
          clusterLogger.logError(e)
          port = CONFIG.NODE_DEFAULT_PORT
        }
        port = await Utils.getFreePortSince(port) // get a free port number since 8643
        this._meta = {
          id: '',
          port,
          localAddrs: '/ip4/0.0.0.0/tcp/' + port // Builds up the local address for the 1st startup. TBD, IPv6 support
        }
      }
      return false
    }

    /**
     * Replaces the IP address in localAddrs if configured.
     * @returns true if the replacement is done
     */
    private replaceHostIp() {
      let spicificHostIp = this._worker.hostSpicificIp
      if (spicificHostIp) {
        const newAddr = `/ip4/${spicificHostIp}/tcp/` + this._meta.port // TBD, IPv6 support
        if (newAddr !== this._meta.localAddrs) {
          clusterLogger.logInfo('spicific host Ip:' + spicificHostIp)
          clusterLogger.logInfo('get local address:' + this._meta.localAddrs)
          this._meta.localAddrs = newAddr // Overrides the localAddrs with spicificHostIp.
          clusterLogger.logInfo('reset local address:' + this._meta.localAddrs)
          return true
        }
      }
      return false
    }

    /**
     * Saves the generated cluster metadata for the 1st time start
     */
    private async saveClusterMetadata() {
      await this._worker._internalDb.insert('meta_node', this._nodeMeta.getNodeMeta()) // Save meta_node to file
      // await this._worker._internalDb.insert('meta_peers', this._nodeMeta.getPeerMeta()) // Save meta_peers to file
    }

    async updateNodeMetadata() {
      let multiaddrs: string[] = []
      this._node.getMultiaddrs().forEach(ma => {
        multiaddrs.push(ma.toString())
      })
      
      if (this._metaCluster.proxy?.ip) { // cluster.proxy.ip configured. 
        const str = multiaddrs[0]
        const addr = str.substring(0, str.indexOf('/', 1) + 1) + this._metaCluster.proxy!.ip + str.substring(str.indexOf('/tcp/'))
        multiaddrs = [addr]
      }
      if (this._metaCluster.proxy?.protocol) { // cluster.proxy.ip configured. 
        const multiaddrsNew: string[] = []
        multiaddrs.forEach(ma => {
          const addr = '/' + this._metaCluster.proxy!.protocol + ma.substring(ma.indexOf('/', 1));
          multiaddrsNew.push(addr)
        })
        multiaddrs = multiaddrsNew
      }
      if (this._metaCluster.proxy?.port) { // cluster.proxy.port configured. 
        const multiaddrsNew: string[] = []
        multiaddrs.forEach(ma => {
          const prefix = ma.substring(0, ma.indexOf('/tcp/') + 5)
          const surfix = ma.indexOf('/p2p/') > 0? ma.substring(ma.indexOf('/p2p')): ''
          multiaddrsNew.push(prefix + this._metaCluster.proxy!.port + surfix)
        })
        multiaddrs = multiaddrsNew
      }
      this._meta.multiaddrs = multiaddrs // Take the multiaddrs generated by libp2p
      this._meta.id = this._node.peerId.toString() // Take the PeerId generated by libp2p
      this._meta.peerId = {
        id: this._meta.id,
        pubKey: Buffer.from(this._node.peerId.publicKey!).toString('base64'), // Save the pub / priv keys as base64 string
        privKey: Buffer.from(this._node.peerId.privateKey!).toString('base64')
      }
      this._meta.version = VERSION
      this._meta.timestamp = Date.now()
      this._meta.nodeType = this._worker._dbMeta.cluster.autoScaling.nodeType
      this._nodeMeta = await NodeMetadata.fromJSON(this._meta) // Recreate the _nodeMeta with new data.
    }

    /**
     * Starts the local node
     */
    async start(){
      const hasMetadata = await this.loadStoredMetadata() // Sets value for this._meta
      const hasReplacedHostIp = this.replaceHostIp() // Replaces this._meta.localAddrs
      this._nodeMeta = await NodeMetadata.fromJSON(this._meta) // Converts JSON data into data types used by libp2p
      const libp2pConfig = {
        peerId: this._nodeMeta.peerIdInst, // it is undefined for the 1st startup.
        addresses: {
          listen: [this._meta.localAddrs]
        },
        transports: [tcp()], // We're only using the TCP transport for this example
        streamMuxers: [mplex()], // We're only using mplex muxing
        connectionEncryption: [noise()],
        connectionProtector: preSharedKey({
          psk: swarmKey
        }),
        start: false
      }
      this.configPeerDiscovery(libp2pConfig)
      this._node = await createLibp2p(libp2pConfig)
      this._msgHandler = new NodeMsgHandler(this, this._worker, this._net)
      if (this._doStart){
        await this._node.start() // Start the local node, and listen to the cluster port.
        await this.updateNodeMetadata()
        await this._worker._internalDb.updateOrInsert('meta_peers', this._nodeMeta.getPeerMeta()) // Overrides peerMeta with possible new ip addr and ts
        this._listening = true
        this._startPromResolve() // resolves the promise when cluster node is started, and metadata is written
        if (!hasMetadata) { // first start
          await this.saveClusterMetadata()
        }
        if(hasReplacedHostIp){ //if changed, use spicific Host Address
          clusterLogger.logInfo('update localAddrs in meta_node,meta_peers')
          await this._worker._internalDb.updateOrInsert('meta_node', this._nodeMeta.getNodeMeta())
        }
        const result = this._worker._internalDb.selectSync('meta_config');
        if(result.rowCount === 0) {
          await this._worker._internalDb.insert('meta_config', this.getMetaConfig())
        }
        clusterLogger.logInfo('node started...')
        await this._net._nodeStateHandler.setAddressBook()
        await this._net.dialAll()
      } else {
        this._startPromResolve() // resolves the promise in case it won't start.
      }
    }

    configPeerDiscovery(libp2pConfig: any) {
      const cluster = this._metaCluster
      clusterLogger.logInfo('Config: ' + JSON.stringify(cluster))
      if (cluster.enabled && cluster.autoScaling.enabled) {
        if (cluster.autoScaling.discovery === PeerDiscovery.BOOTSTRAP) {
          if (cluster.autoScaling.peerList && cluster.autoScaling.peerList.length > 0) { // Do bootstrap when peerList is not empty
            libp2pConfig['peerDiscovery'] = [
              bootstrap({
                list: cluster.autoScaling.peerList,
                timeout: 1000
              })]
          }
        } else { // default value is mdns
          clusterLogger.logInfo('BZDB auto-scaling method: mdns')
          const mdnsOps = {
            broadcast: true,
            interval: 3600 * 1000,
            serviceTag: 'com.rs.te.web.local'
          }
          libp2pConfig['peerDiscovery'] = [mdns(mdnsOps)]
        }
      }
    }

    waitForClusterNodeStart(): Promise<any> {
      return this._startProm
    }

    isListening(): boolean {
      return this._listening
    }

    get metadata(){
      return this._meta
    }

    async stop(){
      await this._node.unhandle(Object.values(CHANNELS))
      await this._node.stop()
    }

    getMetaConfig() {
      return JSON.parse(String(fs.readFileSync(path.join(__dirname, '../../sys/sysconfig.json'))))
    }

    getPeerInfo(): IPeerMeta {
      return this._nodeMeta.getPeerMeta()!
    }

    async dial(peerMeta: NodeMetadata){
      // Add node 2 data to node 1's PeerStore
      clusterLogger.logDebug('dialing peer: ' + JSON.stringify(peerMeta.id))
      try{
        const isPeerKnown = await this._node.peerStore.has(peerMeta.peerIdInst)
        if (isPeerKnown) {
          await this._node.dial(peerMeta.peerIdInst)
        } else {
          clusterLogger.logWarn('Peer data not loaded yet for ' + peerMeta.id)
        }
      }catch(e){
        if(e.message === 'All promises were rejected'){
          clusterLogger.logWarn('Peer is not connected : ' + peerMeta.id)
        }
        throw e
      }
    }

    getMsgHandler(): NodeMsgHandler{
      return this._msgHandler
    }

    async getAddrs() {
      await this.waitForClusterNodeStart()
      return this._meta.multiaddrs || []
    }

}

export{
  PrivateNetNode
}